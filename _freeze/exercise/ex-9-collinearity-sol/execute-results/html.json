{
  "hash": "b607c0120075c9bbaa8ecff3316f4d53",
  "result": {
    "markdown": "---\ntitle: \"Exercise 9: Collinearity Solution\"\neditor: source\nreference-location: margin\n---\n\n\n::: callout-note\nExercises are for practice purpose only.\n:::\n\n\n## Collinearity\n\n<!-- ISL 3.14 -->\n\n1. Perform the code\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(1)\nx1 <- runif(100)\nx2 <- 0.5 * x1 + rnorm (100) / 10\ny <- 2 + 2 * x1 + 0.3 * x2 + rnorm (100)\n```\n:::\n\n\n\n  Write out the form of the linear model. What are the regression coefficients?\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# y = beta0 + beta1 x1 + beta2 x2 + epsilon\n# beta0 = 2; beta1 = 2, beta2 = 0.3\n```\n:::\n\n\n\n2. Create a scatterplot displaying the relationship between `x1` and `x2`.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nplot(x1, x2)\n```\n\n::: {.cell-output-display}\n![](ex-9-collinearity-sol_files/figure-html/unnamed-chunk-3-1.png){width=672}\n:::\n:::\n\n\n\n3. Fit a least squares regression to the data using `x1` and `x2`. How the LSEs relate to the true $\\beta_0$, $\\beta_1$ and $\\beta_2$? Can you reject $H0 : \\beta_1 = 0$? How about $H0 : \\beta_2 = 0$?\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfit <- lm(y ~ x1 + x2)\nround(summary(fit)$coef, 3)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n            Estimate Std. Error t value Pr(>|t|)\n(Intercept)     2.13      0.232   9.188    0.000\nx1              1.44      0.721   1.996    0.049\nx2              1.01      1.134   0.891    0.375\n```\n:::\n:::\n\n\n\n\n4. Fit a least squares regression using only `x1`. Comment on your results. Can you reject $H0 : \\beta_1 = 0$?\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfit1 <- lm(y ~ x1)\nround(summary(fit1)$coef, 3)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n            Estimate Std. Error t value Pr(>|t|)\n(Intercept)    2.112      0.231   9.155        0\nx1             1.976      0.396   4.986        0\n```\n:::\n:::\n\n\n\n5. Fit a least squares regression using only `x2`. Comment on your results. Can you reject $H0 : \\beta_2 = 0$?\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfit2 <- lm(y ~ x2)\nround(summary(fit2)$coef, 3)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n            Estimate Std. Error t value Pr(>|t|)\n(Intercept)     2.39      0.195  12.261        0\nx2              2.90      0.633   4.580        0\n```\n:::\n:::\n\n\n\n6. Do the results obtained in (3)â€“(5) contradict each other? Explain your answer.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# No, because x1 and x2 have collinearity, it is hard to distinguish their effects when regressed upon together. When they are regressed upon separately, the linear relationship between y and each predictor is indicated more clearly. When using two variables that are highly collinear, the effect on the response of one variable can be masked by the other.\n```\n:::\n\n\n\n7. Now suppose we obtain one additional observation, which was unfortunately mismeasured. Re-fit the linear models from (3) to (5) using this new data. What effect does this new observation have on the each of the models? In each model, is this observation an outlier? A high-leverage point? Both? Explain your answers.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nx1 <- c(x1, 0.1)\nx2 <- c(x2, 0.8)\ny <- c(y, 6)\nplot(x1, x2)\npoints(0.1, 0.8, pch = 16, col = 2)\n```\n\n::: {.cell-output-display}\n![](ex-9-collinearity-sol_files/figure-html/unnamed-chunk-8-1.png){width=672}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nmisfit <- lm(y ~ x1 + x2)\nround(summary(misfit)$coef, 3)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n            Estimate Std. Error t value Pr(>|t|)\n(Intercept)    2.227      0.231   9.624    0.000\nx1             0.539      0.592   0.911    0.365\nx2             2.515      0.898   2.801    0.006\n```\n:::\n\n```{.r .cell-code}\nmisfit1 <- lm(y ~ x1)\nround(summary(misfit1)$coef, 3)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n            Estimate Std. Error t value Pr(>|t|)\n(Intercept)    2.257      0.239   9.445        0\nx1             1.766      0.412   4.282        0\n```\n:::\n\n```{.r .cell-code}\nmisfit2 <- lm(y ~ x2)\nround(summary(misfit2)$coef, 3)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n            Estimate Std. Error t value Pr(>|t|)\n(Intercept)    2.345      0.191  12.264        0\nx2             3.119      0.604   5.164        0\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nplot(misfit$fitted.values, rstudent(misfit))\n```\n\n::: {.cell-output-display}\n![](ex-9-collinearity-sol_files/figure-html/unnamed-chunk-10-1.png){width=672}\n:::\n\n```{.r .cell-code}\nrstud <- round(sort(rstudent(misfit), decreasing = TRUE), 2)\nrstud[abs(rstud) > 2]\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n   21   101    16    55    82 \n 2.23  2.11  2.01 -2.27 -2.64 \n```\n:::\n\n```{.r .cell-code}\nhatval <- round(sort(hatvalues(misfit), decreasing = TRUE), 2)\nhatval[hatval > 2 * 3 / 101]\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n 101   11    6   47   18 \n0.41 0.06 0.06 0.06 0.06 \n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nplot(misfit1$fitted.values, rstudent(misfit1))\n```\n\n::: {.cell-output-display}\n![](ex-9-collinearity-sol_files/figure-html/unnamed-chunk-11-1.png){width=672}\n:::\n\n```{.r .cell-code}\nrstud1 <- round(sort(rstudent(misfit1), decreasing = TRUE), 2)\nrstud1[abs(rstud1) > 2]\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n  101    21    56    55    82 \n 3.44  2.27  2.17 -2.38 -2.70 \n```\n:::\n\n```{.r .cell-code}\nhatval1 <- round(sort(hatvalues(misfit1), decreasing = TRUE), 2)\nhatval1[hatval1 > 2 * 2 / 101]\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n  27   47   18   92   10   80   55    7   69 \n0.04 0.04 0.04 0.04 0.04 0.04 0.04 0.04 0.04 \n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nplot(misfit2$fitted.values, rstudent(misfit2))\n```\n\n::: {.cell-output-display}\n![](ex-9-collinearity-sol_files/figure-html/unnamed-chunk-12-1.png){width=672}\n:::\n\n```{.r .cell-code}\nrstud2 <- round(sort(rstudent(misfit2), decreasing = TRUE), 2)\nrstud2[abs(rstud2) > 2]\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n   21     5    55    82 \n 2.30 -2.01 -2.33 -2.55 \n```\n:::\n\n```{.r .cell-code}\nhatval2 <- round(sort(hatvalues(misfit2), decreasing = TRUE), 2)\nhatval2[hatval2 > 2 * 2 / 101]\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n 101    6   18   47   20   91   34   86   27   24   55   72 \n0.10 0.06 0.06 0.06 0.05 0.05 0.04 0.04 0.04 0.04 0.04 0.04 \n```\n:::\n:::\n",
    "supporting": [
      "ex-9-collinearity-sol_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}