---
title: 'Multiple Linear Regression - Matrix Approach `r fontawesome::fa("table")`'
subtitle: "MATH 4780 / MSSC 5780 Regression Analysis"
author: "Dr. Cheng-Han Yu <br> Department of Mathematical and Statistical Sciences <br> Marquette University"
# date: "`r format(Sys.time(), '%B %d %Y')`"
# macros: _macros.tex # import a list of TeX/LaTeX definitions
format: 
  revealjs:
    # code-line-numbers: false
    #     - "macros.tex"
    html-math-method:
      method: mathjax
      url: "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"
    # include-in-header:
    highlight-style: arrow
    code-block-bg: true
    self-contained: false
    slide-number: c/t    
    incremental: false
    width: 1800
    height: 1000
    margin: 0.05
    logo: "https://upload.wikimedia.org/wikipedia/commons/e/e8/Marquette_Golden_Eagles_logo.svg"
    footer: "[math4780-f23.github.io/website](https://math4780-f23.github.io/website/)"
    theme: ["simple", "slides.scss"]
    multiplex: true
    code-link: true
    fig-cap-location: bottom
    fig-align: center
    transition: none ## fade slide convex concave zoom
    title-slide-attributes:
      data-background-color: "#447099"
      # data-background-image: images/paper-texture.jpg
      # data-background-size: cover
      # data-background-color: "#698ED5"
editor: source
execute:
  freeze: true
  echo: false
  purl: true
---

#  {visibility="hidden"}

\def\bx{\mathbf{x}}
\def\bg{\mathbf{g}}
\def\bw{\mathbf{w}}
\def\balpha{\boldsymbol \alpha}
\def\bbeta{\boldsymbol \beta}
\def\beps{\boldsymbol \epsilon}
\def\bX{\mathbf{X}}
\def\by{\mathbf{y}}
\def\bH{\mathbf{H}}
\def\bI{\mathbf{I}}
\def\bS{\mathbf{S}}
\def\bW{\mathbf{W}}
\def\T{\text{T}}
\def\cov{\mathrm{Cov}}
\def\cor{\mathrm{Corr}}
\def\var{\mathrm{Var}}
\def\E{\mathrm{E}}
\def\bmu{\boldsymbol \mu}
\DeclareMathOperator*{\argmin}{arg\,min}
\def\Trace{\text{Trace}}


```{r}
#| label: setup
#| include: false
#| eval: true
library(countdown)
library(emo)
library(knitr)
library(gt)
library(gtExtras)
library(ggplot2)
library(tidyverse)
library(tidymodels)

knitr::opts_chunk$set(
    fig.asp = 0.6,
    fig.align = "center",
    out.width = "100%",
    fig.retina = 10,
    fig.path = "./images/06-mlr-mat/",
    message = FALSE,
    global.par = TRUE
)
options(
  htmltools.dir.version = FALSE,
  dplyr.print_min = 6, 
  dplyr.print_max = 6,
  tibble.width = 80,
  width = 80,
  digits = 2
  )
hook_output <- knitr::knit_hooks$get("output")
knitr::knit_hooks$set(output = function(x, options) {
  lines <- options$output.lines
  if (is.null(lines)) {
    return(hook_output(x, options))  # pass to default hook
  }
  x <- unlist(strsplit(x, "\n"))
  more <- "..."
  if (length(lines)==1) {        # first n lines
    if (length(x) > lines) {
      # truncate the output, but add ....
      x <- c(head(x, lines), more)
    }
  } else {
    x <- c(more, x[lines], more)
  }
  # paste these lines together
  x <- paste(c(x, ""), collapse = "\n")
  hook_output(x, options)
})
```


```{r}
#| echo: false
delivery <- read.csv(file = "./data/data-ex-3-1.csv",
                     header = TRUE)
delivery_data <- delivery[, -1]
colnames(delivery_data) <- c("time", "cases", "distance")
delivery_lm <- lm(time ~ cases + distance, data = delivery_data)
summ_delivery <- summary(delivery_lm)
```


## Regression Model in Matrix Form

$$y_i= \beta_0 + \beta_1x_{i1} + \beta_2x_{i2} + \dots + \beta_kx_{ik} + \epsilon_i, \quad \epsilon_i \stackrel{iid}{\sim} N(0, \sigma^2), \quad i = 1, 2, \dots, n$$

<!-- - $\epsilon_i \stackrel{iid}{\sim} N(0, \sigma^2)$ -->

. . .


$$\bf y = \bf X \bbeta + \beps$$ where

$$\begin{align}
\bf y = \begin{bmatrix}
y_1 \\
y_2 \\
\vdots \\
y_n \end{bmatrix},\quad
\bf X = \begin{bmatrix}
  1 & x_{11} & x_{12} & \cdots & x_{1k} \\
  1 & x_{21} & x_{22} & \cdots & x_{2k} \\
  \vdots & \vdots  & \vdots  & \ddots & \vdots  \\
  1 & x_{n1} & x_{n2} & \cdots & x_{nk} \end{bmatrix}, \quad
\bbeta = \begin{bmatrix}
\beta_0 \\
\beta_1 \\
\vdots \\
\beta_k \end{bmatrix} , \quad
\beps = \begin{bmatrix}
\epsilon_1 \\
\epsilon_2 \\
\vdots \\
\epsilon_n\end{bmatrix}
\end{align}$$

- ${\bf X}_{n \times p}$: Design matrix
- $\beps \sim MVN_n({\bf 0}, \sigma^2 {\bf I}_n)$ [^1]


[^1]: For simplicity and convenience, $N({\bf a}, {\bf B})$ represents a multivariate normal distribution with mean vector ${\bf a}$ and covariance matrix ${\bf B}$.



::: notes
- Let's first write our model in a matrix form.
- $\bf X$ is usually called the design matrix.
:::




## Regression Model in Matrix Form  {visibility="hidden"}

::: fact
For a random vector ${\bf Z} = (Z_1, \dots, Z_n)'$, its mean vector is
$$E({\bf Z}) = E\begin{pmatrix}
Z_1 \\
Z_2 \\
\vdots \\
Z_n\end{pmatrix} = \begin{bmatrix}
E(Z_1) \\
E(Z_2) \\
\vdots \\
E(Z_n)\end{bmatrix}$$

Its (symmetric) variance-covariance matrix is 
 $$\scriptsize \begin{align} \cov({\bf Z}) = {\bf \Sigma} &= \begin{bmatrix} \var(Z_1) & \cov(Z_1, Z_2) & \cdots & \cov(Z_1, Z_n) \\ \cov(Z_2, Z_1) & \var(Z_2) & \cdots & \cov(Z_2, Z_n) \\ \vdots  & \vdots  & \ddots & \vdots  \\ \cov(Z_n, Z_1) & \cov(Z_n, Z_2) & \cdots & \var(Z_n) \end{bmatrix} \end{align}$$
:::


## Least Squares Estimation in Matrix Form
$$S(\beta_0, \beta_1, \dots, \beta_k) = \sum_{i=1}^n\epsilon_i^2 = \sum_{i=1}^n\left(y_i - \beta_0 - \sum_{j=1}^k\beta_j x_{ij}\right)^2$$

$$\begin{align}
\left.\frac{\partial S}{\partial\beta_0}\right\vert_{b_0, b_1, \dots, b_k} &= -2 \sum_{i=1}^n\left(y_i - b_0 - \sum_{j=1}^k b_j x_{ij}\right) = 0\\
\left.\frac{\partial S}{\partial\beta_j}\right\vert_{b_0, b_1, \dots, b_k} &= -2 \sum_{i=1}^n\left(y_i - b_0 - \sum_{j=1}^k b_j x_{ij}\right)x_{ij} = 0, \quad j = 1, 2, \dots, k
\end{align}$$

. . .

$$\begin{align}
S(\bbeta) = \sum_{i=1}^n\epsilon_i^2 = \beps'\beps = ({\bf y} - {\bf X} \bbeta)'({\bf y} - \bf X \bbeta)
\end{align}$$



::: notes

$$\begin{align}
S(\bbeta) = \sum_{i=1}^n\epsilon_i^2 = \beps'\beps &= ({\bf y} - {\bf X} \bbeta)'({\bf y} - \bf X \bbeta) \\
&= {\bf y'}{\bf y} - \bbeta'{\bf X'}{\bf y} - {\bf y'}{\bf X} \bbeta + \bbeta' \bf X' \bf X \bbeta \\
&={\bf y}'{\bf y} - 2\bbeta'{\bf X}'{\bf y} + \bbeta' {\bf X}' {\bf X} \bbeta
\end{align}$$
:::



## Least Squares Estimation in Matrix Form
$$\begin{align}
S(\bbeta) = \sum_{i=1}^n\epsilon_i^2 = \beps'\beps &= ({\bf y} - {\bf X} \bbeta)'({\bf y} - \bf X \bbeta) \\
&= {\bf y'}{\bf y} - \bbeta'{\bf X'}{\bf y} - {\bf y'}{\bf X} \bbeta + \bbeta' \bf X' \bf X \bbeta
\end{align}$$

<!-- ::: fact -->

<!-- Let ${\bf t}$ and ${\bf a}$ be $n \times 1$ column vectors, and ${\bf A}_{n \times n}$ is a symmetric matrix. -->

<!-- - $\frac{\partial {\bf t}'{\bf a} }{\partial {\bf t}} = \frac{\partial {\bf a}'{\bf t} }{\partial {\bf t}} = {\bf a}$ -->

<!-- - $\frac{\partial {\bf t}'{\bf A} {\bf t}}{\partial {\bf t}} = 2{\bf A} {\bf t}$ -->

<!-- ::: -->

- Normal equations:
$\begin{align} \left.\frac{\partial S}{\partial \bbeta}\right \vert_{\bf b} = -2 {\bf X}' {\bf y} + 2 {\bf X}' {\bf X} {\bf b} = \boldsymbol{0} \end{align}$


::: notes
- To get the LSE in a matrix form, we first write the sum of squares using matrices.
:::

. . .

- LSE for $\bbeta$: <span style="color:blue"> $\begin{align} \boxed{{\bf b} = ({\bf X}' {\bf X}) ^{-1} {\bf X}' \bf y} \end{align}$ </span>





## Normal Equations
$(\bf X' \bf X){ \bf b} = {\bf X'} {\bf y}$

```{r}
knitr::include_graphics("./images/06-mlr-mat/normal_eqn.png")
```

::: notes
- Simple linear regression case.
:::


## [R Lab]{.pink} Design Matrix

:::: {.columns}
::: midi
::: {.column width="50%"}
```{r}
#| echo: !expr c(1)
#| class-output: my_classfull
X <- cbind(1, delivery_data[, c(2, 3)])
X <- as.matrix(X)
write.table(format(X, justify="right"), row.names=FALSE, col.names=FALSE, quote=FALSE)
```
:::

::: {.column width="50%"}
```{r}
#| echo: !expr c(1)
#| class-output: my_classfull
y <- as.matrix(delivery_data$time)
write.table(format(y, justify="right"), row.names=FALSE, col.names=FALSE, quote=FALSE)
```
:::
:::
::::


## [R Lab]{.pink} Regression Coefficients
:::: {.columns}
::: {.column width="50%"}
```{r}
#| out-width: 70%
knitr::include_graphics("./images/06-mlr-mat/xtx.png")
knitr::include_graphics("./images/06-mlr-mat/xty.png")
knitr::include_graphics("./images/06-mlr-mat/b.png")
```
<!-- # ```{r} -->
<!-- # #| out-width: 90% -->
<!-- # knitr::include_graphics("./images/06-mlr-mat/xty.png") -->
<!-- # ``` -->
<!-- # ```{r} -->
<!-- # #| out-width: 90% -->
<!-- # knitr::include_graphics("./images/06-mlr-mat/b.png") -->
<!-- # ``` -->
:::


::: {.column width="50%"}
${\bf b} = ({\bf X}' {\bf X}) ^{-1} \bf X' \bf y$
```{r}
#| echo: true
t(X) %*% X
t(X) %*% y
solve(t(X) %*% X) %*% t(X) %*% y
```
:::
::::


## Hat Matrix
- In SLR, $\hat{y}_i = h_{i1}y_1 + h_{i2}y_2 + \cdots + h_{in}y_n = {\bf h}_i'{\bf y}$ where $h_{ij} = \frac{1}{n} + \frac{(x_i-\overline{x})(x_j-\overline{x})}{S_{xx}}$ and ${\bf h}_i' = (h_{i1}, h_{i2}, \dots, h_{in})$. The **hat matrix** is ${\bf H} = (h_{ij})_{n \times n}$.

. . .

$$\hat{\bf y} = {\bf X} {\bf b} = {\bf X} ({\bf X}' {\bf X}) ^{-1} {\bf X'} {\bf y} = {\bf H} {\bf y}$$

- ${\bf H} = {\bf X} ({\bf X}' {\bf X}) ^{-1} \bf X'$

- The vector of residuals $e_i = y_i - \hat{y}_i$ is
$${\bf e} = {\bf y} - \hat{\bf y} ={ \bf y} - {\bf X} {\bf b} = {\bf y} -{\bf H} {\bf y} = ({\bf I} - {\bf H}) {\bf y}$$

. . .

::: alert
- Both $\bf H$ and $\bf I-H$ are *symmetric* and *idempotent*. They are **projection** matrices.
- $\bf H$ projects $\bf y$ to $\hat{\bf y}$ on the $p$-dimensional space spanned by columns of $\bf X$, or the column space of $\bf X$, $Col({\bf X}) = \{ {\bf Xv}: {\bf v} \in {\bf R}^p \}$.
- $\bf I - H$ projects $\bf y$ to $\bf e$ on the space **perpendicular** to $Col(\bf X)$, or $Col(\bf X)^{\bot}$.
:::



::: notes
- The vector of fitted values $\hat{y}_i$ corresponding to $y_i$ is
$$\hat{\bf y} = \bf X \bf b = \bf X (\bf X' \bf X) ^{-1} \bf X' \bf y = \bf H \bf y$$
- $\bf H$ plays an important role in regression analysis.
- And it will be shown up many times later in many topics of this course.
- OK. It's a little abstract. Let's look into the geometrical interpretation of least squares little by little.
:::

## Geometrical Interpretation of Least Squares

:::: {.columns}

::: {.column width="30%"}

- ${\bf y} \notin Col({\bf X})$
- $\hat{{\bf y}} = {\bf Xb} = {\bf H} {\bf y} \in Col({\bf X})$
- Minimize the distance of $\color{red}{A}$ to $Col(\bf X)$: Find the point in $Col(\bf X)$ that is closest to $A$.
:::

::: {.column width="70%"}
```{r}
#| fig-cap: "https://commons.wikimedia.org/wiki/File:OLS_geometric_interpretation.svg"
knitr::include_graphics("./images/06-mlr-mat/ols_geom.png")
```
:::
::::


::: notes
- (1) what is column space of $X$?
  + The col space of $X$ is the set or the collection of all linear combinations of columns of $X$. In other words, column space of $X$ is the space spanned by the columns of $X$.
  + Example
  + For visualization purpose, suppose $X$ has 2 columns, $x_1$ and $x_2$. The column space of $X$ will be the plane spanned by the 2 columns, shown in green color.
- (2) observation $y$.
  + Basically 99% of the time, $Y$ will not be in the column space of $X$.
  + $y_i = b_0 + b_1x_{i1} + ... +  b_kx_{ik} + e_i$
  + $y_i$ is not the linear combo of columns of $X$, and $y$ is not a point in Col(X).
  + So $y$ is point in the $n$-dimensional space, and not on the plane spanned by columns of $X$.
- (3) H (Let's see what the projection matrix $H$ is doing)
  + When we multiply $H$ by $y$, we are finding the projection of $y$ onto the Col(X).
  + The projection of $y$ on the Col(X) is actually the fitted value $\hat{y}$.
  + $\hat{y}_i = b_0 + b_1x_{i1} + ... +  b_kx_{ik}$
  + There are so many (actually infinitely many) linear combo of columns of $X$, why this particular vector is our $\hat{y}$?
  + The distance between $y$ and Col(X) is actually the shortest when we do the projection of $y$ onto the Col(X).
  + In other words the distance between $y$ and $\hat{y}$ is the minimal distance among all the distance between $y$ and any point in the Col(X).

<!-- - Any point in the column space of $\bf X$ (estimation space) is of the form $\bf X \bf \bbeta$. -->
<!-- - Minimizing the distance of point $A$ defined by $\bf y$ to the column space $(S(\bbeta) = \|\bf y - \bf X \bf \bbeta\|^2)$ requires finding the point in the space that is close to $A$.  -->
<!-- - $\bf y - \hat{\bf y} = \bf {y - X b} \perp Col(X)$ -->
<!-- - $\bf X'(y - Xb) = 0$. -->
:::


## Geometrical Interpretation of Least Squares

:::: {.columns}

::: {.column width="30%"}
- The distance is minimized when the point in the space is the foot of the line from $A$ **normal** to the space. This is point $C$.
$\small {\bf e} = ({\bf I} - {\bf H}) {\bf y} \perp Col({\bf X})$
- ${\bf X}'( {\bf y} - {\bf Xb}) = {\bf 0}$

::: alert
Searching for the LS solution $\bf b$ that minimizes $SS_{res}$ is the same as locating the point ${\bf Xb} \in Col({\bf X})$ that is as close to $\bf y$ as possible!
:::
:::


::: {.column width="70%"}
```{r}
#| fig-cap: "https://commons.wikimedia.org/wiki/File:OLS_geometric_interpretation.svg"
knitr::include_graphics("./images/06-mlr-mat/ols_geom.png")
```
:::
::::



::: notes
- (4) (I - H) Now let's see what (I - H) is doing.
  + The residual vector is $y - \hat{y}$. Geometrically, the residual vector is this dash line vector that is perpendicular to the column space of $X$. In other words,the residual vector is the normal vector of Col(X).
  + This is the result of projection. The distance between $y$ and the column space is minimized when the point in the space is the foot of the line from $A$ normal to the space. This is point $C$.
  + We know that e = (I - H)y. So (I - H) project $Y$ onto the space that is perpendicular to Col(X). And the vector in that space is the residual vector.
- (5) Calculus connects to Linear Algebra

:::

## [R Lab]{.pink} Verify Identity
:::: {.columns}

::: {.column width="50%"}
${\bf H} = {\bf X} ({\bf X}' {\bf X}) ^{-1} \bf X'$

```{r}
#| echo: true
H <- X %*% solve(t(X) %*% X) %*% t(X)
```

$\hat{\bf y} = \bf H \bf y$
```{r}
#| echo: true
fitted_y <- H %*% y
fitted_y[1:4]
delivery_lm$fitted.values[1:4]
```
:::

::: {.column width="50%"}

${\bf e} = ({\bf I} - {\bf H}) {\bf y}$

```{r}
#| echo: true
n <- length(y)
I <- diag(n)
res <- (I - H) %*% y
res[1:4]
delivery_lm$residuals[1:4]
## residual vector in the left null space of X
t(X) %*% res
```
:::
::::

## Multivariate Normal Distribution   {visibility="hidden"}



${\bf b} = ({\bf X}' {\bf X}) ^{-1} \bf X' \bf y$

$$\textbf{b} \sim N\left(\bbeta, \sigma^2 {( {\bf X'} {\bf X} )}^{-1} \right)$$
<!-- $$E(\textbf{b}) = E\left[ {( {\bf X'} {\bf X} )}^{-1} {\bf X}' {\bf y}\right] = \bbeta$$ -->
<!-- $$\cov(\textbf{b}) = \cov\left[{( {\bf X'} {\bf X} )}^{-1} {\bf X}' {\bf y} \right] = \sigma^2 {( {\bf X'} {\bf X} )}^{-1}$$ -->


::: alert
The standard error of $b_j$ is ${\sqrt{s^2C_{jj}}}$, where $C_{jj}$ is the diagonal element of $( {\bf X'} {\bf X} )^{-1}$ corresponding to $b_j$.
:::


::: notes
If ${\bf y} \sim N_n(\bsmu, {\bf \Sigma})$, and ${\bf Z = By + c}$ with a constant vector $\bf c$, then $${\bf Z} \sim N({\bf B\bsmu}, {\bf B \Sigma B}')$$.
$$\var(\textbf{b}) = E\left[(\textbf{b} - E(\textbf{b}))(\textbf{b}-E(\textbf{b}))'\right] = \var\left[\bf{(X'X)^{-1}X'y}\right] = \sigma^2 \bf (X'X)^{-1}$$

- The standard error of $b_j$ is ${\sqrt{\hat{\sigma}^2C_{jj}}}$, where $C_{jj}$ is the diagonal element of $({\bf X'X})^{-1}$ corresponding to $b_j$.
:::

# Hypothesis Testing
<h2> Tests on Subsets of Coefficients </h2>
<!-- <h2> Testing the General Linear Hypothesis </h2> -->

## Reduced Model vs. Full Model
- Overall test of significance: *all* predictors vs. Marginal $t$-test: *one single* predictor
- How to test **any subset** of predictors?

. . .

- **Full Model**: $y = \beta_0 + \beta_1x_1+\beta_2x_2 + \beta_3x_3 + \beta_4x_4 + \epsilon$
- $H_0: \beta_{2} = \beta_{4} = 0$

. . .

- **Reduced Model** (under $H_0$): $y = \beta_0 + \beta_1x_1 + \beta_3x_3 + \epsilon$
- Like to see if $x_2$ and $x_4$ can contribute significantly to the regression model when $x_1$ and $x_3$ are in the model.
  + If yes, $\beta_{2} \ne 0$ and/or $\beta_{4} \ne 0$. (Reject $H_0$)
  + Otherwise, $\beta_{2} = \beta_{4} = 0$. (Do not reject $H_0$)

. . .

::: alert
Given $x_1$ and $x_3$ in the model, we examine **how much extra $SS_R$ is increased ( $SS_{res}$ is reduced) if $x_2$ and $x_4$ are added to the model**.
:::


::: notes
- If much more extra $SS_R$ is increased after $x_2$ and $x_4$ are included in the model, meaning that lots of variation in $y$ that were treated as unexplained variation or variation that cannot be explained by the $x_1$ and $x_3$, now are absorbed or explained by $x_2$ and $x_4$.
- $x_2$ and $x_4$ provide some important and valuable information that $x_1$ and $x_3$ cannot provide, and that information helps us better predict response values and explain the variation of $Y$.
- So if we observe a significant increase in SS_R, it means that $x_2$ and $x_4$ are valuable and their coefficient is significantly non-zero. Therefore, we should keep the full model that includes all predictors, or reject $H_0$ because the reduced model is too simple, and lose lots of useful information for explaining variation in $Y$.
:::


## Extra Sum-of-sqaures
- Full Model: $\bf y = X\bbeta+\beps$

- Partition coefficient vector:

$$\bbeta_{p \times 1} = \left[
\begin{array}{c}
  \bbeta_1 \\
  \hline
  \bbeta_2
\end{array}
\right]$$

where $\bbeta_1$ is $(p-r) \times 1$ and $\bbeta_2$ is $r \times 1$

- Test $H_0: \bbeta_2 = {\bf 0}$, $H_1: \bbeta_2 \ne {\bf 0}$

- Example: <span style="color:blue"> $p=5$, $r=2$, $\bbeta_1 = (\beta_0, \beta_1, \beta_3)'$, $\bbeta_2 = (\beta_2, \beta_4)'$.</span>


::: notes
- For simplicity, we can partition the coefficient vector to beta_1 and beta_2, where beta_2 is the subset we'd like to test.
- $1 \le r \le k$
:::

. . .

$${\bf y} = {\bf X} \bbeta+\beps = {\bf X}_1\bbeta_1 + {\bf X}_2\bbeta_2 + \beps$$

+ $n \times (p-r)$ matrix ${\bf X}_1$: the columns of $\bf X$ associated with $\bbeta_1$

+ $n \times r$ matrix ${\bf X}_2$: the columns of $\bf X$ associated with $\bbeta_2$

## Extra Sum-of-sqaures
:::: {.columns}

::: {.column width="50%"}
**Full Model**: $\small \color{red}{{\bf y} = {\bf X}\bbeta+\beps = {\bf X}_1\bbeta_1 + {\bf X}_2\bbeta_2 + \beps}$

- ${\bf b} = ({\bf X}' {\bf X}) ^{-1} {\bf X}' {\bf y}$
<!-- - $SS_R(\bbeta) = {\bf b}{\bf X}' {\bf y}$ with $p$ dfs -->
:::


::: {.column width="50%"}
**Reduced Model**: $\small \color{red}{{\bf y} = {\bf X}_1\bbeta_1+\beps}$ $\color{red}{(\bbeta_2 = {\bf 0})}$

- ${\bf b}_1 = ({\bf X}_1' {\bf X}_1) ^{-1} {\bf X}_1' {\bf y}$
<!-- - $SS_R(\bbeta_1) = {\bf b}_1{\bf X}_1'{\bf y}$ with $p-r$ dfs -->
:::
::::

::: notes
- To find the contribution of the terms in $\bbeta_2$ to the regression, fit the model assuming that $H_0: \bf \bbeta_2 = 0$ is true.
<!-- - To find the contribution of the terms in $\bbeta_2$, fit the model assuming $H_0: \bf \bbeta_2 = 0$ is true. -->
:::

. . .

- The $SS_R$ due to $\bbeta_2$ given that $\bbeta_1$ is in the model is
$$\begin{align} SS_R(\bbeta_2|\bbeta_1) &= SS_R(\bbeta) - SS_R(\bbeta_1)\\ &= SS_R(\bbeta_1, \bbeta_2) - SS_R(\bbeta_1) \end{align}$$ with $r$ dfs.
- This is the **extra sum of squares** due to $\bbeta_2$.
- It measures the increase in the $SS_R$ that results from adding regressors in ${\bf X}_2$ to the model that already contains regressors in ${\bf X}_1$.


::: notes
- The regression sum of squares due to $\bbeta_2$ given that $\bbeta_1$ is in the model is
$$SS_R(\bbeta_2|\bbeta_1) = SS_R(\bbeta) - SS_R(\bbeta_1)$$ with $p - (p-r) = r$ dfs.
- This is **extra sum of squares** due to $\bbeta_2$.
- It measures the increase in the $SS_R$ that results from adding $x_{k-r+1}, x_{k-r+2}, \dots, x_k$ to the model that already contains $x_{1}, x_{2}, \dots, x_{k-r}$.
- To find the contribution of the terms in $\bbeta_2$ to the regression, fit the model assuming that $H_0: \bf \bbeta_2 = 0$ is true.
:::


## Partial $F$-test
- $F_{test} = \frac{SS_R(\bbeta_2|\bbeta_1)/r}{SS_{res}(\bbeta)/(n-p)} = \frac{MS_R(\bbeta_2|\bbeta_1)}{MS_{res}}$
- Under $H_0$ that $\bbeta_2 = \bf 0$, $F_{test} \sim F_{r, n-p}$. $(p = k+1)$.
- Reject $H_0$ if $F_{test} > F_{\alpha, r, n-p}$.

. . .

::: alert
Given the regressors of ${\bf X}_1$ are in the model,

- If the regressors of ${\bf X}_2$ contribute much, $SS_R(\bbeta_2|\bbeta_1)$ will be large.

- A large $SS_R(\bbeta_2|\bbeta_1)$ implies a large $F_{test}$.

- A large $F_{test}$ tends to reject $H_0$, and conclude that $\bbeta_2 \ne \bf 0$.

- $\bbeta_2 \ne \bf 0$ means the regressors of ${\bf X}_2$ provide additional explanatory and prediction power that ${\bf X}_1$ cannot provide.

:::


## Example: Delivery Data
- $H_0: \beta_2 = 0 \qquad H_1: \beta_2 \ne 0$
- Full model: $y = \beta_0 + \beta_1x_1+\beta_2x_2+\epsilon$

. . .

::: question
What is the reduced model?
:::

. . .

- Reduced model: $y = \beta_0 + \beta_1x_1 +\epsilon$

. . .

- $SS_R(\beta_2|\beta_1, \beta_0) = SS_R(\beta_2, \beta_1, \beta_0) -SS_R(\beta_1, \beta_0)$
- $F_{test} = \frac{SS_R(\beta_2|\beta_1, \beta_0)/1}{SS_{res}(\bbeta)/(25-3)}$
- Reject $H_0$ if $F_{test} > F_{\alpha, 1, 22}$


. . .

::: alert
When $r=1$, partial $F$-test is equivalent to marginal $t$-test.
:::


## [R Lab]{.pink} Extra Sum-of-sqaures Approach
```{r}
#| echo: true
full_lm <- delivery_lm
reduced_lm <- lm(time ~ cases, data = delivery_data)
anova(reduced_lm, full_lm)
```

```{r}
#| echo: true
summ_delivery$coefficients
```

## General Linear Hypothesis {visibility="hidden"}
- Previous testing procedures are special cases of the **general linear hypothesis testing**.
- $H_0: {\bf T\bbeta = c}$, where ${\bf T}$ is an $m \times p$ constant matrix and $\bf c$ is a $m \times 1$ constant vector.
- There are $m$ linear hypotheses to be tested. [^2]
- **Full Model**: $y = \beta_0 + \beta_1x_1+\beta_2x_2 + \beta_3x_3 + \epsilon$
- The model under $H_0$ is the reduced model or **restricted model**.
$$H_0: \begin{align}&\beta_1 + 2 \beta_2 = 0 \\
&\beta_3 = 2 \end{align}$$

$$\begin{bmatrix}
0 & 1 & 2 & 0  \\
0 & 0 & 0 & 1
\end{bmatrix}\begin{bmatrix}
\beta_0 \\
\beta_1 \\
\beta_2 \\
\beta_3
\end{bmatrix}=
\begin{bmatrix}
0 \\
2
\end{bmatrix} \iff {\bf T}_{2 \times 4} \bbeta_{4 \times 1} = {\bf c}_{2 \times 1}$$


[^2]: Assume only $r$ of the $m$ equations are independent.



## General $F$-test {visibility="hidden"}

<!-- - $F_{test} = \frac{(\bf Tb - c)'\left[T(X'X)^{-1}T'\right]^{-1}(\bf Tb - c)}{rMS_{res}}$ where $r$ is the rank of $\bf T$ -->
- Reject $H_0$ if $F_{test} > F_{\alpha, r, n-p}$, where $r$ is the rank of $\bf T$. $(F_{test}$ is shown in Eq. (3.43) in LRA)

- Back to Delivery example. Suppose we test $H_0: \beta_{1} = \beta_{2} = 0 \quad H_1: \beta_j \ne 0 \text{ for at least one } j$

. . .

::: question
What are $\bf T$, $\bbeta$ and $\bf c$?
:::

. . .

:::: {.columns}

::: {.column width="50%"}
$$H_0: \begin{align} &\beta_1 = 0 \\
&\beta_2 = 0 \end{align}$$

$$\begin{bmatrix}
0 & 1 & 0  \\
0 & 0 & 1
\end{bmatrix}\begin{bmatrix}
\beta_0 \\
\beta_1 \\
\beta_2
\end{bmatrix}=
\begin{bmatrix}
0 \\
0
\end{bmatrix}$$
:::

::: {.column width="50%"}

```{r}
#| echo: true
TT <- matrix(0, 2, 3)
TT[1, 2] <- 1; TT[2, 3] <- 1
TT
c0 <- c(0, 0)
```
:::
::::


## [R Lab]{.pink} General Linear Hypothesis {visibility="hidden"}
```{r}
#| echo: true
#| class-output: my_class600
library(car)  ## Companion to Applied Regression
car::lht(model = delivery_lm,
         hypothesis.matrix = TT,
         rhs = c0)
```


- This is (of course) exactly the same as the test for significance!


## Standardized Regression Coefficients
::: question
$\hat{y} = 5 + x_1 + 1000x_2$. Can we say the effect of $x_2$ on $y$ is 1000 times larger than the effect of $x_1$?
:::

. . .

- Nope! If $x_1$ is measured in litres and $x_2$ in millilitres, although $b_2$ is 1000 times larger than $b_1$, *both effects on $\hat{y}$ are identical.*

. . .

- The units of $\beta_j$ are $\frac{\text{units of } y}{\text{units of } x_j}$
  + <span style="color:blue"> $\beta_2$: delivery time (min) $(y)$ per distance (ft) walked by the driver $(x_2)$. </span>
- It is helpful to work with **dimensionless or standardized** coefficients.
  + comparison
  + get rid of round-off errors in  $( {\bf X'} {\bf X} )^{-1}$.

<!-- - Two methods of scaling: -->
<!--   + Unit normal scaling -->
<!--   + Unit length scaling -->

::: notes
- interpretation
- It is difficult to compare coefficients because the magnitude of $b_j$ reflects the units of measurement of $x_j$.
:::

. . .

::: question
In Intro Stats, how do we standardize a variable?
:::


## Unit Normal Scaling

- $z_{ij} = \frac{x_{ij}-\overline{x}_j}{s_j}, \, i = 1, \dots, n, \, j = 1, \dots, k$, where $s_j$ is the sample SD of $x_j$.
- $y^*_{i} = \frac{y_{i}-\overline{y}}{s_y}, \, i = 1, \dots, n$, where $s_y$ is the sample SD of $y$.
- The scaled predictors and response have mean 0 and variance 1.
- The new model:
$$y_i^* = \alpha_1z_{i1} + \alpha_2z_{i2} + \cdots + \alpha_kz_{ik} + \epsilon_i$$

::: question
Why no intercept term $\alpha_0$?
:::

. . .

- The least-squares estimator for $\balpha$:
$${\bf a} = {\bf (Z'Z)}^{-1} {\bf Z'y}^*$$


## Unit Length Scaling  {visibility="hidden"}
- $w_{ij} = \frac{x_{ij}-\overline{x}_j}{S_{jj}^{1/2}}, \, i = 1, \dots, n, \, j = 1, \dots, k$, where $S_{jj} = \sum_{i=1}^n(x_{ij}-\overline{x}_j)^2$ is the (corrected) sum of squares for $x_j$.
- $y^0_{i} = \frac{y_{i}-\overline{y}}{SS_T^{1/2}}, \, i = 1, \dots, n$.
- The new regressor and response have mean 0 and length 1 $(\sqrt{\sum_{i=1}^nw_{ij}^2} = 1)$.
- The new model:
$$y_i^0 = \alpha_1w_{i1} + \alpha_2w_{i2} + \cdots + \alpha_kw_{ik} + \epsilon_i$$
- The least-squares estimator for $\bsalpha$:
$${\bf a} = {\bf (W'W)}^{-1} {\bf W'y}^0$$

::: notes
- $S_{jj} = \sum_{i=1}^n(x_{ij}-\overline{x}_j)^2$ is the (corrected) sum of squares for $x_j$.
:::


## Comparison  {visibility="hidden"}
- ${\bf W'W}$ is the correlation matrix formed by $x_1, \dots, x_k$.
- $\bf W'y^0$ is the correlation between $x_j$ and $y$.
- ${\bf Z'Z} = (n-1) {\bf W'W}$
- ${\bf a} = {\bf (Z'Z)}^{-1} {\bf Z'y}^* =  {\bf (W'W)}^{-1} {\bf W'y}^0$
$$b_j = a_j \left( \frac{SS_T}{S_{jj}}\right)^{1/2}, \quad j = 1, 2, \dots, k$$
$$b_0 = \overline{y} - \sum_{j=1}^kb_j\overline{x}_j$$


## [R Lab]{.pink} Standardized Coefficients
```{r}
#| echo: true
## unit normal scaling
scale_data <- scale(delivery_data, center = TRUE, scale = TRUE)  ## becomes a matrix
apply(scale_data, 2, mean)
apply(scale_data, 2, sd)
```


. . .

```{r}
#| echo: true

## No-intercept
scale_lm <- lm(time ~ cases + distance - 1, data = as.data.frame(scale_data)) 
scale_lm$coefficients

## With intercept
scale_lm_0 <- lm(time ~ cases + distance, data = as.data.frame(scale_data))  
scale_lm_0$coefficients
```


::: notes
```{r, eval=FALSE}
## unit length scaling (your homework)
## sum of squares
y <- delivery_data[, 1]
sst <- sum((y - mean(y))^2)
x1 <- delivery_data[, 2]
x2 <- delivery_data[, 3]
s11 <- sum((x1 - mean(x1))^2)
s22 <- sum((x2 - mean(x2))^2)
s12 <- sum((x1 - mean(x1))*(x2 - mean(x2)))
s1y <- sum((x1 - mean(x1))*(y - mean(y)))
s2y <- sum((x2 - mean(x2))*(y - mean(y)))
r12 <- cor(x1, x2)
s12/sqrt(s11*s22)
r1y <- cor(x1, y)
s1y/sqrt(s11*sst)
r2y <- cor(x2, y)
s2y/sqrt(s22*sst)
WtW <- cor(delivery_data[, 2:3])
y0 <- (y - mean(y))/sqrt(sst)
Wty <- cor(delivery_data[, 2:3])
Wty0 <- cor(delivery_data[, 2:3], delivery_data[, 1])
solve(WtW) %*% Wty0
## b1
delivery_lm$coefficients[2]
scale_lm$coefficients[1] * sqrt(sst / s11)
## b2
delivery_lm$coefficients[3]
scale_lm$coefficients[2] * sqrt(sst / s22)
## b0
delivery_lm$coefficients[1]
x_mean <- apply(delivery_data[, 2:3], 2, mean)
mean(y) - sum(delivery_lm$coefficients[2:3] * x_mean)
```
:::



<!-- ## Other Topics -->
<!-- - Justification of distributional properties -->
<!-- - Simultaneous confidence intervals (region) -->
<!-- - Extrapolation problem -->
<!-- - Maximum likelihood estimation -->
<!-- - Colliearity -->

