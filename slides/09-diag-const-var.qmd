---
title: "Regression Diagnostics - Constant Variance `r emo::ji('book')`"
subtitle: "MATH 4780 / MSSC 5780 Regression Analysis"
author: "Dr. Cheng-Han Yu <br> Department of Mathematical and Statistical Sciences <br> Marquette University"
# date: "`r format(Sys.time(), '%B %d %Y')`"
# macros: _macros.tex # import a list of TeX/LaTeX definitions
format: 
  revealjs:
    # code-line-numbers: false
    #     - "macros.tex"
    html-math-method:
      method: mathjax
      url: "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"
    # include-in-header:
    highlight-style: arrow
    code-block-bg: true
    self-contained: false
    slide-number: c/t    
    incremental: false
    width: 1800
    height: 1000
    margin: 0.05
    logo: "https://upload.wikimedia.org/wikipedia/commons/e/e8/Marquette_Golden_Eagles_logo.svg"
    footer: "[math4780-f23.github.io/website](https://math4780-f23.github.io/website/)"
    theme: ["simple", "slides.scss"]
    multiplex: true
    code-link: true
    fig-cap-location: bottom
    fig-align: center
    transition: none ## fade slide convex concave zoom
    title-slide-attributes:
      data-background-color: "#447099"
      # data-background-image: images/paper-texture.jpg
      # data-background-size: cover
      # data-background-color: "#698ED5"
editor: source
execute:
  freeze: true
  echo: false
  purl: true
---

#  {visibility="hidden"}

\def\bx{\mathbf{x}}
\def\bg{\mathbf{g}}
\def\bw{\mathbf{w}}
\def\balpha{\boldsymbol \alpha}
\def\bbeta{\boldsymbol \beta}
\def\beps{\boldsymbol \epsilon}
\def\bX{\mathbf{X}}
\def\by{\mathbf{y}}
\def\bH{\mathbf{H}}
\def\bI{\mathbf{I}}
\def\bS{\mathbf{S}}
\def\bW{\mathbf{W}}
\def\T{\text{T}}
\def\cov{\mathrm{Cov}}
\def\cor{\mathrm{Corr}}
\def\var{\mathrm{Var}}
\def\E{\mathrm{E}}
\def\bmu{\boldsymbol \mu}
\DeclareMathOperator*{\argmin}{arg\,min}
\def\Trace{\text{Trace}}


```{r}
#| label: setup
#| include: false
#| eval: true
library(countdown)
library(emo)
library(knitr)
library(gt)
library(gtExtras)
library(ggplot2)
library(tidyverse)
library(tidymodels)
library(MASS)
library(car)
knitr::opts_chunk$set(
    fig.asp = 0.6,
    fig.align = "center",
    out.width = "70%",
    fig.retina = 10,
    fig.path = "./images/09-diag-const-var/",
    message = FALSE,
    global.par = TRUE
)
options(
  htmltools.dir.version = FALSE,
  dplyr.print_min = 6, 
  dplyr.print_max = 6,
  tibble.width = 80,
  width = 80,
  digits = 2
  )
hook_output <- knitr::knit_hooks$get("output")
knitr::knit_hooks$set(output = function(x, options) {
  lines <- options$output.lines
  if (is.null(lines)) {
    return(hook_output(x, options))  # pass to default hook
  }
  x <- unlist(strsplit(x, "\n"))
  more <- "..."
  if (length(lines)==1) {        # first n lines
    if (length(x) > lines) {
      # truncate the output, but add ....
      x <- c(head(x, lines), more)
    }
  } else {
    x <- c(more, x[lines], more)
  }
  # paste these lines together
  x <- paste(c(x, ""), collapse = "\n")
  hook_output(x, options)
})
par(cex = 4)
```


```{r}
#| echo: false
CIA <- read.table("./data/CIA.txt", header = TRUE)
ciafit <- lm(infant ~ gdp + health + gini, data = CIA)
r_stud <- rstudent(ciafit)
logciafit <- lm(log(infant) ~ gdp + health + gini, data = CIA)
```




# Model Adequacy Checking and Correction

<h2> Non-normality</h2>
<h2> <span style="color:red"> Non-constant Error Variance </span></h2>
<h2> Non-linearity and Lack of Fit </h2>

::: notes
- This week we are going to learn some methods to correct model inadequacy.
- We can either transform our data, our use a more general method, called Generalized Least Squares or Weighted Least Squares.
- $\epsilon_i \stackrel{iid}{\sim} N(0, \sigma^2)$: **mean 0**, **constant variance**, **normally distributed**, and **uncorrelated**.
- $E(\epsilon_i) =0$ implies the function form of the model (**linearity**) is correct.
:::

## Nonconstant Error Variance

Without constant variance, 

- LSEs are still unbiased if linearity and independence of $x$s and errors hold.

- Inference results are not convincing: **distorted $p$-values** and **incorrect coverage** of confidence interval.

- The $b_j$s will have **larger** standard errors (no minimum-variance).

::: question
Is it harder or easier to reject $H_0: \beta_j = 0$ when the standard error of $b_j$ is larger?
:::

- For these consequences to occur, nonconstant error variance must be severe.

## Detecting Nonconstant Error Variance
- Common pattern: the conditional variation of $y$ to increase with the level of $y$.

- It seems natural to plot $e_i$ against $y$

  + $e_i$ have **unequal** variances $(\var(e_i) = \sigma^2(1-h_{ii}))$ even when the errors $\epsilon_i$ have equal variances $\var(\epsilon_i) = \sigma^2$.
  + $e_i = y_i - \hat{y}_i$ are correlated with $y$
  
- Use R-student residual plot $t_i$ vs. $\hat{y}_i$ because R-student residuals have constant variance and they are uncorrelated with $\hat{y}_i$.


::: notes
$r_{ey} = \sqrt{1-R^2}$
https://stats.stackexchange.com/questions/5235/what-is-the-expected-correlation-between-residual-and-the-dependent-variable
:::


## Residual Plots

```{r}
#| eval: false
par(mar = c(3, 3, 2, 1), mgp = c(2, 0.8, 0), las = 1, mfrow = c(2, 3))
# par(mfrow = c(2, 3))
df <- tibble(
    fake_resid = rnorm(500, mean = 0, sd = 30),
    fake_predicted = runif(500, min = 0, max = 200)
)
plot(x = df$fake_predicted, y = df$fake_resid, pch = 19, xlab = "fitted y",
     ylab = "residuals", main = "What we want", cex = 0.3)
abline(h = 0, col = "red", lwd = 2)
set.seed(12346)
df <- tibble(
  fake_resid = c(rnorm(100, mean = 0, sd = 1), 
                 rnorm(100, mean = 0, sd = 15), 
                 rnorm(100, mean = 0, sd = 25), 
                 rnorm(100, mean = 0, sd = 20), 
                 rnorm(100, mean = 0, sd = 25), 
                 rnorm(100, mean = 0, sd = 50), 
                 rnorm(100, mean = 0, sd = 35), 
                 rnorm(100, mean = 0, sd = 40),
                 rnorm(200, mean = 0, sd = 80)),
  fake_predicted = seq(0.2, 200, 0.2)
)
plot(x = df$fake_predicted, y = df$fake_resid, pch = 19, xlab = "fitted y",
     ylab = "residuals", main = "Fanning/funnel", cex = 0.3)
abline(h = 0, col = "red", lwd = 2)
df <- tibble(
  fake_predicted = seq(0.2, 200, 0.2),
  fake_resid = c(
    rnorm(500, mean = -20, sd = 10),
    rnorm(500, mean = 10, sd = 10)
  )
)
plot(x = df$fake_predicted, y = df$fake_resid, pch = 19, xlab = "fitted y",
     ylab = "residuals", main = "Grouping", cex = 0.3)
abline(h = 0, col = "red", lwd = 2)
df <- tibble(
  fake_predicted = seq(0.2, 200, 0.2),
  fake_resid = fake_predicted + rnorm(1000, mean = 0, sd = 50)
)
plot(x = df$fake_predicted, y = df$fake_resid, pch = 19, xlab = "fitted y",
     ylab = "residuals", main = "Correlated e and y_hat", cex = 0.3)
abline(h = 0, col = "red", lwd = 2)
df <- tibble(
  fake_predicted = seq(-100, 100, 0.4),
  fake_resid = -5*fake_predicted^2 - 3*fake_predicted + 20000 + rnorm(501, mean = 0, sd = 10000)
)
plot(x = df$fake_predicted, y = df$fake_resid, pch = 19, xlab = "fitted y",
     ylab = "residuals", main = "Nonlinear", cex = 0.3, yaxt = "n")
axis(2, cex.axis = .6)
abline(h = 0, col = "red", lwd = 2)
```


:::: {.columns}

::: {.column width="50%"}
```{r}
#| out-width: 100%
set.seed(12345)
x <- runif(500, 1, 10)
y1 <- x + rnorm(500, 0, 1)
y2 <- x + rnorm(500, 0, x)
m1 <- lm(y1 ~ x)
m2 <- lm(y2 ~ x)
par(mar = c(3, 3.5, 2, 0), mgp = c(2, 0.8, 0), las = 1)
car::scatterplot(fitted(m1), rstudent(m1), 
                 smooth = list(span = 2/3, lwd.smooth = 3, lwd.spread = 3,
                               style = "lines"),
                 regLine = FALSE, axes = TRUE, grid=FALSE,
                 boxplots = FALSE, col = 1, ellipse=FALSE, cex.lab = 2,
                 main = "Constant variance", reset.par = FALSE, cex.main = 2,
                 xlab = "Fitted Values", ylab = "R-Student Residuals")
abline(a = 0, b = 0, col = 2, lwd = 2)
```
:::

::: {.column width="50%"}
```{r}
#| out-width: 100%
par(mar = c(3, 3.5, 2, 0), mgp = c(2, 0.8, 0), las = 1)
car::scatterplot(fitted(m2), rstudent(m2), 
                 smooth = list(span = 0.5, lwd.smooth = 3, lwd.spread = 3,
                               style = "lines"),cex.lab = 2,
                 regLine = FALSE, axes = TRUE, reset.par = FALSE, cex.main = 2,
                 boxplots = FALSE, col = 1, main = "Nonconstant variance",
                 xlab = "Fitted Values", ylab = "R-Student Residuals")
abline(a = 0, b = 0, col = 2, lwd = 2)
```
:::
::::

- If the spread of the residuals **increases** with the level of the fitted values, we might be able to correct the problem by transforming $y$ **down** the ladder of power and roots.


## Variance Stablizing Transformation
- Constant variance assumption is often violated when the response $y$ follows a distribution whose *variance is functionally related to mean*.
<!-- - The assumption: ${\bf y} \sim N\left({\bf X\bsbeta}, \sigma^2{\bf I}\right)$. -->

::: question
Can you provide a distribution whose variance is functionally related to its mean?
:::

. . .

Relationship of $\sigma^2$ to $E(y)$ | Transformation               
-------------------------------------|------------------------------ 
$\sigma^2 \propto$ constant          | $y' = y$ (no transformation)  
$\sigma^2 \propto E(y)$              | $y' = \sqrt{y}$ (square root; Poisson)
$\sigma^2 \propto E(y)[1-E(y)]$      | $y' = \sin^{-1}(\sqrt{y})$ (arcsin; binomial proportions $0 \le y_i \le 1$)
$\sigma^2 \propto [E(y)]^2$          | $y' = \ln(y)$ (natural log)
$\sigma^2 \propto [E(y)]^3$          | $y' = y^{-1/2}$ (reciprocal square root) 
$\sigma^2 \propto [E(y)]^4$          | $y' = y^{-1}$ (reciprocal)

::: notes
- The strength of a transformation depends on the amount of curvature that it induces
- a mild transformation applied over a relatively narrow range of values (e.g., y max / y min < 2, 3) has little effect. On the other hand, a strong transformation over a wide range of values will have a dramatic effect on the analysis.
:::



## [R Lab]{.pink} Nonconstant Variance (CIA Example)

:::: {.columns}

::: {.column width="50%"}
```{r}
#| out-width: 100%

par(mar = c(3, 3.5, 2, 0), mgp = c(2, 0.8, 0), las = 1)
scatterplot(fitted(ciafit), rstudent(ciafit), 
            smooth = list(span = 2/3, lwd.smooth = 3, lwd.spread = 4), 
            regLine = FALSE, boxplots = FALSE, main = "infant as y",
            xlab = "Fitted Values", ylab = "R-Student Residuals",
            cex.lab = 2, cex.main = 2)
abline(a = 0, b = 0, col = 2, lwd = 2)
```

- The residuals increase with fitted values.

- Nonlinear pattern.

- Nonsense negative fitted values.
:::




::: {.column width="50%"}

::: fragment
```{r}
#| out-width: 100%
# infant mortality log-transformed
par(mar = c(3, 3.5, 2, 0), mgp = c(2, 0.8, 0), las = 1)
scatterplot(fitted(logciafit), rstudent(logciafit), 
            smooth = list(span = 2/3, lwd.smooth = 3, lwd.spread = 3), 
            regLine = FALSE, boxplots = FALSE, main = "ln(infant) as y",
            xlab = "Fitted Values", ylab = "Studentized Residuals",
            cex.lab = 2, cex.main = 2)
abline(a = 0, b = 0, col = 2, lwd = 2)
```

- Variation around the central smooth seems approximately constant.

- Nonlinearity is still apparent. (Linearity and lack of fit next week)

:::
:::

::::


::: notes
- The nonlinear pattern suggests there are serious problems of the linear model.

- The residuals increases with fitted values.

- Several nonsense negative fitted values.
:::

::: notes
residual plot can reveal nonlinearity, but can;t determine where the problem lies and how to correct it.
:::



## Tukey's Spread-Level Plot

- Plot log of the absolute R-Student residuals vs. log of the (positive) fitted values.
- The fitted line slope $b$ suggests a variance-stablizing power transformation $\lambda = 1 - b$ for $y$.


:::: {.columns}

::: midi
::: {.column width="50%"}

```{r}
#| out-width: 100%
#| echo: !expr c(-1)
#| code-line-numbers: false
par(mar = c(3, 3, 2, 0), mgp = c(2, 0.8, 0))
car::spreadLevelPlot(ciafit, smooth = FALSE)
```
:::

::: {.column width="50%"}
```{r}
#| out-width: 100%
#| code-line-numbers: false
#| echo: !expr c(-1)
par(mar = c(3, 3, 2, 0), mgp = c(2, 0.8, 0))
spreadLevelPlot(logciafit, smooth = FALSE)
```
:::
:::
::::


::: notes
- Plot log of the absolute R-Student residuals vs. log of the (positive) fitted values

- Fit by robust regression to generate fitted lines.

- The slope $b$ of the fitted line suggests a variance-stablizing power transformation $\lambda = 1 - b$ for $y$.
:::



## Weighted Least Squares (WLS)
- Suppose that the other assumptions hold but that the error variances differ: $\epsilon_i \stackrel{indep}{\sim} N(0, \sigma_i^2)$

- The resulting model isn't estimable because we have more parameters than data points.

- If we **know** the pattern of unequal error variances, so that $\sigma_i^2 = \sigma^2/w_i$, for **known** values $w_i$, we can compute the coefficient estimates that minimize the **Weighted Residual Sum of Squares**.

- In simple linear regression, 
$$S(\beta_0, \beta_1) = \sum_{i=1}^nw_i\epsilon_i^2 = \sum_{i=1}^n w_i \left(y_i - \beta_0 - \beta_1x_i \right)^2$$


::: alert
Observations with large variances will have smaller weights.
:::


::: notes
$$(b_0, b_1, \dots, b_k) = \underset{{\beta_0, \beta_1, \dots, \beta_k}}{\mathrm{arg \, min}}  S(\beta_0, \beta_1, \dots, \beta_k)$$

$$S(\beta_0, \beta_1, \dots, \beta_k) = \sum_{i=1}^nw_i\epsilon_i^2 = \sum_{i=1}^nw_i\left(y_i - \beta_0 - \sum_{j=1}^k\beta_j x_{ij}\right)^2$$
:::


## Weighted Least Squares (WLS)  {visibility="hidden"}

- The resulting least squares normal equations:
$$\begin{align} \beta_0\sum_{i=1}^n w_i+\beta_1\sum_{i=1}^n w_ix_i &= \sum_{i=1}^n w_iy_i \\ \beta_0 \sum_{i=1}^n w_ix_i + \beta_1 \sum_{i=1}^n w_ix_i^2 &= \sum_{i=1}^n w_ix_iy_i \end{align}$$ which can be written as $${\bf (X'WX)\bbeta = X'Wy}$$



## Methods of Choosing Weights

- Prior knowledge or information

- Residual analysis
  + $\var(\epsilon_i) \propto x_{i}$, suggest $w_i = 1/x_{i}$
  + When $y_i$ is an *average* of $n_i$ observations at $x_i$, $\var(y_i) =\sigma^2 / n_{i}$ and suggest $w_i = n_i$
  
- Chosen inversely proportional to variances of measurement error

<!-- - Guess at the weights and iteratively estimate them. -->



::: notes
.alert[
If we have no idea of $\bf W$, consider **feasible GLS** estimator $\hat{\bsbeta}_{FGLS} = {\bf(X'\hat{W}X)^{-1}X'\hat{W}y}$, where $\bf \hat{W}$ is an estimate of $\bf W$ from some method. 
]
- The weights, $w_i$, or in general $\bf V$ must be *known.*
- Prior knowledge or information
- Residual analysis
  + $\var(\epsilon_i) \propto x_{i}$, suggest $w_i = 1/x_{i}$
  + When $y_i$ is an *average* of $n_i$ observations at $x_i$, $\var(y_i) =\sigma^2 / n_{i}$ and suggest $w_i = n_i$
- Chosen inversely proportional to variances of measurement error
- Guess at the weights and iteratively estimate them.
:::


## [R Lab]{.pink} Example 5.5 Food Sales in LRA
- The average monthly revenue vs. annual advertising
expenses for 30 restaurants.

```{r}
#| code-fold: true
#| echo: true
x <- c(3000, 3150, 3085, 5225, 5350, 6090, 8925, 9015, 8885, 8950, 9000, 11345,
       12275, 12400, 12525, 12310, 13700, 15000, 15175, 14995, 15050, 15200,
       15150, 16800, 16500, 17830, 19500, 19200, 19000, 19350)
y <- c(81464, 72661, 72344, 90743, 98588, 96507, 126574, 114133, 115814, 123181,
       131434, 140564, 151352, 146926, 130963, 144630, 147041, 179021, 166200, 180732,
       178187, 185304, 155931, 172579, 188851, 192424, 203112, 192482, 218715, 214317)
group <- c(1, 1, 1, 2, 2, 3, 4, 4, 4, 4, 4, 5, 5, 5, 5, 5, 6, 7, 7, 7, 7, 7, 7, 
           8, 8, 9, 10, 10, 10, 10)
ex5_5 <- data.frame("expense" = x, "sales" = y, "group" = group)
par(mar = c(3.3, 3.3, 0, 0), mgp = c(2, 1, 0))
plot(ex5_5$expense, ex5_5$sales, xlab = "Advertising expense", 
     ylab = "Food sales", col = group, pch = 16, cex = 2, cex.lab = 1.5)
```




::: notes
- The average monthly income from food sales vs. annual advertising
expenses for 30 restaurants.
- Management is interested in the relationship between these variables.
<!-- - Management is interested in the relationship between these variables. -->
:::


## [R Lab]{.pink} Non-constant Variance
- $\hat{y} = 49443.384 + 8.048x$

```{r}
lm_fit <- lm(y~x)
par(mar = c(3.3, 3.3, 0, 0), mgp = c(2, 1, 0), las = 1)
plot(lm_fit$fitted.values, rstudent(lm_fit), cex = 2, cex.lab = 1.5,
     ylab = "R-Student residual", xlab = "fitted y", pch = 16, col = group)
abline(h = 0, lty = 2, col = "red")
```


## [R Lab]{.pink}  Determine Weights
- There are several sets of $x$ values that are *"near neighbors"*.

```{r}
#| echo: true
ex5_5$expense
ex5_5$group
```

- Assume that these near neighbors are close enough to be considered repeat points.
- Use the variance of the responses at those repeat points to investigate how $\var(y)$ changes with $x.$


## [R Lab]{.pink}  Determine Weights

- The empirical variance of $y$, $s_y^2$, increases approximately linearly with $\bar{x}$.

```{r}
x_bar <- rep(0, 10)
s2_y <- rep(0, 10)
for (i in 1:10) {
  x_bar[i] <- mean(ex5_5$expense[ex5_5$group == i])
  s2_y[i] <- var(ex5_5$sales[ex5_5$group == i])
}
x_bar <- x_bar[!is.na(s2_y)]
s2_y <- s2_y[!is.na(s2_y)]
# idx <- ex5_5$group == 3 | ex5_5$group == 6 | ex5_5$group == 9
# s2_y[is.na(s2_y)] <- ex5_5$sales[idx]
par(mar = c(3, 3, 0, 0), mgp = c(2, 1, 0))
plot(x_bar, s2_y, cex.axis = 1, pch = 16, yaxt = 'n', ylab = "Var(y)",
     col = (1:10)[-c(3, 6, 9)], cex = 2, cex.lab = 1.5)
```


## [R Lab]{.pink}  Determine Weights
- $\hat{s}^2_y = -9226002 + 7781.6\bar{x}$
- Substituting each $x_i$ value into this equation will give an estimate of the variance of the corresponding observation $y_i$, $\hat{s}^2_i$
- The inverse of these  $\hat{s}^2_i$ will be reasonable estimates of the weights $w_i$.

```{r}
#| echo: true
s2_fit <- lm(s2_y~x_bar)
(coef <- s2_fit$coef)
s2_i <- coef[1] + coef[2] * ex5_5$expense
(wt <- 1 / s2_i)
```


## [R Lab]{.pink} WLS fit
```{r}
#| echo: true
#| output.lines: !expr c(5:7)
(wls_fit <- lm(ex5_5$sales ~ ex5_5$expense, 
               weights = wt))
```

- $\hat{y} = 51024.8 + 7.918x$
- We must examine the residuals to determine if using WLS has improved the fit. 
- Plot the **weighted** residuals $w_i^{1/2}(y_i - \hat{y}_i^W)$, where $\hat{y}_i^W$ comes form the WLS fit, against $w_i^{1/2}\hat{y}_i^W$



::: notes
.alert[
The WLS residuals and the WLS fitted values when the model assumes $\var(\beps) = \sigma^2 {\bf W}^{-1}$ should behave as the OLS residuals and OLS fitted values when the model assumes $\var(\beps) = \sigma^2 {\bf I}$.
:::

## [R Lab]{.pink} WLS Residual Plot
- The residual plot has no significant fanning.

```{r}
# library(MASS)
# r_stud <- studres(wls_fit)
# plot(sqrt(wls_fit$weights) * wls_fit$fitted.values, 
#      sqrt(wls_fit$weights) * r_stud, ylim = c(-3, 2),
#      ylab = "(weighted) residual", xlab = "(weighted) fitted y", pch = 16)

# plot(sqrt(wls_fit$weights) * wls_fit$fitted.values,
#      sqrt(wls_fit$weights) * rstudent(wls_fit),
#      ylab = "(weighted) residual", xlab = "(weighted) fitted y", pch = 16)

par(mar = c(3, 3, 0, 0), mgp = c(2, 1, 0))
plot(sqrt(wls_fit$weights) * wls_fit$fitted.values,
     sqrt(wls_fit$weights) * wls_fit$residuals, ylim = c(-3, 2), cex = 2, cex.lab = 1.4,
     ylab = "(weighted) residual", xlab = "(weighted) fitted y", pch = 16)
abline(h = 0, lty = 2, col = "red")
```

<!-- ## Other Methods for Dealing with Nonconstant Variance {visibility="hidden"} -->

<!-- - Testing (Breusch and Pagan (1979)) -->

<!-- ```{r, eval=FALSE} -->
<!-- car::ncvTest(ciafit) -->
<!-- ``` -->


<!-- Nonconstant variance results in unreliable inference results: -->

<!-- - Robust coefficient standard errors (Huber (1967), White (1980), Long and Ervin (2000)) -->

<!-- ```{r, eval=FALSE} -->
<!-- car::hccm(ciafit)  ## Heteroscedasticity-Corrected Covariance Matrices -->
<!-- ``` -->

<!-- - Bootstrapping (Efron and Tibshirani (1993)) -->

<!-- ```{r, eval=FALSE} -->
<!-- car::Boot(ciafit) -->
<!-- ``` -->
