[
  {
    "objectID": "course-links.html",
    "href": "course-links.html",
    "title": "Useful links",
    "section": "",
    "text": "RStudio containers\nüîó on Duke Container Manager\n\n\nCourse GitHub organization\nüîó on GitHub\n\n\nDiscussion forum\nüîó on Sakai\n\n\nLecture streaming and recordings\nüîó on Panopto\n\n\nGradebook\nüîó on Sakai\n\n\nVirtual meetings\nüîó on Sakai"
  },
  {
    "objectID": "LICENSE.html#creative-commons-attribution-sharealike-4.0-international-public-license",
    "href": "LICENSE.html#creative-commons-attribution-sharealike-4.0-international-public-license",
    "title": "MATH 4780 - Fall 2023",
    "section": "Creative Commons Attribution-ShareAlike 4.0 International Public License",
    "text": "Creative Commons Attribution-ShareAlike 4.0 International Public License\nBy exercising the Licensed Rights (defined below), You accept and agree to be bound by the terms and conditions of this Creative Commons Attribution-ShareAlike 4.0 International Public License (‚ÄúPublic License‚Äù). To the extent this Public License may be interpreted as a contract, You are granted the Licensed Rights in consideration of Your acceptance of these terms and conditions, and the Licensor grants You such rights in consideration of benefits the Licensor receives from making the Licensed Material available under these terms and conditions.\n\nSection 1 ‚Äì Definitions.\n\nAdapted Material means material subject to Copyright and Similar Rights that is derived from or based upon the Licensed Material and in which the Licensed Material is translated, altered, arranged, transformed, or otherwise modified in a manner requiring permission under the Copyright and Similar Rights held by the Licensor. For purposes of this Public License, where the Licensed Material is a musical work, performance, or sound recording, Adapted Material is always produced where the Licensed Material is synched in timed relation with a moving image.\nAdapter‚Äôs License means the license You apply to Your Copyright and Similar Rights in Your contributions to Adapted Material in accordance with the terms and conditions of this Public License.\nBY-SA Compatible License means a license listed at creativecommons.org/compatiblelicenses, approved by Creative Commons as essentially the equivalent of this Public License.\nCopyright and Similar Rights means copyright and/or similar rights closely related to copyright including, without limitation, performance, broadcast, sound recording, and Sui Generis Database Rights, without regard to how the rights are labeled or categorized. For purposes of this Public License, the rights specified in Section 2(b)(1)-(2) are not Copyright and Similar Rights.\nEffective Technological Measures means those measures that, in the absence of proper authority, may not be circumvented under laws fulfilling obligations under Article 11 of the WIPO Copyright Treaty adopted on December 20, 1996, and/or similar international agreements.\nExceptions and Limitations means fair use, fair dealing, and/or any other exception or limitation to Copyright and Similar Rights that applies to Your use of the Licensed Material.\nLicense Elements means the license attributes listed in the name of a Creative Commons Public License. The License Elements of this Public License are Attribution and ShareAlike.\nLicensed Material means the artistic or literary work, database, or other material to which the Licensor applied this Public License.\nLicensed Rights means the rights granted to You subject to the terms and conditions of this Public License, which are limited to all Copyright and Similar Rights that apply to Your use of the Licensed Material and that the Licensor has authority to license.\nLicensor means the individual(s) or entity(ies) granting rights under this Public License.\nShare means to provide material to the public by any means or process that requires permission under the Licensed Rights, such as reproduction, public display, public performance, distribution, dissemination, communication, or importation, and to make material available to the public including in ways that members of the public may access the material from a place and at a time individually chosen by them.\nSui Generis Database Rights means rights other than copyright resulting from Directive 96/9/EC of the European Parliament and of the Council of 11 March 1996 on the legal protection of databases, as amended and/or succeeded, as well as other essentially equivalent rights anywhere in the world.\nYou means the individual or entity exercising the Licensed Rights under this Public License. Your has a corresponding meaning.\n\n\n\nSection 2 ‚Äì Scope.\n\nLicense grant.\n\nSubject to the terms and conditions of this Public License, the Licensor hereby grants You a worldwide, royalty-free, non-sublicensable, non-exclusive, irrevocable license to exercise the Licensed Rights in the Licensed Material to:\nA. reproduce and Share the Licensed Material, in whole or in part; and\nB. produce, reproduce, and Share Adapted Material.\nExceptions and Limitations. For the avoidance of doubt, where Exceptions and Limitations apply to Your use, this Public License does not apply, and You do not need to comply with its terms and conditions.\nTerm. The term of this Public License is specified in Section 6(a).\nMedia and formats; technical modifications allowed. The Licensor authorizes You to exercise the Licensed Rights in all media and formats whether now known or hereafter created, and to make technical modifications necessary to do so. The Licensor waives and/or agrees not to assert any right or authority to forbid You from making technical modifications necessary to exercise the Licensed Rights, including technical modifications necessary to circumvent Effective Technological Measures. For purposes of this Public License, simply making modifications authorized by this Section 2(a)(4) never produces Adapted Material.\nDownstream recipients.\nA. Offer from the Licensor ‚Äì Licensed Material. Every recipient of the Licensed Material automatically receives an offer from the Licensor to exercise the Licensed Rights under the terms and conditions of this Public License.\nB. Additional offer from the Licensor ‚Äì Adapted Material. Every recipient of Adapted Material from You automatically receives an offer from the Licensor to exercise the Licensed Rights in the Adapted Material under the conditions of the Adapter‚Äôs License You apply.\nC. No downstream restrictions. You may not offer or impose any additional or different terms or conditions on, or apply any Effective Technological Measures to, the Licensed Material if doing so restricts exercise of the Licensed Rights by any recipient of the Licensed Material.\nNo endorsement. Nothing in this Public License constitutes or may be construed as permission to assert or imply that You are, or that Your use of the Licensed Material is, connected with, or sponsored, endorsed, or granted official status by, the Licensor or others designated to receive attribution as provided in Section 3(a)(1)(A)(i).\n\nOther rights.\n\nMoral rights, such as the right of integrity, are not licensed under this Public License, nor are publicity, privacy, and/or other similar personality rights; however, to the extent possible, the Licensor waives and/or agrees not to assert any such rights held by the Licensor to the limited extent necessary to allow You to exercise the Licensed Rights, but not otherwise.\nPatent and trademark rights are not licensed under this Public License.\nTo the extent possible, the Licensor waives any right to collect royalties from You for the exercise of the Licensed Rights, whether directly or through a collecting society under any voluntary or waivable statutory or compulsory licensing scheme. In all other cases the Licensor expressly reserves any right to collect such royalties.\n\n\n\n\nSection 3 ‚Äì License Conditions.\nYour exercise of the Licensed Rights is expressly made subject to the following conditions.\n\nAttribution.\n\nIf You Share the Licensed Material (including in modified form), You must:\nA. retain the following if it is supplied by the Licensor with the Licensed Material:\n\nidentification of the creator(s) of the Licensed Material and any others designated to receive attribution, in any reasonable manner requested by the Licensor (including by pseudonym if designated);\na copyright notice;\na notice that refers to this Public License;\na notice that refers to the disclaimer of warranties;\na URI or hyperlink to the Licensed Material to the extent reasonably practicable;\n\nB. indicate if You modified the Licensed Material and retain an indication of any previous modifications; and\nC. indicate the Licensed Material is licensed under this Public License, and include the text of, or the URI or hyperlink to, this Public License.\nYou may satisfy the conditions in Section 3(a)(1) in any reasonable manner based on the medium, means, and context in which You Share the Licensed Material. For example, it may be reasonable to satisfy the conditions by providing a URI or hyperlink to a resource that includes the required information.\nIf requested by the Licensor, You must remove any of the information required by Section 3(a)(1)(A) to the extent reasonably practicable.\n\nShareAlike.\n\nIn addition to the conditions in Section 3(a), if You Share Adapted Material You produce, the following conditions also apply.\n\nThe Adapter‚Äôs License You apply must be a Creative Commons license with the same License Elements, this version or later, or a BY-SA Compatible License.\nYou must include the text of, or the URI or hyperlink to, the Adapter‚Äôs License You apply. You may satisfy this condition in any reasonable manner based on the medium, means, and context in which You Share Adapted Material.\nYou may not offer or impose any additional or different terms or conditions on, or apply any Effective Technological Measures to, Adapted Material that restrict exercise of the rights granted under the Adapter‚Äôs License You apply.\n\n\n\nSection 4 ‚Äì Sui Generis Database Rights.\nWhere the Licensed Rights include Sui Generis Database Rights that apply to Your use of the Licensed Material:\n\nfor the avoidance of doubt, Section 2(a)(1) grants You the right to extract, reuse, reproduce, and Share all or a substantial portion of the contents of the database;\nif You include all or a substantial portion of the database contents in a database in which You have Sui Generis Database Rights, then the database in which You have Sui Generis Database Rights (but not its individual contents) is Adapted Material, including for purposes of Section 3(b); and\nYou must comply with the conditions in Section 3(a) if You Share all or a substantial portion of the contents of the database.\n\nFor the avoidance of doubt, this Section 4 supplements and does not replace Your obligations under this Public License where the Licensed Rights include other Copyright and Similar Rights.\n\n\nSection 5 ‚Äì Disclaimer of Warranties and Limitation of Liability.\n\nUnless otherwise separately undertaken by the Licensor, to the extent possible, the Licensor offers the Licensed Material as-is and as-available, and makes no representations or warranties of any kind concerning the Licensed Material, whether express, implied, statutory, or other. This includes, without limitation, warranties of title, merchantability, fitness for a particular purpose, non-infringement, absence of latent or other defects, accuracy, or the presence or absence of errors, whether or not known or discoverable. Where disclaimers of warranties are not allowed in full or in part, this disclaimer may not apply to You.\nTo the extent possible, in no event will the Licensor be liable to You on any legal theory (including, without limitation, negligence) or otherwise for any direct, special, indirect, incidental, consequential, punitive, exemplary, or other losses, costs, expenses, or damages arising out of this Public License or use of the Licensed Material, even if the Licensor has been advised of the possibility of such losses, costs, expenses, or damages. Where a limitation of liability is not allowed in full or in part, this limitation may not apply to You.\nThe disclaimer of warranties and limitation of liability provided above shall be interpreted in a manner that, to the extent possible, most closely approximates an absolute disclaimer and waiver of all liability.\n\n\n\nSection 6 ‚Äì Term and Termination.\n\nThis Public License applies for the term of the Copyright and Similar Rights licensed here. However, if You fail to comply with this Public License, then Your rights under this Public License terminate automatically.\nWhere Your right to use the Licensed Material has terminated under Section 6(a), it reinstates:\n\nautomatically as of the date the violation is cured, provided it is cured within 30 days of Your discovery of the violation; or\nupon express reinstatement by the Licensor.\n\nFor the avoidance of doubt, this Section 6(b) does not affect any right the Licensor may have to seek remedies for Your violations of this Public License.\nFor the avoidance of doubt, the Licensor may also offer the Licensed Material under separate terms or conditions or stop distributing the Licensed Material at any time; however, doing so will not terminate this Public License.\nSections 1, 5, 6, 7, and 8 survive termination of this Public License.\n\n\n\nSection 7 ‚Äì Other Terms and Conditions.\n\nThe Licensor shall not be bound by any additional or different terms or conditions communicated by You unless expressly agreed.\nAny arrangements, understandings, or agreements regarding the Licensed Material not stated herein are separate from and independent of the terms and conditions of this Public License.\n\n\n\nSection 8 ‚Äì Interpretation.\n\nFor the avoidance of doubt, this Public License does not, and shall not be interpreted to, reduce, limit, restrict, or impose conditions on any use of the Licensed Material that could lawfully be made without permission under this Public License.\nTo the extent possible, if any provision of this Public License is deemed unenforceable, it shall be automatically reformed to the minimum extent necessary to make it enforceable. If the provision cannot be reformed, it shall be severed from this Public License without affecting the enforceability of the remaining terms and conditions.\nNo term or condition of this Public License will be waived and no failure to comply consented to unless expressly agreed to by the Licensor.\nNothing in this Public License constitutes or may be interpreted as a limitation upon, or waiver of, any privileges and immunities that apply to the Licensor or You, including from the legal processes of any jurisdiction or authority.\n\n\nCreative Commons is not a party to its public licenses. Notwithstanding, Creative Commons may elect to apply one of its public licenses to material it publishes and in those instances will be considered the ‚ÄúLicensor.‚Äù The text of the Creative Commons public licenses is dedicated to the public domain under the CC0 Public Domain Dedication. Except for the limited purpose of indicating that material is shared under a Creative Commons public license or as otherwise permitted by the Creative Commons policies published at creativecommons.org/policies, Creative Commons does not authorize the use of the trademark ‚ÄúCreative Commons‚Äù or any other trademark or logo of Creative Commons without its prior written consent including, without limitation, in connection with any unauthorized modifications to any of its public licenses or any other arrangements, understandings, or agreements concerning use of licensed material. For the avoidance of doubt, this paragraph does not form part of the public licenses.\nCreative Commons may be contacted at creativecommons.org."
  },
  {
    "objectID": "weeks/week-6.html",
    "href": "weeks/week-6.html",
    "title": "Week 6",
    "section": "",
    "text": "Important\n\n\n\n\nDue date: HW 3 - Fri, 10/6 11:59 PM"
  },
  {
    "objectID": "weeks/week-6.html#participate",
    "href": "weeks/week-6.html#participate",
    "title": "Week 6",
    "section": "Participate",
    "text": "Participate\nüñ•Ô∏è Slides - MLR in Matrix Form\n Data - Delivery"
  },
  {
    "objectID": "weeks/week-6.html#reading",
    "href": "weeks/week-6.html#reading",
    "title": "Week 6",
    "section": "Reading",
    "text": "Reading\nüìñ LRA - Ch 3.1 - 3.5, 3.10\nüìñ CMR - Ch 3.1 - 3.5, 3.9"
  },
  {
    "objectID": "weeks/week-6.html#perform",
    "href": "weeks/week-6.html#perform",
    "title": "Week 6",
    "section": "Perform",
    "text": "Perform\n‚úçÔ∏è HW 3 Multiple Linear Regression due 10/6 11:59 PM\n\n\nBack to course schedule ‚èé"
  },
  {
    "objectID": "weeks/week-7.html",
    "href": "weeks/week-7.html",
    "title": "Week 7",
    "section": "",
    "text": "Important"
  },
  {
    "objectID": "weeks/week-7.html#participate",
    "href": "weeks/week-7.html#participate",
    "title": "Week 7",
    "section": "Participate",
    "text": "Participate\nüñ•Ô∏è Slides - Regression Diagnostics: Residuals and Unusual Data\nüñ•Ô∏è Slides - Regression Diagnostics: Symmetry and Normality\n Data - Delivery\n carData::Duncan (https://www.john-fox.ca/Companion/downloads.html)\n Data - CIA World Factbook Data"
  },
  {
    "objectID": "weeks/week-7.html#reading",
    "href": "weeks/week-7.html#reading",
    "title": "Week 7",
    "section": "Reading",
    "text": "Reading\nüìñ LRA - Ch 4.1 - 4.4, 6.1 - 6.5\nüìñ CMR - Ch 5 - 6\nüìñ CAR - Ch 8.1 - 8.4\nüìñ Regression Diagnostics (RD) - Ch 3 - 5"
  },
  {
    "objectID": "weeks/week-7.html#perform",
    "href": "weeks/week-7.html#perform",
    "title": "Week 7",
    "section": "Perform",
    "text": "Perform\n\n\nBack to course schedule ‚èé"
  },
  {
    "objectID": "weeks/week-5.html",
    "href": "weeks/week-5.html",
    "title": "Week 5",
    "section": "",
    "text": "Important"
  },
  {
    "objectID": "weeks/week-5.html#participate",
    "href": "weeks/week-5.html#participate",
    "title": "Week 5",
    "section": "Participate",
    "text": "Participate\nüñ•Ô∏è Slides - Multiple Linear Regression\n Data - Delivery"
  },
  {
    "objectID": "weeks/week-5.html#reading",
    "href": "weeks/week-5.html#reading",
    "title": "Week 5",
    "section": "Reading",
    "text": "Reading\nüìñ CAR - Ch 4.1 - 4.3, 4.9"
  },
  {
    "objectID": "weeks/week-5.html#exercise",
    "href": "weeks/week-5.html#exercise",
    "title": "Week 5",
    "section": "Exercise",
    "text": "Exercise\nüìã Exercise 3\n\n\nBack to course schedule ‚èé"
  },
  {
    "objectID": "weeks/week-4.html",
    "href": "weeks/week-4.html",
    "title": "Week 4",
    "section": "",
    "text": "Important\n\n\n\n\nDue date: HW 2 - Fri, 9/22 11:59 PM"
  },
  {
    "objectID": "weeks/week-4.html#participate",
    "href": "weeks/week-4.html#participate",
    "title": "Week 4",
    "section": "Participate",
    "text": "Participate\nüñ•Ô∏è Slides - Multiple Linear Regression\n Data - Delivery"
  },
  {
    "objectID": "weeks/week-4.html#reading",
    "href": "weeks/week-4.html#reading",
    "title": "Week 4",
    "section": "Reading",
    "text": "Reading\nüìñ ISL - Ch 3.2"
  },
  {
    "objectID": "weeks/week-4.html#perform",
    "href": "weeks/week-4.html#perform",
    "title": "Week 4",
    "section": "Perform",
    "text": "Perform\n‚úçÔ∏è HW 2 Simple Linear Regression due 9/22 11:59 PM\n\n\nBack to course schedule ‚èé"
  },
  {
    "objectID": "weeks/week-1.html",
    "href": "weeks/week-1.html",
    "title": "Week 1",
    "section": "",
    "text": "üìñ Read the syllabus\nüìñ Get your computing environment ready\nüìñ Be familiar with your computing language"
  },
  {
    "objectID": "weeks/week-1.html#reading",
    "href": "weeks/week-1.html#reading",
    "title": "Week 1",
    "section": "Reading",
    "text": "Reading\nüìñ Introduction to Statistics (IS) - Ch 3 - 5, 8 - 13, 15.\nüìñ An R Companion to Applied Regression (CAR) - Ch 1 - 3, 9, 10."
  },
  {
    "objectID": "weeks/week-1.html#participate",
    "href": "weeks/week-1.html#participate",
    "title": "Week 1",
    "section": "Participate",
    "text": "Participate\nüñ•Ô∏è Slides - Welcome to MATH 4780\nüñ•Ô∏è Slides - Overview of Regression\nüñ•Ô∏è Slides - Probability and Statistics\n Data - Delivery"
  },
  {
    "objectID": "weeks/week-1.html#exercise",
    "href": "weeks/week-1.html#exercise",
    "title": "Week 1",
    "section": "Exercise",
    "text": "Exercise\nüìã Exercise 1\n\n\nBack to course schedule ‚èé"
  },
  {
    "objectID": "weeks/week-3.html",
    "href": "weeks/week-3.html",
    "title": "Week 3",
    "section": "",
    "text": "Important\n\n\n\n\nDrop deadline 9/5 11:59 PM\nDue date: HW 1 - Fri, 9/8 11:59 PM"
  },
  {
    "objectID": "weeks/week-3.html#participate",
    "href": "weeks/week-3.html#participate",
    "title": "Week 3",
    "section": "Participate",
    "text": "Participate\nüñ•Ô∏è Slides - Simple Linear Regression\n Data - Delivery"
  },
  {
    "objectID": "weeks/week-3.html#reading",
    "href": "weeks/week-3.html#reading",
    "title": "Week 3",
    "section": "Reading",
    "text": "Reading\nüìñ ISL - Ch 3.1\nüìñ LRA - Ch 2.1 - 2.6, 2.10\nüìñ CMR - Ch 2.1 - 2.3, 2.5, 2.6, 2.8, 2.9"
  },
  {
    "objectID": "weeks/week-3.html#exercise",
    "href": "weeks/week-3.html#exercise",
    "title": "Week 3",
    "section": "Exercise",
    "text": "Exercise\nüìã Exercise 2\n\n\nBack to course schedule ‚èé"
  },
  {
    "objectID": "weeks/week-2.html",
    "href": "weeks/week-2.html",
    "title": "Week 2",
    "section": "",
    "text": "Important\n\n\n\n\nDrop deadline 9/5 11:59 PM\nDue date: HW 1 - Fri, 9/8 11:59 PM"
  },
  {
    "objectID": "weeks/week-2.html#reading",
    "href": "weeks/week-2.html#reading",
    "title": "Week 2",
    "section": "Reading",
    "text": "Reading\nüìñ IS - Ch 23: Linear Regression"
  },
  {
    "objectID": "weeks/week-2.html#participate",
    "href": "weeks/week-2.html#participate",
    "title": "Week 2",
    "section": "Participate",
    "text": "Participate\nüñ•Ô∏è Slides - Simple Linear Regression\n ggplot2::mpg\n Data - Delivery"
  },
  {
    "objectID": "weeks/week-2.html#perform",
    "href": "weeks/week-2.html#perform",
    "title": "Week 2",
    "section": "Perform",
    "text": "Perform\n‚úçÔ∏è HW 1 Probability and Statistics due 9/8 11:59 PM\n\n\nBack to course schedule ‚èé"
  },
  {
    "objectID": "weeks/week-10.html",
    "href": "weeks/week-10.html",
    "title": "Week 10",
    "section": "",
    "text": "Important"
  },
  {
    "objectID": "weeks/week-10.html#participate",
    "href": "weeks/week-10.html#participate",
    "title": "Week 10",
    "section": "Participate",
    "text": "Participate\nüñ•Ô∏è Slides - Bootstrapping\n Data - Manhattan\n Data - Delivery"
  },
  {
    "objectID": "weeks/week-10.html#reading",
    "href": "weeks/week-10.html#reading",
    "title": "Week 10",
    "section": "Reading",
    "text": "Reading\nüìñ Introduction to Modern Statistics (IMS) - Ch 12, 24.3\nüìñ CAR - Ch 5.1 - 5.2"
  },
  {
    "objectID": "weeks/week-10.html#exercise",
    "href": "weeks/week-10.html#exercise",
    "title": "Week 10",
    "section": "Exercise",
    "text": "Exercise\nüìã Exercise 5\n\n\nBack to course schedule ‚èé"
  },
  {
    "objectID": "weeks/week-11.html",
    "href": "weeks/week-11.html",
    "title": "Week 11",
    "section": "",
    "text": "Important"
  },
  {
    "objectID": "weeks/week-11.html#participate",
    "href": "weeks/week-11.html#participate",
    "title": "Week 11",
    "section": "Participate",
    "text": "Participate\nüñ•Ô∏è Slides - Polynomial Regression\nüñ•Ô∏è Slides - Nonparametric Regression\n Data - Hardwood\n Data - U.S. Birth Rate"
  },
  {
    "objectID": "weeks/week-11.html#reading",
    "href": "weeks/week-11.html#reading",
    "title": "Week 11",
    "section": "Reading",
    "text": "Reading\nüìñ LRA - Ch 7.1 - 7.3"
  },
  {
    "objectID": "weeks/week-11.html#exercise",
    "href": "weeks/week-11.html#exercise",
    "title": "Week 11",
    "section": "Exercise",
    "text": "Exercise\nüìã Exercise 6\nüìã Exercise 7"
  },
  {
    "objectID": "weeks/week-11.html#perform",
    "href": "weeks/week-11.html#perform",
    "title": "Week 11",
    "section": "Perform",
    "text": "Perform\n\n\nBack to course schedule ‚èé"
  },
  {
    "objectID": "weeks/week-13.html",
    "href": "weeks/week-13.html",
    "title": "Week 13",
    "section": "",
    "text": "Important"
  },
  {
    "objectID": "weeks/week-13.html#participate",
    "href": "weeks/week-13.html#participate",
    "title": "Week 13",
    "section": "Participate",
    "text": "Participate\nüñ•Ô∏è Slides - Collinearity\n Data - Manpower"
  },
  {
    "objectID": "weeks/week-13.html#reading",
    "href": "weeks/week-13.html#reading",
    "title": "Week 13",
    "section": "Reading",
    "text": "Reading\nüìñ LRA - Ch 3.11 - 3.12, 9.1 - 9.5\nüìñ CMR - Ch 3.8, 8.1 - 8.2, 8.4"
  },
  {
    "objectID": "weeks/week-13.html#exercise",
    "href": "weeks/week-13.html#exercise",
    "title": "Week 13",
    "section": "Exercise",
    "text": "Exercise\nüìã Exercise 9\n\n\nBack to course schedule ‚èé"
  },
  {
    "objectID": "weeks/week-12.html",
    "href": "weeks/week-12.html",
    "title": "Week 12",
    "section": "",
    "text": "Important\n\n\n\n\nDue date: HW 5 - Fri, 11/17 11:59 PM"
  },
  {
    "objectID": "weeks/week-12.html#participate",
    "href": "weeks/week-12.html#participate",
    "title": "Week 12",
    "section": "Participate",
    "text": "Participate\nüñ•Ô∏è Slides - Categorical Variable\n Data - Tool Life"
  },
  {
    "objectID": "weeks/week-12.html#reading",
    "href": "weeks/week-12.html#reading",
    "title": "Week 12",
    "section": "Reading",
    "text": "Reading\nüìñ LRA - Ch 8.1 - 8.2"
  },
  {
    "objectID": "weeks/week-12.html#exercise",
    "href": "weeks/week-12.html#exercise",
    "title": "Week 12",
    "section": "Exercise",
    "text": "Exercise\nüìã Exercise 8"
  },
  {
    "objectID": "weeks/week-12.html#perform",
    "href": "weeks/week-12.html#perform",
    "title": "Week 12",
    "section": "Perform",
    "text": "Perform\n‚úçÔ∏è HW 5 due 11/17 11:59 PM\n\n\nBack to course schedule ‚èé"
  },
  {
    "objectID": "weeks/week-9.html",
    "href": "weeks/week-9.html",
    "title": "Week 9",
    "section": "",
    "text": "Important\n\n\n\n\nDue date: HW 4 - Fri, 10/27 11:59 PM"
  },
  {
    "objectID": "weeks/week-9.html#participate",
    "href": "weeks/week-9.html#participate",
    "title": "Week 9",
    "section": "Participate",
    "text": "Participate\nüñ•Ô∏è Slides - Diagnostics: Linearity\n Data - CIA World Factbook Data"
  },
  {
    "objectID": "weeks/week-9.html#reading",
    "href": "weeks/week-9.html#reading",
    "title": "Week 9",
    "section": "Reading",
    "text": "Reading\nüìñ LRA - Ch 5.3\nüìñ CMR - Ch 7.3\nüìñ CAR - Ch 8.4\nüìñ RD - Ch 6"
  },
  {
    "objectID": "weeks/week-9.html#exercise",
    "href": "weeks/week-9.html#exercise",
    "title": "Week 9",
    "section": "Exercise",
    "text": "Exercise\nüìã Exercise 4"
  },
  {
    "objectID": "weeks/week-9.html#perform",
    "href": "weeks/week-9.html#perform",
    "title": "Week 9",
    "section": "Perform",
    "text": "Perform\n‚úçÔ∏è HW 4 Diagnostics due 10/27 11:59 PM\n\n\nBack to course schedule ‚èé"
  },
  {
    "objectID": "weeks/week-8.html",
    "href": "weeks/week-8.html",
    "title": "Week 8",
    "section": "",
    "text": "Important\n\n\n\n\nIn-Class Exam Tue, 10/17"
  },
  {
    "objectID": "weeks/week-8.html#participate",
    "href": "weeks/week-8.html#participate",
    "title": "Week 8",
    "section": "Participate",
    "text": "Participate\nüñ•Ô∏è Slides - Diagnostics: Constant Variance\n Data - CIA World Factbook Data"
  },
  {
    "objectID": "weeks/week-8.html#reading",
    "href": "weeks/week-8.html#reading",
    "title": "Week 8",
    "section": "Reading",
    "text": "Reading\nüìñ LRA - Ch 5.2, 5.4 - 5.5\nüìñ CMR - Ch 7.1\nüìñ CAR - Ch 3.4, 8.5\nüìñ RD - Ch 5"
  },
  {
    "objectID": "weeks/week-8.html#perform",
    "href": "weeks/week-8.html#perform",
    "title": "Week 8",
    "section": "Perform",
    "text": "Perform\n‚úÖ In-Class Exam\n\n\nBack to course schedule ‚èé"
  },
  {
    "objectID": "weeks/week-15.html",
    "href": "weeks/week-15.html",
    "title": "Week 15",
    "section": "",
    "text": "Important\n\n\n\n\nDue date: HW 6 - Fri, 12/8 11:59 PM"
  },
  {
    "objectID": "weeks/week-15.html#participate",
    "href": "weeks/week-15.html#participate",
    "title": "Week 15",
    "section": "Participate",
    "text": "Participate\nüñ•Ô∏è Slides - Logistic Regression\n ISLR2::Default\n Data - Strength of Fastener"
  },
  {
    "objectID": "weeks/week-15.html#reading",
    "href": "weeks/week-15.html#reading",
    "title": "Week 15",
    "section": "Reading",
    "text": "Reading\nüìñ ISL - Ch 4.1 - 4.3"
  },
  {
    "objectID": "weeks/week-15.html#exercise",
    "href": "weeks/week-15.html#exercise",
    "title": "Week 15",
    "section": "Exercise",
    "text": "Exercise\nüìã Exercise 11"
  },
  {
    "objectID": "weeks/week-15.html#perform",
    "href": "weeks/week-15.html#perform",
    "title": "Week 15",
    "section": "Perform",
    "text": "Perform\n‚úçÔ∏è HW 6 due 12/8 11:59 PM\n\n\nBack to course schedule ‚èé"
  },
  {
    "objectID": "weeks/week-14.html",
    "href": "weeks/week-14.html",
    "title": "Week 14",
    "section": "",
    "text": "Important"
  },
  {
    "objectID": "weeks/week-14.html#participate",
    "href": "weeks/week-14.html#participate",
    "title": "Week 14",
    "section": "Participate",
    "text": "Participate\nüñ•Ô∏è Slides - Variable Selection\n Data - Manpower"
  },
  {
    "objectID": "weeks/week-14.html#reading",
    "href": "weeks/week-14.html#reading",
    "title": "Week 14",
    "section": "Reading",
    "text": "Reading\nüìñ LRA - Ch 10.1 - 10.2\nüìñ CMR - Ch 4"
  },
  {
    "objectID": "weeks/week-14.html#exercise",
    "href": "weeks/week-14.html#exercise",
    "title": "Week 14",
    "section": "Exercise",
    "text": "Exercise\nüìã Exercise 10"
  },
  {
    "objectID": "weeks/week-14.html#perform",
    "href": "weeks/week-14.html#perform",
    "title": "Week 14",
    "section": "Perform",
    "text": "Perform\n\n\nBack to course schedule ‚èé"
  },
  {
    "objectID": "activity-work.html",
    "href": "activity-work.html",
    "title": "Class activity work",
    "section": "",
    "text": "MSSC 5780 Presenters: David Aguilera, Rachel Cutlan\nMATH 4780 Questioners: Ethan Baierl, Patrick Campbell, Dylan Cardoza"
  },
  {
    "objectID": "activity-work.html#class-activity-2---0926",
    "href": "activity-work.html#class-activity-2---0926",
    "title": "Class activity work",
    "section": "Class activity 2 - 09/26",
    "text": "Class activity 2 - 09/26\n\nMSSC 5780 Presenters: Timothy Goodwin, Matt Jaeggin\nMATH 4780 Questioners: Huangkun Chen, Davion Cook, Nicky Diaz"
  },
  {
    "objectID": "activity-work.html#class-activity-3---1010",
    "href": "activity-work.html#class-activity-3---1010",
    "title": "Class activity work",
    "section": "Class activity 3 - 10/10",
    "text": "Class activity 3 - 10/10\n\nMSSC 5780 Presenters: Samantha Juedemann, Edward Liu\nMATH 4780 Questioners: Nathan Fisher, Alberto Gamez Gonzalez, Andrew Hiller"
  },
  {
    "objectID": "activity-work.html#class-activity-4---1024",
    "href": "activity-work.html#class-activity-4---1024",
    "title": "Class activity work",
    "section": "Class activity 4 - 10/24",
    "text": "Class activity 4 - 10/24\n\nMSSC 5780 Presenters: Sylvester Mensah, Navid Mohseni\nMATH 4780 Questioners: Jake Konrad, Vanessa Lattas, Danny O‚ÄôShea"
  },
  {
    "objectID": "activity-work.html#class-activity-5---1114",
    "href": "activity-work.html#class-activity-5---1114",
    "title": "Class activity work",
    "section": "Class activity 5 - 11/14",
    "text": "Class activity 5 - 11/14\n\nMSSC 5780 Presenters: Emmanuel Nyamekye, Adam Reeson\nMATH 4780 Questioners: Kayley Reith, Donny Robbins, Ray Sun"
  },
  {
    "objectID": "activity-work.html#class-activity-6---1205",
    "href": "activity-work.html#class-activity-6---1205",
    "title": "Class activity work",
    "section": "Class activity 6 - 12/05",
    "text": "Class activity 6 - 12/05\n\nMSSC 5780 Presenters: Anesti Sotirovski, Tanjina Zaman\nMATH 4780 Questioners: Violet Wang, T.J. James Whapham II, Serena Yang"
  },
  {
    "objectID": "hw/hw-5.html",
    "href": "hw/hw-5.html",
    "title": "Homework 5 - Bootstrapping, Polynomial Regression and Nonparametric Regression",
    "section": "",
    "text": "Homework 5 covers course materials of Week 1 to 11.\nPlease submit your work in one PDF file including all parts to D2L > Assessments > Dropbox. Multiple files or a file that is not in pdf format are not allowed.\nIn your homework, please number and answer questions in order.\nYour entire work on Statistical Computing and Data Analysis should be completed by any word processing software (Microsoft Word, Google Docs, (R)Markdown, Quarto, LaTex, etc) and your preferred programming language. Your document should be a PDF file.\nQuestions starting with (MSSC) are for MSSC 5780 students. MATH 4780 students could possibly earn extra points from them.\nIt is your responsibility to let me understand what you try to show. If you type your answers, make sure there are no typos. I grade your work based on what you show, not what you want to show. If you choose to handwrite your answers, write them neatly. If I can‚Äôt read your sloppy handwriting, your answer is judged as wrong."
  },
  {
    "objectID": "hw/hw-5.html#bootstrapping",
    "href": "hw/hw-5.html#bootstrapping",
    "title": "Homework 5 - Bootstrapping, Polynomial Regression and Nonparametric Regression",
    "section": "Bootstrapping",
    "text": "Bootstrapping\n\nFor data set mpg.csv, use the response gasoline mileage \\(y\\) (miles per gallon), and the predictor engine displacement \\(x_1\\) (cubic inches) to generate and plot 200 bootstrapped regression lines with the sample regression line.\nMake the histogram of bootstrapped samples of \\(\\beta_1\\), and provide the \\(95\\%\\) bootstrapped CI for \\(\\beta_1\\) using the percentile method and the standard error method."
  },
  {
    "objectID": "hw/hw-5.html#polynomial-regression",
    "href": "hw/hw-5.html#polynomial-regression",
    "title": "Homework 5 - Bootstrapping, Polynomial Regression and Nonparametric Regression",
    "section": "Polynomial Regression",
    "text": "Polynomial Regression\nA solid-fuel rocket propellant loses weight after it is produced. The following data are available:\n\n\n\nMonths since Production, \\(x\\)\nWeight Loss \\(y\\) (kg)\n\n\n\n\n0.25\n1.42\n\n\n0.50\n1.39\n\n\n0.75\n1.55\n\n\n1.00\n1.89\n\n\n1.25\n2.43\n\n\n1.50\n3.15\n\n\n1.75\n4.05\n\n\n2.00\n5.15\n\n\n2.25\n6.43\n\n\n2.50\n7.89\n\n\n\n\nFit a second-order polynomial \\(y = \\beta_0 + \\beta_1x + \\beta_{2}x^2 + \\epsilon\\) to the data.\nTest for significance of regression.\nTest the hypothesis \\(H_0:\\beta_2 = 0\\). Comment on the need for the quadratic term in this model.\nAre there any potential hazards in extrapolating with this model?\nCompute the R-student residuals for the second-order model. Analyze the residuals and comment on the adequacy of the model.\nFit a second-order model \\(y = \\beta_0 + \\beta_1z + \\beta_{2}z^2 + \\epsilon\\) to the data, where \\(z = x - \\bar{x}\\), i.e., \\(z\\) is the centered \\(x\\).\nConstruct the design matrix \\({\\bf W} = \\begin{bmatrix} {\\bf w}_1 & {\\bf w}_2 \\end{bmatrix}\\) of the model in (1), where \\({\\bf w}_1\\) is the predictor \\({\\bf x}\\) vector after unit normal scaling and \\({\\bf w}_2\\) is the predictor \\({\\bf x}^2\\) vector after unit normal scaling. That is, for each \\(i = 1, 2, \\dots, n\\), \\(w_{1i} = \\frac{x_i - \\bar{x}}{s_x}\\), where \\(s_x\\) is the the standard deviation of \\(x_i\\)s; \\(w_{2i} = \\frac{d_i - \\bar{d}}{s_d}\\) where \\(d_i = x_i^2\\), and \\(s_d\\) is the standard deviation of \\(d_i\\)s. Similarly, construct the design matrix for the model in (6). Remember to use the predictor \\(z\\), the centered version of \\(x\\).\nCompute \\(\\mathbf{\\Sigma} = \\mathbf{W}'\\mathbf{W}/(n-1)\\) of the model in (1) and (6). In fact, \\(\\mathbf{\\Sigma}\\) is the correlation matrix of predictors. Which model has higher correlation between predictors?\nCompute the inverse of \\(\\mathbf{\\Sigma}\\) of the model in (1) and (6). In fact, the diagonal elements of the inverse matrix measures the degree of ill-conditioning. Does the centering of \\(x\\) help alleviate the ill-conditioning problem?"
  },
  {
    "objectID": "hw/hw-5.html#nonlinearity-diagnostics",
    "href": "hw/hw-5.html#nonlinearity-diagnostics",
    "title": "Homework 5 - Bootstrapping, Polynomial Regression and Nonparametric Regression",
    "section": "Nonlinearity Diagnostics",
    "text": "Nonlinearity Diagnostics\nConsider the Canadian occupational-prestige data set Prestige in the carData package. Treat the variable prestige as the response, and education, income and women are the three predictors.\n\nGenerate the scatterplot matrix. Does the plot suggest any nonlinear relationship between the response and predictors or among predictors? Is the plot useful for detecting nonlinearity assumption of regression?\nFit the linear regression model, and plot the residual plots. Comment the plot.\nGenerate partial residual plots (component-plus-residual plot). Based on the plot, transform any predictors if necessary, and refit the regression. Check the partial residual plot of the refitted model and confirm that the nonlinearity issue is fixed."
  },
  {
    "objectID": "hw/hw-5.html#kernel-methods",
    "href": "hw/hw-5.html#kernel-methods",
    "title": "Homework 5 - Bootstrapping, Polynomial Regression and Nonparametric Regression",
    "section": "Kernel Methods",
    "text": "Kernel Methods\n\nGenerate the 40 data points \\(\\{x_i, y_i\\}_{i=1}^{40}\\) from the model \\(y = f(x) + \\epsilon\\), where \\(f(x) = 2\\sin(x)\\) and \\(\\epsilon\\sim N(0, 1)\\). Use seq() to generate the \\(x_i\\)s.\nUse KernSmooth::locpoly() to generate Gaussian kernel smoothers with bandwidth 0.2, 1, and 100. Plot them as the figure shown below.\n\n\n\n\n\n\n\n\n\n\n\nUse locpoly() to fit the Gaussian local linear regressions (degree = 1) with bandwidth 0.2, 1, and 100. Generate the plot as the one in (2).\nUse locpoly() to fit the Gaussian local quadratic regressions (degree = 2) with bandwidth 0.2, 1, and 100. Generate the plot as the one in (2).\nUse loess() to fit the tricube local quadratic regressions with the span parameter \\(\\alpha = 0.2, 0.5, 0.9\\). Generate the plot as the one in (2).\nComment on your plots in (2)-(5), and discuss your findings. Which method you prefer?"
  },
  {
    "objectID": "hw/hw-4.html",
    "href": "hw/hw-4.html",
    "title": "Homework 4 - Diagnostics",
    "section": "",
    "text": "Homework 4 covers course materials of Week 1 to 9.\nPlease submit your work in one PDF file including all parts to D2L > Assessments > Dropbox. Multiple files or a file that is not in pdf format are not allowed.\nIn your homework, please number and answer questions in order.\nYour entire work on Statistical Computing and Data Analysis should be completed by any word processing software (Microsoft Word, Google Docs, (R)Markdown, Quarto, LaTex, etc) and your preferred programming language. Your document should be a PDF file.\nQuestions starting with (MSSC) are for MSSC 5780 students. MATH 4780 students could possibly earn extra points from them.\nIt is your responsibility to let me understand what you try to show. If you type your answers, make sure there are no typos. I grade your work based on what you show, not what you want to show. If you choose to handwrite your answers, write them neatly. If I can‚Äôt read your sloppy handwriting, your answer is judged as wrong."
  },
  {
    "objectID": "hw/hw-4.html#diagnostics-on-gasoline-mileage-data",
    "href": "hw/hw-4.html#diagnostics-on-gasoline-mileage-data",
    "title": "Homework 4 - Diagnostics",
    "section": "Diagnostics on Gasoline Mileage Data",
    "text": "Diagnostics on Gasoline Mileage Data\nWe use the same data set mpg.csv for data analysis. Consider the multiple regression model \\(y = \\beta_0 + \\beta_1x_1 + \\beta_6x_6+\\epsilon\\) fit to the gasoline mileage data in your Homework 3.\n\nCompare R-student residuals \\(t_i\\) with Student-t \\(t_{n-p-1}\\) using a qqplot. Generate the histogram and density plot of \\(t_i\\) as well. Does there seem to be any problem with the normality assumption?\nPerform the Box-Cox method and discuss the necessity of any transformation on \\(y\\).\nUse \\(\\lambda = 0\\) (log transformation) and the \\(\\lambda\\) selected by the Box-Cox method to refit the model to the transformed data. Compare their R-student residuals with the R-student residuals from the non-transformed data using a boxplot.\nConstruct a plot of the R-student residuals \\(t_i\\) versus the fitted responses and the Tukey‚Äôs spread-level plot. Any sign of violation of constant variance?\nIn fact, we can perform some formal hypothesis testing on constant variance like \\(H_0:\\) Constant variance vs.¬†\\(H_1:\\) Variance changes with \\(E(y\\mid x)\\). Use car::ncvTest() to perform the test. Explain the testing result."
  },
  {
    "objectID": "hw/hw-4.html#influence-diagnostics-on-squid-data",
    "href": "hw/hw-4.html#influence-diagnostics-on-squid-data",
    "title": "Homework 4 - Diagnostics",
    "section": "Influence Diagnostics on Squid Data",
    "text": "Influence Diagnostics on Squid Data\nAn experiment was conducted to study the size of squid eaten by sharks and tuna. The regressors are characteristics of the beak or month of the squid. The squid.csv data contain the variables\n\n\\(x_1\\): Rostral length in inches\n\\(x_2\\): Wing length in inches\n\\(x_3\\): Rostral to notch length\n\\(x_4\\): Notch to wing length\n\\(x_5\\): Width in inches\n\\(y\\): Weights in pounds\n\nPerform a thorough leverage and influence diagnostics of the squid data.\n\nCompute R-studentized residuals, hat values, Cook‚Äôs distance, DFFITS, DFBETAS, and COVRATIO measures. Describe how you detect leverage and influential points. Discuss the effect of data points on coefficients, fitted values, and precision of coefficients. (The influence.measures() function provides all influence measures.)\nLet‚Äôs use some visualization tools.\n\nCreate the bubble plot.\nCreate the influence index plot (car::influenceIndexPlot())\n\nProduce the added-valued plot (car::avPlots()) for each regressor \\(x_i, i = 1, \\dots, 5\\). Are there any joint influence of data points on the regression coefficients?\n(MSSC) Numerically verify the following properties of the added-valued plot.\n\nThe slope of the least squares simple regression line of \\(e(y \\mid x_{(1)})\\) on \\(e(x_1 \\mid x_{(1)})\\) is the same as the least-squares slope \\(b_1\\) for \\(x_1\\) in the full multiple regression.\nThe residuals from the simple regression \\(e(y \\mid x_{(1)})\\) vs.¬†\\(e(x_1 \\mid x_{(1)})\\) are the same as the residuals \\(e_i\\) from the full multiple regression.\nThe standard error of \\(b_1\\) is \\(s / \\sqrt{\\sum_{i=1}^ne_i^2(x_1 \\mid x_{(1)})}\\), where \\(s = \\sqrt{MS_{res}}\\) from the full multiple regression."
  },
  {
    "objectID": "hw/hw-4.html#simulation",
    "href": "hw/hw-4.html#simulation",
    "title": "Homework 4 - Diagnostics",
    "section": "Simulation",
    "text": "Simulation\nConsider a simple linear regression model \\(y_i =\\beta_0 + \\beta_1x_i + \\epsilon_i, i = 1, 2, \\dots, n\\) with \\(\\epsilon_i \\stackrel{\\rm iid}{\\sim} N(0, \\sigma^2)\\). Set \\(x_i = i\\), \\(\\beta_0 = 2\\), \\(\\beta_1 = -1.5\\), \\(\\sigma = 1.2\\), \\(n= 15\\).\n\n(MSSC) Generate a sample \\(\\{y_i\\}_{i=1}^n\\) from this model, and compute the regression coefficients \\(b_0\\), \\(b_1\\) and variance estimate \\(s^2\\). Repeat 10,000 times.\n(MSSC) Compute the sample mean of \\(b_0\\), \\(b_1\\), and \\(s^2\\). Compare them with their true expected value. What is your conclusion?\n(MSSC) Plot the histograms of \\(b_0\\), \\(b_1\\), and \\(s^2\\). Compare the histograms with their sampling distribution.\n(MSSC) Compute the covariance matrix of \\(b_0\\) and \\(b_1\\) and the true covariance matrix. Compare the two.\n\n\n\n(MSSC) Suppose now \\(\\beta_1 = 0.0001\\). Generate the simulated data \\(\\{x_i, y_i\\}_{i=1}^n\\) with \\(n = 10, 100, 1000, 10000\\). Show that as \\(n\\) increases, for the test \\(H_0: \\beta_1 = 0\\), its \\(t_{test}\\) gets larger and its \\(p\\)-value gets smaller, and therefore \\(H_0\\) is rejected for a sufficiently large \\(n\\), even though \\(\\beta_1\\) is practically zero."
  },
  {
    "objectID": "hw/hw-6.html",
    "href": "hw/hw-6.html",
    "title": "Homework 6 - Collinearity and Variable Selection",
    "section": "",
    "text": "Homework Instruction and Requirement\n\nHomework 6 covers course materials of Week 1 to 14.\nPlease submit your work in one PDF file including all parts to D2L > Assessments > Dropbox. Multiple files or a file that is not in pdf format are not allowed.\nIn your homework, please number and answer questions in order.\nYour entire work on Statistical Computing and Data Analysis should be completed by any word processing software (Microsoft Word, Google Docs, (R)Markdown, Quarto, LaTex, etc) and your preferred programming language. Your document should be a PDF file.\n\n\n\nIt is your responsibility to let me understand what you try to show. If you type your answers, make sure there are no typos. I grade your work based on what you show, not what you want to show. If you choose to handwrite your answers, write them neatly. If I can‚Äôt read your sloppy handwriting, your answer is judged as wrong.\n\n\n\nReading and Writing\nIn this course, we have been using the classical or frequentist approach to do a variety of statistical inferences, marginal \\(t\\) test for \\(\\beta_j\\) and \\(F\\) tests for model comparison for example. But Dr.¬†Yu once said he never uses p-value in his own research, and what is taught in Intro Stats MATH 4720 is mostly problematic. In fact, the null hypothesis significance testing (NHST) paradigm and the p-value usage have been much criticized and shown to be problematic, misused, and resulting in reproducibility and replication crisis in scientific research. Please write a summary paper at least two pages including\n\nInterpretation of p-value\nList and discussion about the problems of the NHST and p-value method\nPossible solutions to those problems\n\nSome references are\n\nWikipedia: Misuse of p-values\nA. Reinhart (2015), ‚ÄúStatistics Done Wrong‚Äù, No Starch Press, San Francisco.\nR. L. Wasserstein and Nicole A. Lazar (2016), ‚ÄúThe ASA Statement on p-Values: Context, Process, and Purpose‚Äù, The American Statistician, 70:2, 129-133.\nV. Amrhein, S. Greenland and B. McShane (2019), ‚ÄúRetire statistical significance‚Äù, Nature, 567, 305-307.\nB. McShane, D. Gal, A. Gelman, C. Robert and Jennifer L. Tackett (2019), ‚ÄúAbandon Statistical Significance‚Äù, The American Statistician, 73:sup1, 235-245.\nA. Gelamn and E. Loken (2014), ‚ÄúThe Statistical Crisis in Science‚Äù, American Scientist, 102, 460-465.\nR. L. Wasserstein, A. L. Schirm and N. A. Lazar (2019), ‚ÄúMoving to a World Beyond p < 0.05‚Äù, The American Statistician, 73:sup1, 1-19.\n\nThere are lots of discussions and papers out there. You are welcome to google more resources to support your argument. The work should be entirely your effort. You are not allowed to copy anyone‚Äôs words, and you have to cite any resources you use, papers, blogs, videos, lecture notes, etc, or you violate Marquette academic misconduct policy.\n\n\n\n\n\n\n\nStatistical Computing and Data Analysis\nPlease perform a data analysis using \\(\\texttt{R}\\) or your preferred language. Any results should be generated by computer outputs, and your work should be done entirely by your computer. Handwriting is not allowed. Relevant code should be attached.\nWe use the same data set mpg.csv for data analysis. For the following analysis, if any regressor contains a missing value NA, remove the corresponding row of the data matrix.\n\nBuild a linear regression model relating gasoline mileage \\(y\\) to vehicle weight \\(x_{10}\\) and the type of transmission \\(x_{11}\\) (1 automatic; 0 manual). Does the type of transmission significantly affect the mileage performance?\nModify the model developed in (1) to include an interaction between vehicle weight and the type of transmission. What conclusions can you draw about the effect of the type of transmission on gasoline mileage? Interpret the parameters in this model.\n\nFor the following questions (3) to (9), consider all the regressors except \\(x_4\\), \\(x_5\\) and \\(x_{11}\\).\n\n\\(y\\): MPG\n\\(x_1\\): Displacement (cubic in.)\n\\(x_2\\): Horsepower (ft-lb)\n\\(x_3\\): Torque (ft-lb)\n\\(x_6\\): Carburetor (barrels)\n\\(x_7\\): No.¬†of transmission speeds\n\\(x_8\\): Overall length (in.)\n\\(x_9\\): Width (in.)\n\\(x_{10}\\): Weight (lb)\n\nStandardize the response and predictors before later analysis.\n\n\nObtain the correlation matrix of regressors. Does it give any indication of collinearity?\nCalculate the variance inflation factors (VIFs). Is there any evidence of collinearity?\nFind the eigenvectors associated with the smallest eigenvalues of \\({\\mathbf X}'{\\mathbf X}/(n - 1)\\) or the correlation matrix of regressors. Interpret the elements of these vectors. What can you say about the source of collinearity in these data?\nCompute the condition indices and variance-decomposition proportions. What statements can you make about collinearity in these data?\nFit the ordinary multiple regression and the ridge regression. Use the ridge trace to select an appropriate value of \\(\\delta\\). Explain how the ridge regression coefficients change as the parameter \\(\\delta\\) increases. This gives you the idea of why the ridge regression is a shrinkage method.\n\n\n\n\n\n\nUse the all-possible-regressions approach to find an appropriate regression model.\nUse stepwise regression to specify a subset regression model. Does this lead to the same model found in (8)?"
  },
  {
    "objectID": "hw/hw-3.html",
    "href": "hw/hw-3.html",
    "title": "Homework 3 - Multiple Linear Regression",
    "section": "",
    "text": "Homework Instruction and Requirement\n\nHomework 3 covers course materials of Week 1 to 6.\nPlease submit your work in one PDF file including all parts to D2L > Assessments > Dropbox. Multiple files or a file that is not in pdf format are not allowed.\nIn your homework, please number and answer questions in order.\nYour answers may be handwritten on the Mathematical Derivation and Reasoning part. However, you need to scan your paper and make it a PDF file.\nYour entire work on Statistical Computing and Data Analysis should be completed by any word processing software (Microsoft Word, Google Docs, (R)Markdown, Quarto, LaTex, etc) and your preferred programming language. Your document should be a PDF file.\nQuestions starting with (MSSC) are for MSSC 5780 students. MATH 4780 students could possibly earn extra points from them.\nIt is your responsibility to let me understand what you try to show. If you type your answers, make sure there are no typos. I grade your work based on what you show, not what you want to show. If you choose to handwrite your answers, write them neatly. If I can‚Äôt read your sloppy handwriting, your answer is judged as wrong.\n\n\n\nMathematical Derivation and Reasoning\nThe simple linear regression and multiple linear regression models and notations are the same as defined in our course slides and textbook.\nIn simple linear regression,\n\n(MSSC) Show that \\(r^2 = \\frac{SS_R}{SS_T} = R^2\\), that is, the square of the sample correlation coefficient between \\(y\\) and \\(x\\) is equal to the coefficient of determination.\n\n\n\n\nSuppose \\({\\bf A}_{n \\times n}\\) is a symmetric idempotent matrix.\n\nIn multiple linear regression, let the hat matrix \\({\\bf H}_{n \\times n} = {\\bf X(X'X)}^{-1}{\\bf X'}\\).\n\n(MSSC) Show that \\({\\bf H}\\) and \\((\\bf I - H)\\) are symmetric and idempotent.\n(MSSC) Show that \\(\\text{tr}({\\bf H}) = p\\).\n\n\n\nStatistical Computing and Data Analysis\nPlease perform a data analysis using \\(\\texttt{R}\\) or your preferred language. Any results should be generated by computer outputs, and your work should be done entirely by your computer. Handwriting is not allowed. Relevant code should be attached.\nWe use the same data set mpg.csv for data analysis.\n\nFit a MLR model \\(y = \\beta_0 + \\beta_1x_1 + \\beta_6x_6+\\epsilon\\) relating gasoline mileage \\(y\\) (miles per gallon) to engine displacement \\(x_1\\) and the number of carburetor barrels \\(x_6\\). Interpret the regression coefficients \\(\\beta_1\\) and \\(\\beta_6\\).\nWrite down the \\(H_0\\) and \\(H_1\\) for testing significance of regression, and construct the ANOVA table to test the significance. Explain your decision rule and conclusion.\nObtain \\(R^2\\) and \\(R_{Adj}^2\\) for this MLR model. Compare these to the \\(R^2\\) and \\(R_{Adj}^2\\) for the SLR model relating mileage to engine displacement.\nFind a \\(95\\%\\) confidence interval (CI) for \\(\\beta_1\\). Interpret your results.\nWith \\(\\alpha = 0.05\\), do the marginal test \\(H_0: \\beta_6 = 0\\). Interpret your results.\nFind a \\(95\\%\\) CI on the mean gasoline mileage when \\(x_1 = 275\\) in\\(^3\\) and \\(x_6 = 2\\) barrels.\nFind a \\(95\\%\\) prediction interval (PI) for a new observation on gasoline mileage when \\(x_1 = 275\\) in\\(^3\\) and \\(x_6 = 2\\) barrels.\nIn Homework 2 you were asked to compute \\(95\\%\\) CI on mean gasoline mileage and PI on a car‚Äôs gasoline mileage when the engine displacement \\(x_1 = 275\\) in\\(^3\\). Compare the length of these intervals to the length of the CI and PI from the question 6 and 7 above. Does adding \\(x_6\\) to the model help in terms of prediction or uncertainty reduction?\nPerform matrix operations to compute \\(({\\bf y - X b})'({\\bf y - X b})\\) and verify it is \\(SS_{res} = \\sum_{i=1}^n(y_i - \\hat{y}_i)^2\\).\nGenerate the predictor effect plot for \\(x_1\\) and \\(x_6\\). Explain the plot by discussing the effect of each predictor on \\(y\\).\nConstruct the \\(95\\%\\) confidence region for the coefficients \\((\\beta_1, \\beta_6)\\). Interpret the region."
  },
  {
    "objectID": "hw/hw-2.html",
    "href": "hw/hw-2.html",
    "title": "Homework 2 - Simple linear regression",
    "section": "",
    "text": "Homework 2 covers course materials of Week 1 to 4.\nPlease submit your work in one PDF file including all parts to D2L > Assessments > Dropbox. Multiple files or a file that is not in pdf format are not allowed.\nIn your homework, please number and answer questions in order.\nYour answers may be handwritten on the Mathematical Derivation and Reasoning part. However, you need to scan your paper and make it a PDF file.\nYour entire work on Statistical Computing and Data Analysis should be completed by any word processing software (Microsoft Word, Google Docs, (R)Markdown, Quarto, LaTex, etc) and your preferred programming language. Your document should be a PDF file.\nQuestions starting with (MSSC) are for MSSC 5780 students. MATH 4780 students could possibly earn extra points from them.\nIt is your responsibility to let me understand what you try to show. If you type your answers, make sure there are no typos. I grade your work based on what you show, not what you want to show. If you choose to handwrite your answers, write them neatly. If I can‚Äôt read your sloppy handwriting, your answer is judged as wrong."
  },
  {
    "objectID": "hw/hw-2.html#simulation",
    "href": "hw/hw-2.html#simulation",
    "title": "Homework 2 - Simple linear regression",
    "section": "Simulation",
    "text": "Simulation\n\nGenerate a simulated data of size \\(n = 100\\) from the regression\n\n\\[y_i = 10 + 5x_i + \\epsilon_i, ~~ \\epsilon_i \\stackrel{iid}{\\sim} N(0, 2)\\]\nby completing the code\n\nx <- runif(_____)\ny <- ____ + ____ * ____ + r____(_____, sd = _____)\n\nFit a simple linear regression model to the data, then check whether the true slope is captured by the 90% confidence interval for the slope."
  },
  {
    "objectID": "hw/hw-2.html#data-analysis",
    "href": "hw/hw-2.html#data-analysis",
    "title": "Homework 2 - Simple linear regression",
    "section": "Data Analysis",
    "text": "Data Analysis\nThe data set mpg.csv presents data on the gasoline mileage performance of 32 different automobiles. (Table B.3 in the textbook LRA)\nTo import the data set into your R session, use read.csv() like\n\ndata_name_you_like <- read.csv(\"the_path_that_saves_your_data/mpg.csv\")\n\nOnce you load the data set, type its name on the R console. The data should be a data frame with 32 rows and 12 columns that looks like\n\n\n      y  x1  x2  x3   x4   x5 x6 x7    x8   x9  x10 x11\n1 18.90 350 165 260 8.00 2.56  4  3 200.3 69.9 3910   1\n2 17.00 350 170 275 8.50 2.56  4  3 199.6 72.9 3860   1\n3 20.00 250 105 185 8.25 2.73  1  3 196.7 72.2 3510   1\n4 18.25 351 143 255 8.00 3.00  2  3 199.9 74.0 3890   1\n5 20.07 225  95 170 8.40 2.76  1  3 194.1 71.8 3365   0\n6 11.20 440 215 330 8.20 2.88  4  3 184.5 69.0 4215   1\n\n\nIf this is what you get, you are good to start!\nThe variables are\n\n\n\n\\(y\\): Miles per gallon\n\\(x_1\\): Displacement (cubic in.)\n\\(x_2\\): Horsepower (ft-lb)\n\\(x_3\\): Torque (ft-lb)\n\\(x_4\\): Compression ratio\n\\(x_5\\): Rear axle ratio\n\n\n\n\\(x_6\\): Carburetor (barrels)\n\\(x_7\\): No.¬†of transmission speeds\n\\(x_8\\): Overall length (in.)\n\\(x_9\\): Width (in.)\n\\(x_{10}\\): Weight (lb)\n\\(x_{11}\\): Type of transmission (A automatic; M manual)\n\n\n\n\nFit a simple linear regression model relating gasoline mileage \\(y\\) (miles per gallon) to engine displacement \\(x_1\\) (cubic inches). Explain your coefficients. Any potential concern?\nProvide the \\(95\\%\\) CI for \\(\\beta_0\\), \\(\\beta_1\\) and \\(\\sigma^2\\).\nWith \\(\\alpha = 0.05\\), test if \\(\\beta_1\\) is significantly different from 0. Provide procedure and steps, for example, \\(H_0\\) and \\(H_1\\), the test statistic or \\(p\\)-value, and decision rule.\nConstruct the ANOVA table and test for significance of regression.\nWhat percent of the total variability in gasoline mileage is accounted for by the linear relationship with engine displacement?\nFind a \\(95\\%\\) CI on the mean gasoline mileage if the engine displacement is 275 in\\(^3\\) engine.\nSuppose that we wish to predict the gasoline mileage obtained from a car with a 275 in\\(^3\\) engine. Give a point estimate of mileage. Find a 95% prediction interval (PI) on the mileage. Compare the PI with the CI in 6. Explain the difference between them. Which one is wider, and why?\nPlot data \\(\\{(x_{1i}, y_i)\\}_{i=1}^{32}\\), the fitted regression line, CI for \\(\\mu\\) and PI for \\(y\\) in one figure. Add appropriate labels of axes, title, and legend. [Hint: Create a sequence of values of \\(x\\), and obtain CI and PI for each value of \\(x\\). Use legend() to add legends to a plot.]\nUse the data and your fitted result to verify that\n\n\n\\(\\scriptstyle \\sum_{i=1}^{32}(y_i - \\hat{y}_i) = \\sum_{i=1}^ne_i = 0\\)\n\n\n\\(\\scriptstyle \\sum_{i=1}^{32}y_i = \\sum_{i=1}^{32}\\hat{y}_i\\)\n\n\nThe LS regression line passes through the centroid \\((\\overline{x}, \\overline{y})\\)\n\n\n\\(\\scriptstyle \\sum_{i=1}^{32}x_ie_i = 0\\) (may not be exactly but numerically 0)\n\n\n\\(\\scriptstyle \\sum_{i=1}^{32}\\hat{y}_ie_i = 0\\) (may not be exactly but numerically 0)"
  },
  {
    "objectID": "hw/hw-1.html",
    "href": "hw/hw-1.html",
    "title": "Homework 1 - Probaility and Statistics Review",
    "section": "",
    "text": "Homework Instruction and Requirement\n\nHomework 1 covers course materials of Week 1 to 2.\nPlease submit your work in one PDF file to D2L > Assessments > Dropbox. Multiple files or a file that is not in pdf format are not allowed.\nIn your homework, please number and answer questions in order.\nIt is your responsibility to let me understand what you try to show. If you type your answers, make sure there are no typos. I grade your work based on what you show, not what you want to show. If you choose to handwrite your answers, write them neatly. If I can‚Äôt read your sloppy handwriting, your answer is judged as wrong.\nRelevant code should be attached.\n\n\n\nProgramming and Computing\nPlease sharpen your coding skill using R or any language you prefer. No need to show your work on this part!\n\n\nProbability and Statistics Review\n\n\n\n\\(Y_1 \\sim N(3, 8)\\) and \\(Y_2 \\sim N(1, 4)\\), and \\(Y_1\\) and \\(Y_2\\) are independent. What is the distribution the variable \\(2Y_1 + 3Y_2\\) follows?\n\n\n\n\n\n\n\nPlot normal density curves with different choices of mean and standard deviation.\nInstall the R package ISLR2. Choose a continuous variable in the ISLR2::Boston data set. Use the sample() function to draw a simple random sample of size 20 from this population. Calculate the sample average.\nRepeat the sampling in 3. several times to plot a sampling distribution of the sample mean.\nSuppose \\(Y_i \\stackrel{iid}{\\sim} N(\\mu, \\sigma^2)\\), \\(i = 1, 2, \\dots, n\\), with unknown \\(\\mu\\) and \\(\\sigma\\). The \\(100(1-\\alpha)\\%\\) confidence interval (CI) for the population mean \\(\\mu\\) is \\(\\left( \\overline{y}- t_{\\alpha/2, n-1}\\frac{s}{\\sqrt{n}}, \\overline{y} + t_{\\alpha/2, n-1}\\frac{s}{\\sqrt{n}} \\right)\\). Use simulation with \\(\\alpha = 0.1\\), \\(\\mu = 4\\) and \\(\\sigma = 2\\) to verify that such CIs contain \\(\\mu\\) about \\(100(1-\\alpha)\\%\\) of times. Fill the percentage in the following table, and comment on your results.\n\n\n\n\nSimulation times\n\\(n=5\\)\n\\(n=30\\)\n\\(n=200\\)\n\n\n\n\n\\(20\\)\n\n\n\n\n\n\\(1000\\)\n\n\n\n\n\n\\(20000\\)\n\n\n\n\n\n\n\nIf \\(U_1\\) and \\(U_2\\) are independent and both are uniform random variables over \\([0, 1]\\) interval https://en.wikipedia.org/wiki/Continuous_uniform_distribution, then \\(X_1\\) and \\(X_2\\) defined by \\[X_1 = \\sqrt{-2\\ln(U_1)}\\cos(2\\pi U_2), \\quad X_2 = \\sqrt{-2\\ln(U_1)}\\sin(2\\pi U_2)\\] are independent \\(N(0, 1)\\) variables. Draw 10,000 samples for \\(U_1\\) and \\(U_2\\) using the runif() function, and use the transformation to generate the samples of \\(X_1\\) and \\(X_2\\). Verify\n\nthe standard normality of \\(X_1\\) and \\(X_2\\) by plotting their histogram with a superimposed standard normal density.\nthe independence of \\(X_1\\) and \\(X_2\\) by plotting the scatterplot of \\(X_1\\) and \\(X_2\\) and computing their correlation coefficient."
  },
  {
    "objectID": "project-tips-resources.html",
    "href": "project-tips-resources.html",
    "title": "Project tips + resources",
    "section": "",
    "text": "To be announced."
  },
  {
    "objectID": "activity-description.html",
    "href": "activity-description.html",
    "title": "Class activity description",
    "section": "",
    "text": "There are six in-class activities on Tuesdays 09/19, 09/26, 10/10, 10/24, 11/14, and 12/05."
  },
  {
    "objectID": "activity-description.html#format",
    "href": "activity-description.html#format",
    "title": "Class activity description",
    "section": "Format",
    "text": "Format\nOn each class activity day,\n\nTwo assigned MSSC 5780 students are presenting the regression problems assigned two days before the presentation date.\nEach presentation including Q & A is about 5-minute long. You will be forced to end your presentation if it is too long.\nEach presenter should prepare at least two non-Yes-No questions for all participants to answer.\nEach of the three assigned MATH 4780 students should ask either one or both presenters at least one non-Yes-No question."
  },
  {
    "objectID": "activity-description.html#presentation-materials",
    "href": "activity-description.html#presentation-materials",
    "title": "Class activity description",
    "section": "Presentation Materials",
    "text": "Presentation Materials\n\nNo specific presentation style/format/materials are required. You, the presenter, decide how you present your work.\nThe presenters should send the presentation materials, PDF, slides for example, to Dr.¬†Yu before the class begins on the class activity day.\nYour presentation materials will be posted on Activity work page."
  },
  {
    "objectID": "activity-description.html#evaluation",
    "href": "activity-description.html#evaluation",
    "title": "Class activity description",
    "section": "Evaluation",
    "text": "Evaluation\nThere are 60 points for class participation in the final grade calculation. Dr.¬†Yu is the only one who evaluates your participation. Your goal is to convince Dr.¬†Yu that you understand your work well and get engaged in the class activity.\n\nMSSC 5780\nFor MSSC 5780 students, your class activity grade is determined by\n\noral presentation\npresentation material\nyour non-Yes-No questions\ncorrectness of your answers to MATH 4780 students\n5-minute time management\n\n\n\nMATH 4780\nFor MATH 4780 students, your class activity grade is determined by\n\nquality of your non-Yes-No questions\nthe willingness to answer presenters‚Äô questions\ncorrectness of your answers to the questions asked by MSSC 5780 presenters"
  },
  {
    "objectID": "computing-access.html",
    "href": "computing-access.html",
    "title": "Computing access",
    "section": "",
    "text": "To access computing resources for the introductory data science courses offered by the Duke University Department of Statistical Science, go to the Duke Container Manager website, cmgr.oit.duke.edu/containers.\nIf this is your first time accessing the containers, click on reserve STA210 on the Reservations available menu on the right. You only need to do this once, and when you do, you‚Äôll see this container moved to the My reservations menu on the left.\nNext, click on STA210 under My reservations to access the RStudio instance you‚Äôll use for the course."
  },
  {
    "objectID": "course-overview.html",
    "href": "course-overview.html",
    "title": "MATH 4780: Regression Analysis",
    "section": "",
    "text": "Starting from simple linear regression, this course provides a brief, yet complete foundation for the understanding of basic regression theory and applications that is a core of supervised learning in statistical machine learning. Topics include multiple linear regression, diagnostic analysis, collinearity, nonparametric regression, variable selection, generalized linear models and other selected topics. This course focus on data analysis rather than mathematical derivation. For advanced regression topics, consider MSSC 6250 Statistical Machine Learning."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "MATH 4780 - Regression Analysis",
    "section": "",
    "text": "This page contains an outline of the topics, content, and assignments for the semester. Note that this schedule will be updated as the semester progresses, with all changes documented here.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWeek\nDate\nTopic\nTo Do\nSlides\nExercise\nHW\nProject\n\n\n\n\n1\nTue, Aug 29\nGreetings + Overview of Regression\nüìñ\nüñ•Ô∏è üñ•Ô∏è\n\n\n\n\n\n\nThu, Aug 31\nProbability and Statistics Review\n\nüñ•Ô∏è\nüìã\n‚úçÔ∏è\n\n\n\n2\nTue, Sep 5\nSimple Linear Regression (SLR)\nüìñ\nüñ•Ô∏è\n\n\n\n\n\n\nThu, Sep 7\nSimple Linear Regression\n\n\n\n\n\n\n\n3\nTue, Sep 12\nSimple Linear Regression\nüìñ\n\nüìã\n\n\n\n\n\nThu, Sep 14\nSimple Linear Regression\n\n\n\n‚úçÔ∏è\n\n\n\n4\nTue, Sep 19\nMultiple Linear Regression (MLR)\nüìñ\nüñ•Ô∏è\n\n\n\n\n\n\nThu, Sep 21\nMultiple Linear Regression\n\n\n\n\n\n\n\n5\nTue, Sep 26\nMultiple Linear Regression\nüìñ\n\nüìã\n\n\n\n\n\nThu, Sep 28\nMultiple Linear Regression\n\n\n\n‚úçÔ∏è\n\n\n\n6\nTue, Oct 3\nMatrix Algebra Review\nüìñ\nüñ•Ô∏è\n\n\n\n\n\n\nThu, Oct 5\nMLR in Matrix Form (MSSC only)\n\nüñ•Ô∏è\n\n\n\n\n\n7\nTue, Oct 10\nDiagnostics: Residuals and Unusual Data\nüìñ\nüñ•Ô∏è\n\n\n\n\n\n\nThu, Oct 12\nDiagnostics: Symmetry and Normality\n\nüñ•Ô∏è\n\n\n\n\n\n8\nTue, Oct 17\nIn-Class Exam ‚úÖ\nüìñ\n\n\n\n\n\n\n\nThu, Oct 19\nDiagnostics: Constant Variance\n\nüñ•Ô∏è\n\n‚úçÔ∏è\n\n\n\n9\nTue, Oct 24\nDiagnostics: Linearity\nüìñ\nüñ•Ô∏è\nüìã\n\n\n\n\n\nThu, Oct 26\nDiagnostics: Lack of Fit\n\n\n\n\n\n\n\n10\nTue, Oct 31\nBootstrapping and Simulation-based Inference\nüìñ\nüñ•Ô∏è\nüìã\n\nüìÇ\n\n\n\nThu, Nov 2\nPolynomial Regression\n\nüñ•Ô∏è\nüìã\n\n\n\n\n11\nTue, Nov 7\nNonparametric Regression\nüìñ\nüñ•Ô∏è\nüìã\n\n\n\n\n\nThu, Nov 9\nCategorical Variables\n\nüñ•Ô∏è\n\n‚úçÔ∏è\n\n\n\n12\nTue, Nov 14\nCategorical Variables\nüìñ\n\nüìã\n\nüìÇ\n\n\n\nThu, Nov 16\nCollinearity\n\nüñ•Ô∏è\nüìã\n\n\n\n\n13\nTue, Nov 21\nCollinearity\nüìñ\n\n\n\n\n\n\n\nThu, Nov 23\nNO CLASS: Thanksgiving\n\n\n\n\n\n\n\n14\nTue, Nov 28\nVariable Selection\nüìñ\nüñ•Ô∏è\nüìã\n\nüìÇ\n\n\n\nThu, Nov 30\nVariable Selection\n\n\n\n‚úçÔ∏è\n\n\n\n15\nTue, Dec 5\nLogistic Regression\nüìñ\nüñ•Ô∏è\nüìã\n\n\n\n\n\nThu, Dec 7\nLogistic Regression\n\n\n\n\n\n\n\n16\nTue, Dec 12\nProject Presentation\n\n\n\n\nüìÇ\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nI reserve the right to make changes to the schedule."
  },
  {
    "objectID": "course-team.html",
    "href": "course-team.html",
    "title": "Teaching team",
    "section": "",
    "text": "Dr.¬†Mine √áetinkaya-Rundel (she/her) is Professor of the Practice and Director of Undergraduate Studies at the Department of Statistical Science at Duke University. Mine‚Äôs work focuses on innovation in statistics and data science pedagogy, with an emphasis on computing, reproducible research, student-centered learning, and open-source education as well as pedagogical approaches for enhancing retention of women and under-represented minorities in STEM. Mine also works with RStudio as a Developer Educator.\n\n\n\nOffice hours\nLocation\n\n\n\n\nMondays 10:30 am - 11:30 am\nZoom\n\n\nThursdays 11:00 am - 12:00 pm\nOld Chem 213\n\n\n\nIf these times don‚Äôt work for you or you‚Äôd like to schedule a one-on-one meeting, you can do so at bit.ly/meet-mine."
  },
  {
    "objectID": "course-team.html#teaching-assistants",
    "href": "course-team.html#teaching-assistants",
    "title": "Teaching team",
    "section": "Teaching assistants",
    "text": "Teaching assistants\n\n\n\n\nName\nOffice hours\nLocation\n\n\n\n\n\nMartha Aboagye\nThursday 7 - 9 pm\nZoom\n\n\n\nRichard Fremgen\nLead TA for Section 3 (5:15 pm)\nTues 7 - 9 pm\nZoom\n\n\n\nEmily Gentles\nLead TA for Section 1 (1:45pm)\nWednesday 1 - 3 pm\nOld Chem 203B (On Zoom until January 18)\n\n\n\nSara Mehta\nMon 2 - 4 pm\nZoom\n\n\n\nRick Presman\nHead TA\nLead TA for Section 2 (3:30pm)\nThursday 3:30 - 4:30 pm\nFri 9 - 10 am\nZoom\n\n\n\nShari Tian\nTue 8 - 10 am\nZoom\n\n\n\nAaditya Warrier\nMon 12pm - 1pm\nFri 1:30 - 2:30 pm\nZoom"
  },
  {
    "objectID": "slides/14-colliearity.html#what-is-collinearity",
    "href": "slides/14-colliearity.html#what-is-collinearity",
    "title": "Collinearity",
    "section": "What is Collinearity",
    "text": "What is Collinearity\n\n\nCollinearity refers to the situation in which two or more predictors are closely related to one another.\n\nlimit and age appear to have no obvious relationship, which is good!\n\nlimit and rating are highly correlated, and they are said to be collinear.\n\n\n\nCollinearity is a fundamental problem with the data rather than model specification.\nThere is usually no satisfactory solution for a true collinearity problem.\nCollinearity means some regressors in your model are highly correlated.\nWe want to have regressors that are NOT moving with each other.\nIdeally we desire to have orthogonal regressors.\nThe intuition is, we hope predictors can explain the variation of \\(y\\), right? That‚Äôs why we put these predictors in the model.\nAnd ideally we hope each predictor can explain a part of variation of \\(y\\) that can only be explained by that predictor and cannot be explained by any other predictors.\nThis kind of partition can be done if all the predictors are orthogonal.\nIf x1 and x2 are highly correlated, meaning that they are moving together, then the two predictors are gonna explain a large part of the same variation of of \\(y\\).\nThink about it. If x1 and y are correlated in some way, and x1 and x2 are highly correlated, it means that x2 and y are going to be correlated in same way as x1 and y.\nSo we actually use two predictors to explain the same variation of \\(y\\), which is redundant.\nThe model will be confused and may not be able to understand this explained variation is due to x1 or due to x2.\nLater, we‚Äôll see why we don‚Äôt want the predictors to be correlated.\nThere are lots of bad effects on our regression model."
  },
  {
    "objectID": "slides/14-colliearity.html#sources-of-collinearity",
    "href": "slides/14-colliearity.html#sources-of-collinearity",
    "title": "Collinearity",
    "section": "Sources of Collinearity",
    "text": "Sources of Collinearity\nFour primary sources\n\nThe data collection method employed\nConstraints on the model or in the population\nModel specification\nA model with \\(p>n\\)\n\n\n\nLet‚Äôs see what causes Collinearity. Here are 4 sources of Collinearity.\nThe data collection method employed: the way we collect our data may cause the predictors to be moving together. We need to be more careful, and see if we can collect our data in another way, so that the correlation between predictors can be alleviated or weakened.\nConstraints on the model or in the population: If two predictors are correlated in nature, then it is unavoidable to have Collinearity, and if we want to keep the two predictors in our model, we should consider other methods other than OLS.\nModel specification: Polynomial regression.\nA \\(p>n\\) model"
  },
  {
    "objectID": "slides/14-colliearity.html#data-collection",
    "href": "slides/14-colliearity.html#data-collection",
    "title": "Collinearity",
    "section": "Data Collection",
    "text": "Data Collection\n\nCollinearity occurs when only a subspace of the entire sample space has been explored.\nMay be able to reduce this collinearity through the sampling technique used.\n\nThere is no physical reason why you can‚Äôt sample in that area."
  },
  {
    "objectID": "slides/14-colliearity.html#constraints",
    "href": "slides/14-colliearity.html#constraints",
    "title": "Collinearity",
    "section": "Constraints",
    "text": "Constraints\n\nPhysical constraints are present, and the collinearity will exist regardless of collection method."
  },
  {
    "objectID": "slides/14-colliearity.html#model-specification",
    "href": "slides/14-colliearity.html#model-specification",
    "title": "Collinearity",
    "section": "Model Specification",
    "text": "Model Specification\n\nPolynomial terms can cause ill-conditioning in \\({\\bf X'X}\\).\nAs the order of the model increases, \\({\\bf X'X}\\) matrix inversion will become inaccurate, and error can be introduced into the parameter estimates.\nIf range on a regressor variable is small, adding an \\(x^2\\) term can result in significant collinearity.\n\n\\(x\\) and \\(x^2\\) are highly (linearly correlated) - Polynomial terms can cause ill-conditioning (a sign of Collinearity) in the \\(({\\bf X'X})\\) matrix. - As the order of the model increases, \\(({\\bf X'X})\\) matrix inversion will become more inaccurate, and error can be introduced into the parameter estimates - If range on a regressor variable is small, adding \\(x^2\\) term can result in significant Collinearity."
  },
  {
    "objectID": "slides/14-colliearity.html#pn-model",
    "href": "slides/14-colliearity.html#pn-model",
    "title": "Collinearity",
    "section": "\n\\(p>n\\) Model",
    "text": "\\(p>n\\) Model\n\nMore regressor variables than observations.\nThe best way to counter this is to remove/reconstruct regressor variables.\n\nPrincipal Component Regression\nVariable Selection (Next Topic)"
  },
  {
    "objectID": "slides/14-colliearity.html#effect-of-collinearity",
    "href": "slides/14-colliearity.html#effect-of-collinearity",
    "title": "Collinearity",
    "section": "Effect of Collinearity",
    "text": "Effect of Collinearity\n\nüëâ Large variances and covariances for the LSE \\(b_j\\)s.\n\n\nüëâ Tends to produce LSE \\(b_j\\) that are too large in absolute value. Therefore, the vector \\({\\bf b}\\), on average, is much longer than the vector \\(\\boldsymbol \\beta\\).\n\n\n\nLarge variances and large magnitude of coefficients lead to instable and wrong signed coefficients.\nPoor coefficients do not necessarily imply bad fit or poor prediction.\nThe predictions should be confined to the \\(x\\) space where the collinearity holds approximately.\nCollinearity causes very poor extrapolated prediction.\n\n\n\nThe squared distance from \\({\\bf b}\\) to \\(\\bsbeta\\) is \\(D^2 = \\sum_{j=1}^k(b_j - \\beta_j)^2\\).\nThe expected squared distance is \\[E(D^2) = E\\left(\\sum_{j=1}^k(b_j - \\beta_j)^2 \\right) = \\sum_{j=1}^kE\\left[(b_j - \\beta_j)^2\\right] = \\sum_{j=1}^k\\mathrm{Var}(b_j) = \\sigma^2 \\mathrm{tr}\\left[({\\bf X'X})^{-1}\\right]\\]\n\nStrong Collinearity between \\(x\\)‚Äôs results in\nthe trace of a matrix (Tr) is the sum of the diagonal elements."
  },
  {
    "objectID": "slides/14-colliearity.html#perfectly-correlated-regressors",
    "href": "slides/14-colliearity.html#perfectly-correlated-regressors",
    "title": "Collinearity",
    "section": "Perfectly Correlated Regressors",
    "text": "Perfectly Correlated Regressors\n\nSuppose the true population regression equation is \\(y = 3 + 4x\\).\nSuppose we try estimating that equation using perfected correlated variables \\(x\\) and \\(z = x/10\\).\n\n\\[\n\\begin{aligned}\\hat{y}&= \\hat{\\beta}_0 + \\hat{\\beta}_1x  + \\hat{\\beta}_2z\\\\\n&= \\hat{\\beta}_0 + \\hat{\\beta}_1x  + \\hat{\\beta}_2\\frac{x}{10}\\\\\n&= \\hat{\\beta}_0 + \\bigg(\\hat{\\beta}_1 + \\frac{\\hat{\\beta}_2}{10}\\bigg)x\n\\end{aligned}\n\\] \n\n\nWe can set \\(\\hat{\\beta}_1\\) and \\(\\hat{\\beta}_2\\) to any two numbers such that \\(\\hat{\\beta}_1 + \\frac{\\hat{\\beta}_2}{10} = 4\\).\nWe are unable to choose the ‚Äúbest‚Äù combination of \\(\\hat{\\beta}_1\\) and \\(\\hat{\\beta}_2\\).\n\n\nCollinearity is a fundamental problem with the data rather than model specification.\nThere is usually no satisfactory solution for a true collinearity problem.\nCollinearity means some regressors in your model are highly correlated.\nWe want to have regressors that are NOT moving with each other.\nIdeally we desire to have orthogonal regressors.\nThe intuition is, we hope predictors can explain the variation of \\(y\\), right? That‚Äôs why we put these predictors in the model.\nAnd ideally we hope each predictor can explain a part of variation of \\(y\\) that can only be explained by that predictor and cannot be explained by any other predictors.\nThis kind of partition can be done if all the predictors are orthogonal.\nIf x1 and x2 are highly correlated, meaning that they are moving together, then the two predictors are gonna explain a large part of the same variation of of \\(y\\).\nThink about it. If x1 and y are correlated in some way, and x1 and x2 are highly correlated, it means that x2 and y are going to be correlated in same way as x1 and y.\nSo we actually use two predictors to explain the same variation of \\(y\\), which is redundant.\nThe model will be confused and may not be able to understand this explained variation is due to x1 or due to x2.\nLater, we‚Äôll see why we don‚Äôt want the predictors to be correlated.\nThere are lots of bad effects on our regression model."
  },
  {
    "objectID": "slides/14-colliearity.html#collinearity-diagnostics",
    "href": "slides/14-colliearity.html#collinearity-diagnostics",
    "title": "Collinearity",
    "section": "Collinearity Diagnostics",
    "text": "Collinearity Diagnostics\nIdeal characteristics of a collinearity diagnostic:\n\nCorrectly indicate if collinearity is present\nHow severe the problem is\nProvide insight as to which regressors are causing the problem"
  },
  {
    "objectID": "slides/14-colliearity.html#examination-of-the-correlation-matrix-of-xs",
    "href": "slides/14-colliearity.html#examination-of-the-correlation-matrix-of-xs",
    "title": "Collinearity",
    "section": "Examination of the Correlation Matrix of \\(x\\)s",
    "text": "Examination of the Correlation Matrix of \\(x\\)s\n\nAfter unit length scaling, \\({\\bf X'X} = \\left[r_{ij}\\right]_{k\\times k}\\) is the correlation matrix of \\(x\\)s denoted as \\({\\bf \\Sigma}\\). 1 For example, \\[{\\bf X'X} = \\begin{bmatrix} 1 & 0.992 \\\\ 0.992 & 1 \\end{bmatrix}\\]\n\\(r_{ij}\\) is the pairwise correlation between \\(x_i\\) and \\(x_j\\).\nLarge \\(|r_{ij}|\\) is an indication of collinearity.\nWhen more than two regressors are involved in collinearity, there may be instances when collinearity is present, but the pairwise correlations are not large.\nInspecting \\(r_{ij}\\) is not sufficient for detecting more complex collinearity.\n\nAfter unit length scaling, \\({\\bf X'X}\\) is the correlation matrix of regressors. - If we scale and center the regressors, we have the correlation matrix. \\[{\\bf X'X} = \\begin{bmatrix} 1 & 0.992 \\\\ 0.992 & 1 \\end{bmatrix}\\] - The off diagonal elements of the centered and scaled \\({\\bf X'X}\\) matrix are the pairwise correlations between \\(x_i\\) and \\(x_j\\), denoted as \\(r_{ij}\\). For example, \\(r_{12} = 0.992\\). - If \\(|r_{ij}| \\approx 1\\), there is an indication of Collinearity. But, the opposite does not always hold. - When there are more than two regressors, there may be instances when Collinearity is present, but the pairwise correlations do not indicate a problem. (Webster, Gunst, and Mason [1974] Table 9.4) - Inspection of the \\(r_{ij}\\) is not sufficient for detecting anything more complex than pairwise Collinearity.\nThe term correlation is a bit of misnomer. The regressors are not random variables. The correlation coefficient \\(r_{ij}\\) does measure linear dependency between \\(x_i\\) and \\(x_j\\) in the data."
  },
  {
    "objectID": "slides/14-colliearity.html#variance-inflation-factors",
    "href": "slides/14-colliearity.html#variance-inflation-factors",
    "title": "Collinearity",
    "section": "Variance Inflation Factors",
    "text": "Variance Inflation Factors\n\n\nThe diagonals of \\({\\bf \\Sigma}^{-1} = {\\bf C}\\) in correlation form are called variance inflation factors \\[\\text{VIF}_j = {\\bf C}_{jj}\\]\nExample: \\[{\\bf \\Sigma} = \\begin{bmatrix} 1 & 0.992 \\\\ 0.992 & 1 \\end{bmatrix}; \\quad {\\bf \\Sigma}^{-1} = \\begin{bmatrix} 62.8 & -62.2 \\\\ -62.2 & 62.8 \\end{bmatrix}\\] and \\(\\text{VIF}_1 = 62.8\\).\n\n\n\nThe collinearity produces an inflation in the variances of the estimated coefficients, an increase in 60-fold over the ideal case when the two regressors are orthogonal.\nVIFs \\(> 10\\) are considered significant."
  },
  {
    "objectID": "slides/14-colliearity.html#variance-inflation-factors-1",
    "href": "slides/14-colliearity.html#variance-inflation-factors-1",
    "title": "Collinearity",
    "section": "Variance Inflation Factors",
    "text": "Variance Inflation Factors\n\\[\\text{VIF}_j = \\frac{1}{1 - R^2_{X_j | X_{-j}}}\\] where \\(R^2_{X_j | X_{-j}}\\) is the coefficient of determination obtained when \\(x_j\\) is regressed on the other regressors \\(x_i, i \\ne j\\).\n\\[\\mathrm{Var}(b_j) = \\frac{s^2}{\\sum_{i=1}^n(x_{ij} - \\bar{x}_j)^2} \\times \\text{VIF}_j\\]\n\n\\(\\text{VIF}_j\\) measures the combined effect of the dependencies among the regressors on the variance of \\(b_j\\).\nIf \\(x_j\\) is near linearly dependent on some subset of the remaining regressors, \\({\\bf C}_{jj}\\) is large.\n\n\nRemember what does linear (in)dependence mean?\n\n\\[\\text{VIF}_j = {\\bf C}_{jj} = \\frac{1}{1 - R^2_j}\\] + \\(R_j^2\\): the coefficient of determination obtained when \\(x_j\\) is regressed on the remaining regressors \\(x_i, i \\ne j\\). - If \\(x_j\\) can be explained a lot from other regressors, \\(x_j\\) is probably unnecessary in the regression model when all others are in the model. When it is in the model, the model cannot understand what the real effect the predictor can provide, and therefore its coefficient has a large variance. - The regressors that have high VIFs probably have poorly estimated coefficients. - The Collinearity produces an inflation in the variances of the estimated coefficients, an increase in 60-fold over the ideal case when the two regressors are orthogonal. - Since the variance of the j th regression coefficients is C jj œÉ 2 , we can view C jj as the factor by which the variance of ÀÜŒ≤j is increased due to near - linear dependences among the regressors."
  },
  {
    "objectID": "slides/14-colliearity.html#r-lab-hospital-manpower---data",
    "href": "slides/14-colliearity.html#r-lab-hospital-manpower---data",
    "title": "Collinearity",
    "section": "\nR Lab Hospital Manpower - Data",
    "text": "R Lab Hospital Manpower - Data\n\n\n\nmanpower\n\n       y  x1    x2    x3    x4   x5\n1    567  16  2463   473  18.0  4.5\n2    697  44  2048  1340   9.5  6.9\n3   1033  20  3940   620  12.8  4.3\n4   1604  19  6505   568  36.7  3.9\n5   1611  49  5723  1498  35.7  5.5\n6   1613  45 11520  1366  24.0  4.6\n7   1854  55  5779  1687  43.3  5.6\n8   2161  59  5969  1640  46.7  5.2\n9   2306  94  8461  2872  78.7  6.2\n10  3504 128 20106  3655 180.5  6.2\n11  3572  96 13313  2912  60.9  5.9\n12  3741 131 10771  3921 103.7  4.9\n13  4027 127 15543  3866 126.8  5.5\n14 10344 253 36194  7684 157.7  7.0\n15 11732 409 34703 12446 169.4 10.8\n16 15415 464 39204 14098 331.4  7.0\n17 18854 510 86533 15524 371.6  6.3\n\n\n\n\\(y\\): Monthly man-hours\n\n\\(x_1\\): Average daily patient load\n\n\\(x_2\\): Monthly X-ray exposures\n\n\\(x_3\\): Monthly occupied bed days\n\n\\(x_4\\): Eligible population in the area / 1000\n\n\\(x_5\\): Average length of patients‚Äô stay in days\n\n\n\nDo you expect to see positive or negative relationship between \\(y\\) and \\(x_i\\)?"
  },
  {
    "objectID": "slides/14-colliearity.html#r-lab-hospital-manpower---pairwise-dependence",
    "href": "slides/14-colliearity.html#r-lab-hospital-manpower---pairwise-dependence",
    "title": "Collinearity",
    "section": "\nR Lab Hospital Manpower - Pairwise Dependence",
    "text": "R Lab Hospital Manpower - Pairwise Dependence"
  },
  {
    "objectID": "slides/14-colliearity.html#r-lab-hospital-manpower---model-fit",
    "href": "slides/14-colliearity.html#r-lab-hospital-manpower---model-fit",
    "title": "Collinearity",
    "section": "\nR Lab Hospital Manpower - Model Fit",
    "text": "R Lab Hospital Manpower - Model Fit\n\nlm_full <- lm(y ~ ., data = manpower)\n(summ_full <- summary(lm_full))\n\n...\n             Estimate Std. Error t value Pr(>|t|)  \n(Intercept) 1962.9482  1071.3617    1.83    0.094 .\nx1           -15.8517    97.6530   -0.16    0.874  \nx2             0.0559     0.0213    2.63    0.023 *\nx3             1.5896     3.0921    0.51    0.617  \nx4            -4.2187     7.1766   -0.59    0.569  \nx5          -394.3141   209.6395   -1.88    0.087 .\nResidual standard error: 642 on 11 degrees of freedom\nMultiple R-squared:  0.991, Adjusted R-squared:  0.987 \n...\n\n\n\nExcellent fit. But any issues of the this fitted result?\n\n\n\nThe coefficients \\(b_1\\), \\(b_4\\) and \\(b_5\\) are negative.\n\nIn the case of \\(x_1\\), an increase in patient load, when other \\(x\\)‚Äôs are held constant, corresponds to a decrease in hospital manpower. (wrong sign due to large variance)\n\n\nEven though the regression model fits the data quite well, the rather curious signs on the regression coefficients may be the result of the effect of Collinearity."
  },
  {
    "objectID": "slides/14-colliearity.html#r-lab-hospital-manpower---vif",
    "href": "slides/14-colliearity.html#r-lab-hospital-manpower---vif",
    "title": "Collinearity",
    "section": "\nR Lab Hospital Manpower - VIF",
    "text": "R Lab Hospital Manpower - VIF\n\n\n\nX <- manpower[, -1]\n(Sig <- cor(X))\n\n     x1   x2   x3   x4   x5\nx1 1.00 0.91 1.00 0.94 0.67\nx2 0.91 1.00 0.91 0.91 0.45\nx3 1.00 0.91 1.00 0.93 0.67\nx4 0.94 0.91 0.93 1.00 0.46\nx5 0.67 0.45 0.67 0.46 1.00\n\n\n\n\n(C <- solve(Sig))\n\n      x1    x2    x3     x4    x5\nx1  9598  11.9 -9247 -318.8 -93.9\nx2    12   7.9   -18   -1.9   1.8\nx3 -9247 -18.5  8933  294.4  83.4\nx4  -319  -1.9   294   23.3   6.4\nx5   -94   1.8    83    6.4   4.3\n\n## VIF \ndiag(C)\n\n    x1     x2     x3     x4     x5 \n9597.6    7.9 8933.1   23.3    4.3 \n\n\n\n\n\n\n## put the fitted model in vif()\n(vif_all <- car::vif(lm_full))\n\n    x1     x2     x3     x4     x5 \n9597.6    7.9 8933.1   23.3    4.3 \n\n\n\\(x_1\\) Average daily patient load and \\(x_3\\) Monthly occupied bed days are highly correlated."
  },
  {
    "objectID": "slides/14-colliearity.html#r-lab-hospital-manpower---confidence-interval",
    "href": "slides/14-colliearity.html#r-lab-hospital-manpower---confidence-interval",
    "title": "Collinearity",
    "section": "\nR Lab Hospital Manpower - Confidence Interval",
    "text": "R Lab Hospital Manpower - Confidence Interval\n\n\n\n\n\n\n\n\n\n\n\nMarginally, \\(b_1\\) and \\(b_3\\) vary a lot. The CI for \\(\\beta_1\\) and CI for \\(\\beta_3\\) both contain zero.\n\n\n\n    2.5 % 97.5 %\nx1 -230.8  199.1\nx3   -5.2    8.4\n\n\n\n\n\n\n\n\n\n\n\n\n\nBut very confident that \\(\\beta_1\\) and \\(\\beta_3\\) cannot be both zero.\n\n\n\n\n\nThe model is quite not sure how much \\(x_1\\) and \\(x_3\\) affect \\(y\\)."
  },
  {
    "objectID": "slides/14-colliearity.html#r-lab-hospital-manpower---confidence-interval-1",
    "href": "slides/14-colliearity.html#r-lab-hospital-manpower---confidence-interval-1",
    "title": "Collinearity",
    "section": "\nR Lab Hospital Manpower - Confidence Interval",
    "text": "R Lab Hospital Manpower - Confidence Interval\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\(x_4\\) and \\(x_5\\) are not highly correlated pairwisely. \\(r_{45} = 0.46\\).\nHowever, their CI is still inflated due to the collinearity effect of other variables.\n\n\n\n   2.5 % 97.5 %\nx4   -20     12\nx5  -856     67\n\n\nDon‚Äôt just look at the pairwise correlation"
  },
  {
    "objectID": "slides/14-colliearity.html#eigensystem-analysis-condition-indices",
    "href": "slides/14-colliearity.html#eigensystem-analysis-condition-indices",
    "title": "Collinearity",
    "section": "Eigensystem Analysis: Condition Indices",
    "text": "Eigensystem Analysis: Condition Indices\n\nThe eigenvalues of \\({\\bf \\Sigma}\\) (the correlation matrix of \\(\\bf X\\)), \\(\\lambda_1, \\lambda_2, \\dots, \\lambda_k\\), can measure collinearity.\n\nIf there are one or more near-linear dependencies, one or more of the \\(\\lambda_i\\)s will be (relatively) small.   \n\n\nCondition indices of \\({\\bf \\Sigma}\\) are \\(\\kappa_j = \\frac{\\lambda_{max}}{\\lambda_{j}}\\).\nThe number of \\(\\kappa_j > 1000\\) is a measure of the number of near-linear dependencies in \\({\\bf \\Sigma}.\\)\n\n\n\nThe eigenvalues of \\({\\bf X'X}\\), \\(\\lambda_1, \\lambda_2, \\dots, \\lambda_k\\), can measure Collinearity.\n\nIf there are one or more near-linear dependencies in the data, one or more of the \\(\\lambda_i\\)s will be small.\n\nCondition number of \\({\\bf X'X}\\) is \\(\\kappa = \\frac{\\lambda_{max}}{\\lambda_{min}}\\).\n\n\\(\\kappa > 100\\) implies Collinearity.\n\n\\(\\kappa\\) does not tell us how many regressors are involved.\n\nCondition indices of \\({\\bf X'X}\\) is \\(\\kappa_j = \\frac{\\lambda_{max}}{\\lambda_{j}}\\).\nThe number of \\(\\kappa_j > 1000\\) is a measure of the number of near-linear dependencies in \\({\\bf X'X}.\\)"
  },
  {
    "objectID": "slides/14-colliearity.html#r-lab-hospital-manpower---eigensystem-analysis",
    "href": "slides/14-colliearity.html#r-lab-hospital-manpower---eigensystem-analysis",
    "title": "Collinearity",
    "section": "\nR Lab Hospital Manpower - Eigensystem Analysis",
    "text": "R Lab Hospital Manpower - Eigensystem Analysis\n\neigen_Sig <- eigen(Sig)\n## eigenvalues\n(lambda <- eigen_Sig$values)\n\n[1] 4.2e+00 6.7e-01 9.5e-02 4.1e-02 5.4e-05\n\n## Conditional indices\nmax(lambda) / lambda\n\n[1]     1.0     6.3    44.4   103.1 77769.7\n\n\n\n\n\\(\\lambda_5 \\approx 0\\) and \\(\\kappa_5 \\approx 77770\\), indicating collinearity.\n\n\nEigenvalues are listed in a decreasing order, \\(\\lambda_1 > \\lambda_2 > \\cdots > \\lambda_k\\), and \\(\\lambda_5\\) is not the eigenvalue of \\(x_5\\).\n\n\n\n      [,1]     [,2]  [,3]  [,4]    [,5]\n[1,] -0.49 -0.00203 -0.17  0.47  0.7195\n[2,] -0.45 -0.33561  0.80 -0.19  0.0012\n[3,] -0.48 -0.00085 -0.15  0.51 -0.6941\n[4,] -0.46 -0.31080 -0.54 -0.63 -0.0234\n[5,] -0.33  0.88925  0.12 -0.29 -0.0068\n\n\n[1] 1 1 1 1 1\n\n\nThere is one near linear dependency."
  },
  {
    "objectID": "slides/14-colliearity.html#eigensystem-analysis-eigendecomposition",
    "href": "slides/14-colliearity.html#eigensystem-analysis-eigendecomposition",
    "title": "Collinearity",
    "section": "Eigensystem Analysis: Eigendecomposition",
    "text": "Eigensystem Analysis: Eigendecomposition\n\nEigendecomposition \\[{\\bf \\Sigma = V\\boldsymbol \\Lambda V'}\\]\n\n\n\\(\\boldsymbol \\Lambda\\) is a \\(k \\times k\\) diagonal matrix whose elements are \\(\\lambda_j\\).\n\n\\({\\bf V} = [{\\bf v}_1 \\quad {\\bf v}_2 \\quad \\dots \\quad {\\bf v}_k]\\) is a \\(k \\times k\\) orthogonal matrix whose columns are the eigenvectors of \\({\\bf \\Sigma}\\).\n\n\nIf \\(\\lambda_j \\approx 0\\), the associated \\({\\bf v}_j = (v_{1j}, v_{2j}, \\dots, v_{kj})'\\) describes how (and what) regressors are linearly dependent: \\[\\sum_{i=1}^kv_{ij}{\\bf x}_i \\cong \\mathbf{0}\\]\n\n\n\nIf \\(\\lambda_j \\approx 0\\), the associated \\({\\bf v}_j = (v_{1j}, v_{2j}, \\dots, v_{kj})'\\) describes the nature of this linear dependence.\n\n\\(\\sum_{i=1}^kc_i{\\bf x}_i \\cong \\mathbf{0}\\): The ‚Äúweights‚Äù \\(c_i\\) are the individual elements in the \\({\\bf v}_j\\)."
  },
  {
    "objectID": "slides/14-colliearity.html#r-lab-hospital-manpower---eigensystem-analysis-1",
    "href": "slides/14-colliearity.html#r-lab-hospital-manpower---eigensystem-analysis-1",
    "title": "Collinearity",
    "section": "\nR Lab Hospital Manpower - Eigensystem Analysis",
    "text": "R Lab Hospital Manpower - Eigensystem Analysis\n\n\n\n\n\n\n## eigenvector matrix\n(V <- eigen_Sig$vectors)\n\n      [,1]     [,2]  [,3]  [,4]    [,5]\n[1,] -0.49 -0.00203 -0.17  0.47  0.7195\n[2,] -0.45 -0.33561  0.80 -0.19  0.0012\n[3,] -0.48 -0.00085 -0.15  0.51 -0.6941\n[4,] -0.46 -0.31080 -0.54 -0.63 -0.0234\n[5,] -0.33  0.88925  0.12 -0.29 -0.0068\n\n\n\n\n\\({\\bf X}{\\bf v}_5 = \\sum_{i=1}^5v_{i5}{\\bf x}_i \\approx {\\bf 0}\\).\n\\(0.720 {\\bf x}_1 + 0.001 {\\bf x}_2 - 0.694 {\\bf x}_3 - 0.023 {\\bf x}_4 - 0.007 {\\bf x}_5 \\approx {\\bf 0}\\)\nHighly correlated \\(x_1\\) and \\(x_3\\) causes collinearity.\n\n\n\n\n\n\nX_s <- apply(X, 2, unit_len_scale)\nX_s %*% V[, 5]\n\n          [,1]\n [1,] -0.00037\n [2,] -0.00144\n [3,]  0.00032\n [4,] -0.00058\n [5,] -0.00108\n [6,]  0.00047\n [7,] -0.00131\n [8,]  0.00492\n [9,] -0.00225\n[10,]  0.00230\n[11,] -0.00050\n[12,]  0.00209\n[13,] -0.00251\n[14,] -0.00016\n[15,]  0.00131\n[16,] -0.00098\n[17,] -0.00023\n\n\n\n\n\n\n     [,1] [,2]  [,3]  [,4]    [,5]\n[1,]  4.2 0.00 0.000 0.000 0.0e+00\n[2,]  0.0 0.67 0.000 0.000 0.0e+00\n[3,]  0.0 0.00 0.095 0.000 0.0e+00\n[4,]  0.0 0.00 0.000 0.041 0.0e+00\n[5,]  0.0 0.00 0.000 0.000 5.4e-05\n\n\n\n## Orthogonal matrix\nround(t(V)%*%V, 1)\n\n     [,1] [,2] [,3] [,4] [,5]\n[1,]    1    0    0    0    0\n[2,]    0    1    0    0    0\n[3,]    0    0    1    0    0\n[4,]    0    0    0    1    0\n[5,]    0    0    0    0    1\n\n##  V(Lambda)V' = X'X\nV %*% diag(lambda) %*% t(V)\n\n     [,1] [,2] [,3] [,4] [,5]\n[1,] 1.00 0.91 1.00 0.94 0.67\n[2,] 0.91 1.00 0.91 0.91 0.45\n[3,] 1.00 0.91 1.00 0.93 0.67\n[4,] 0.94 0.91 0.93 1.00 0.46\n[5,] 0.67 0.45 0.67 0.46 1.00\n\n\n\n## near-zero vector\nX_s%*%V[, 5]\n\n          [,1]\n [1,] -0.00037\n [2,] -0.00144\n [3,]  0.00032\n [4,] -0.00058\n [5,] -0.00108\n [6,]  0.00047\n [7,] -0.00131\n [8,]  0.00492\n [9,] -0.00225\n[10,]  0.00230\n[11,] -0.00050\n[12,]  0.00209\n[13,] -0.00251\n[14,] -0.00016\n[15,]  0.00131\n[16,] -0.00098\n[17,] -0.00023"
  },
  {
    "objectID": "slides/14-colliearity.html#eigensystem-analysis-variance-proportion",
    "href": "slides/14-colliearity.html#eigensystem-analysis-variance-proportion",
    "title": "Collinearity",
    "section": "Eigensystem Analysis: Variance Proportion",
    "text": "Eigensystem Analysis: Variance Proportion\n\n\n\n\n\n\\(\\text{VIF}_j = \\sum_{i=1}^k\\frac{v_{ji}^2}{\\lambda_i}\\) \n\n\n\\(\\pi_{ij} = \\frac{v_{ji}^2/\\lambda_i}{\\text{VIF}_j}\\) is the variance decomposition proportion that measures the proportion of the variance of \\(b_j\\) contributed by \\(\\lambda_i\\).\n\n\\(\\pi_{ij} > 0.5\\) indicates collinearity.\n\n\n\nA small \\(\\lambda_i\\), accompanied by a subset of regressors with high variance proportions \\(\\pi_{ij}\\) represents a dependency involving the regressors in that subset.\nIf \\(\\pi_{32}\\) and \\(\\pi_{34}\\) are large, \\(\\lambda_3\\) is associated with a collinearity that inflats the variance of \\(b_2\\) and \\(b_4\\).\n\n\n\n\n\\({\\bf X'X = V\\bsLambda V'}\\), therefore \\({\\bf (X'X)^{-1} = V\\bsLambda^{-1} V'}\\)\n\n\\(\\mathrm{Var}\\left( {\\bf b} \\right) = \\sigma^2 {\\bf (X'X)} ^{-1} = \\sigma^2 {\\bf V\\bsLambda^{-1} V'}\\)\n\\(\\mathrm{Var}\\left( b_j \\right) = \\sigma^2 \\sum_{i=1}^k\\frac{v_{ji}^2}{\\lambda_i} =\\sigma^2\\text{VIF}_j\\)\n\n\\(\\sum_{j=1}^k \\mathrm{Var}\\left( b_j \\right) =\\sigma^2 \\sum_{i=1}^k\\sum_{j=1}^k\\frac{v_{ji}^2}{\\lambda_i} = \\sigma^2\\sum_{i=1}^k\\frac{1}{\\lambda_i}\\) (length of \\({\\bf v}_j\\) is 1)\n\n\\(\\pi_{ij} = \\frac{v_{ji}^2/\\lambda_i}{\\text{VIF}_j}\\) is the variance decomposition proportion, which is attributed to (or blamed on) the colliearity characterized by the eigenvalue \\(\\lambda_i\\).\n\n\\(\\pi_{ij} > 0.5\\) indicates Collinearity.\n\nA small \\(\\lambda_i\\), accompanied by a subset of regressors with high variance proportions \\(\\pi_{ij}\\): + represents a dependency involving the regressors in that subset, and the dependency is damaging to the precision of estimation of the coefficients in the subset."
  },
  {
    "objectID": "slides/14-colliearity.html#r-lab-hospital-manpower---variance-proportion",
    "href": "slides/14-colliearity.html#r-lab-hospital-manpower---variance-proportion",
    "title": "Collinearity",
    "section": "\nR Lab Hospital Manpower - Variance Proportion",
    "text": "R Lab Hospital Manpower - Variance Proportion\n\n# Variance Proportion\nvar_prop <- function(V, lambda, vif_all) {\n    ## diag(1 / lambda) means divide by lambda row-wise\n    round(t(V ^ 2 %*% diag(1 / lambda)) %*% diag(1/vif_all), 5)\n}\nvar_prop(V = V, lambda = lambda, vif_all = vif_all)\n\n        [,1]   [,2]    [,3]   [,4]   [,5]\n[1,] 0.00001 0.0062 0.00001 0.0022 0.0062\n[2,] 0.00000 0.0213 0.00000 0.0062 0.2768\n[3,] 0.00003 0.8607 0.00003 0.1309 0.0328\n[4,] 0.00056 0.1087 0.00071 0.4237 0.4851\n[5,] 0.99940 0.0031 0.99925 0.4370 0.1991\n\n\n\n\n\\(\\pi_{51}\\) and \\(\\pi_{53}\\) are large, indicating that \\(\\lambda_5\\) is inflating the variance of \\(b_1\\) and \\(b_3\\).\n\n\n## diag(V (Lambda)^-1 V') = VIF\ndiag(V %*% diag(1/lambda) %*% t(V))\n\n[1] 9597.6    7.9 8933.1   23.3    4.3\n\nvif_all\n\n    x1     x2     x3     x4     x5 \n9597.6    7.9 8933.1   23.3    4.3 \n\n## sum(vif) = sum(1/lambda)\nsum(vif(lm_full))\n\n[1] 18566\n\nsum(1/lambda)\n\n[1] 18566"
  },
  {
    "objectID": "slides/14-colliearity.html#other-diagnostics",
    "href": "slides/14-colliearity.html#other-diagnostics",
    "title": "Collinearity",
    "section": "Other Diagnostics",
    "text": "Other Diagnostics\nCollinearity may exist if\n\noverall \\(F\\)-test for regression is significant, but individual \\(t\\)-tests are all non-significant.\n\nthe coefficient estimates are instable\n\nadding or removing a regressor produces large changes in the estimates\ndeleting one or more observations results in large changes in the estimates\nif the signs or magnitudes of the estimates are contrary to prior expectation"
  },
  {
    "objectID": "slides/14-colliearity.html#r-lab-hospital-manpower---significance",
    "href": "slides/14-colliearity.html#r-lab-hospital-manpower---significance",
    "title": "Collinearity",
    "section": "\nR Lab Hospital Manpower - Significance",
    "text": "R Lab Hospital Manpower - Significance\n\nsumm_full$coefficients  ## t-test not significant\n\n            Estimate Std. Error t value Pr(>|t|)\n(Intercept) 1962.948    1.1e+03    1.83    0.094\nx1           -15.852    9.8e+01   -0.16    0.874\nx2             0.056    2.1e-02    2.63    0.023\nx3             1.590    3.1e+00    0.51    0.617\nx4            -4.219    7.2e+00   -0.59    0.569\nx5          -394.314    2.1e+02   -1.88    0.087\n\n\n\nsumm_full$fstatistic  ## F-test significant\n\nvalue numdf dendf \n  238     5    11 \n\n\n\nlm_no_x3 <- lm(y~x1+x2+x4+x5, data = manpower)\nsummary(lm_no_x3)$coef\n\n            Estimate Std. Error t value Pr(>|t|)\n(Intercept) 2161.962    967.871     2.2  4.5e-02\nx1            34.284      4.897     7.0  1.4e-05\nx2             0.057      0.021     2.8  1.7e-02\nx4            -6.600      5.311    -1.2  2.4e-01\nx5          -440.297    183.696    -2.4  3.4e-02"
  },
  {
    "objectID": "slides/14-colliearity.html#diagonastics-summary",
    "href": "slides/14-colliearity.html#diagonastics-summary",
    "title": "Collinearity",
    "section": "Diagonastics Summary",
    "text": "Diagonastics Summary\n\nSummary:\n\nEigenvalue (or ratios): assess the seriousness of a particular dependency\nVariance proportion: signify what regressors are involved and to what extent\nVIF: determine the damage to the individual coefficient"
  },
  {
    "objectID": "slides/14-colliearity.html#methods-for-dealing-with-collinearity",
    "href": "slides/14-colliearity.html#methods-for-dealing-with-collinearity",
    "title": "Collinearity",
    "section": "Methods for Dealing with Collinearity",
    "text": "Methods for Dealing with Collinearity\nData collection:  Collect more data to break up the collinearity  in the existing data\n\n\nModel specification/An overdefined model: Respecify the model\n\n\n redefining the regressors : Use \\(x = x_1+x_2\\) or \\(x = x_1x_2\\).\n\nAvoid combining regressors in different units.\n\n\n\n eliminating regressors : remove \\(x_1\\) or \\(x_2\\).\n\nMay damage the predictive power if the removed regressors have significant explanatory power. (Variable selection)\nIf we remove \\(x_2\\), we estimate the marginal relationship between \\(y\\) and \\(x_1\\), ignoring \\(x_2\\), rather than the partial relationship conditioning on \\(x_2\\).\n\n\n\n\n\nConstraint on the model or in the population: Say goodbye to least-squares estimation.\n\n Ridge Regression, Principal Component Regression, Bayesian Regression, etc"
  },
  {
    "objectID": "slides/14-colliearity.html#unbiased-vs.-biased-estimators",
    "href": "slides/14-colliearity.html#unbiased-vs.-biased-estimators",
    "title": "Collinearity",
    "section": "Unbiased vs.¬†Biased Estimators",
    "text": "Unbiased vs.¬†Biased Estimators\n\nThe LSE \\({\\bf b}\\) is unbiased, i.e., \\(E({\\bf b}) = \\boldsymbol \\beta\\).\nLSE is a BLUE: has minimum variance among all unbiased linear estimators\nNo guarantee that this variance will be small.\nWhen collinearity exists, \\(\\mathrm{Var}({\\bf b})\\) is largely inflated.\n\n\nimplying that confidence intervals on Œ≤ would be wide and the point estimate ÀÜb is very unstable."
  },
  {
    "objectID": "slides/14-colliearity.html#biased-estimators",
    "href": "slides/14-colliearity.html#biased-estimators",
    "title": "Collinearity",
    "section": "Biased Estimators",
    "text": "Biased Estimators\n\n\nA good estimator is the one that balances bias and variance well, or the one that minimizes the mean square error \\[\\small \\text{MSE}(\\hat{\\beta}) = E[(\\hat{\\beta} - \\beta)^2] = \\mathrm{Var}(\\hat{\\beta}) + \\text{bias}(\\hat{\\beta})^2\\]\n\nBiasedness and variance have a trade-off relationship!\nFind a biased estimator \\({\\bf \\hat{b}}\\) that has smaller variance and MSE than \\({\\bf b}\\)."
  },
  {
    "objectID": "slides/14-colliearity.html#ridge-regression-estimator",
    "href": "slides/14-colliearity.html#ridge-regression-estimator",
    "title": "Collinearity",
    "section": "Ridge Regression Estimator",
    "text": "Ridge Regression Estimator\n\nWith a small \\(\\delta >0\\), the ridge estimator \\({\\bf b}_R\\) is \\[{\\bf b}_R = ({\\bf X'X + \\delta I})^{-1}{\\bf X'y}\\]\n\\({\\bf b}_R\\) is a biased estimator for \\(\\boldsymbol \\beta\\).\n\n\n\n\\(\\sum_{i=1}^k \\mathrm{Var}\\left( b_{Ri} \\right) \\le \\sum_{i=1}^k \\mathrm{Var}\\left( b_i \\right)\\) with equality when \\(\\delta = 0\\).\n\n\nAs \\(\\delta\\) increases,\n\n\n\\(\\sum_{i=1}^k \\mathrm{Var}\\left( b_{Ri} \\right)\\) decreases\nBias \\(E\\left[ {\\bf b}_R \\right] - \\boldsymbol \\beta\\) increases\n\n\\(R^2\\) decreases (it‚Äôs OK as we care more about stable estimates and better prediction)\n\n\n\n\n\n\nChoose \\(\\delta\\) by the ridge trace that is a plot of \\(\\{ b_{Ri} \\}_{i=1}^k\\) vs.¬†\\(\\delta\\).\n\nSelect a small value of \\(\\delta\\) at which the ridge estimates \\({\\bf b}_R\\) are stable.\n\n\n\n\nCross Validation\nThe ridge estimator \\({\\bf b}_R\\) is \\[{\\bf b}_R = ({\\bf X'X + \\delta I})^{-1}{\\bf X'y}\\]\n\n\n\\({\\bf b}_R\\) is a biased estimator of \\(\\bsbeta\\): \\[E\\left[ {\\bf b}_R \\right] = E\\left[({\\bf X'X + \\delta I})^{-1}{\\bf X'y} \\right]  = E\\left[({\\bf X'X + \\delta I})^{-1}({\\bf X'X}){\\bf b}\\right] = {\\bf Z_{\\delta}}\\bsbeta\\]\n\nThe covariance matrix of \\({\\bf b}_R\\) is \\[\\mathrm{Var}\\left({\\bf b}_R \\right) = \\sigma^2 ({\\bf X'X + \\delta I})^{-1}{\\bf X'X }({\\bf X'X + \\delta I})^{-1}\\]\n\nFor LSE, \\(\\sum_{i=1}^k \\mathrm{Var}\\left( b_i \\right) = \\sigma^2\\sum_{i=1}^k\\frac{1}{\\lambda_i}\\)\n\nFor ridge estimator, \\(\\sum_{i=1}^k \\mathrm{Var}\\left( b_{Ri} \\right) = \\sigma^2\\sum_{i=1}^k\\frac{\\lambda_i}{(\\lambda_i+\\delta)^2}\\)\n\n\n\\(\\sum_{i=1}^k \\mathrm{Var}\\left( b_{Ri} \\right) \\le \\sum_{i=1}^k \\mathrm{Var}\\left( b_i \\right)\\) with equality when \\(\\delta = 0\\).\nAs \\(\\delta\\) increases, \\(\\sum_{i=1}^k \\mathrm{Var}\\left( b_{Ri} \\right)\\) decreases."
  },
  {
    "objectID": "slides/14-colliearity.html#r-lab-hospital-manpower---ridge-regression",
    "href": "slides/14-colliearity.html#r-lab-hospital-manpower---ridge-regression",
    "title": "Collinearity",
    "section": "\nR Lab Hospital Manpower - Ridge Regression",
    "text": "R Lab Hospital Manpower - Ridge Regression\n\n\n\n\nmanpower_scale <- apply(manpower, 2, \n                        unit_length_scale)\ndf <- as.data.frame(manpower_scale)\ndelta <- seq(0, 0.5, by = 0.01)\nridge_fit <- MASS::lm.ridge(\n  y ~ . -1, data = df, lambda = delta)\nmatplot(coef(ridge_fit), type = \"l\",\n        xlab = \"delta\", ylab = \"Coef\", \n        main = \"Ridge Trace\")\nabline(v = which(delta == 0.07), \n       col = \"orange\", lty = 2)"
  },
  {
    "objectID": "slides/14-colliearity.html#r-lab-hospital-manpower---ridge-regression-1",
    "href": "slides/14-colliearity.html#r-lab-hospital-manpower---ridge-regression-1",
    "title": "Collinearity",
    "section": "\nR Lab Hospital Manpower - Ridge Regression",
    "text": "R Lab Hospital Manpower - Ridge Regression\n\n\n\n\n\n\n\n\n\n\n\nmath4780-f23.github.io/website"
  },
  {
    "objectID": "slides/15-var-select.html#model-building",
    "href": "slides/15-var-select.html#model-building",
    "title": "Model Building, Selection and Validation",
    "section": "Model Building",
    "text": "Model Building\nSo far, we assume that we\n\nhave a very good idea of the basic form of the model (linear form after transformation)\nknow (nearly) all of the regressors that are important and should be used.\n\n\n\nModel Adequacy\n\n\n\n\n\n\n\n\n\nModel Selection\n\n\n\n\n\n\n\n\n\n\nOur strategy is - Fit the full model - Perform a thorough analysis (residual analysis, outliers, collinearity, etc) - Transformation of response/regressors - Test regressor significance - Perform a thorough analysis"
  },
  {
    "objectID": "slides/15-var-select.html#variable-selection",
    "href": "slides/15-var-select.html#variable-selection",
    "title": "Model Building, Selection and Validation",
    "section": "Variable Selection",
    "text": "Variable Selection\n\nWe have a large pool of candidate regressors, of which only a few are likely to be important.\nFinding an appropriate subset of regressors for the model is called model/variable selection.\n\n\nTwo ‚Äúconflicting‚Äù goals in model building:\n\nas many regressors as possible for better predictive performance on new data (smaller bias).\n\nas few regressors as possible because as the number of regressors increases,\n\n\n\\(\\mathrm{Var}(\\hat{y})\\) will increase (larger variance)\ncost more in data collecting and maintaining\nmore model complexity\n\n\n\nA compromise between the two hopefully leads to the ‚Äúbest‚Äù regression equation.\n\nWhat does best mean?\n\n\nIn most practical problems, we have a large pool of possible candidate regressors, of which only a few are likely to be important.\nFinding an appropriate subset of regressors for the model is called variable selection.\nTwo ‚Äúconflicting‚Äù goals in model building:\n\nas many regressors as possible for more information for prediction\nas few regressors as possible (1) the variance of \\(\\hat{y}\\) will increase as the number of regressors increases (2) cost more in data collection (3) more model complexity\n\n\n\nA compromise between the two hopefully leads to the ‚Äúbest‚Äù regression equation.\n\n\n\nThere is no unique definition of ‚Äúbest‚Äù, and different methods specify different subsets of the candidate regressors as best."
  },
  {
    "objectID": "slides/15-var-select.html#predictive-performance",
    "href": "slides/15-var-select.html#predictive-performance",
    "title": "Model Building, Selection and Validation",
    "section": "Predictive Performance",
    "text": "Predictive Performance\n\nA selected model that fits the observed sample data well may not predict well on new observations.\nBuild a good regression function/model in terms of prediction accuracy. (Model validation/assessment)\nWant: The selected model minimizes the mean square prediction error (MSPE) on the new data: \\[\\small MSPE(\\hat{y}) = E\\left[ (\\hat{y} - y)^2\\right] = E\\left[ (\\hat{y} - E(\\hat{y}))^2\\right] + [E(\\hat{y}) - y]^2 = \\mathrm{Var}(\\hat{y}) + \\text{Bias}^2(\\hat{y})\\]\n\n\n\n\nThe mechanics of prediction is easy:\n\nPlug in values of predictors to the model equation.\nCalculate the predicted value of the response \\(\\hat{y}\\)\n\n\n\n\n\nSo, the mechanics of prediction is easy:\n\nOnce you have your model, you can Plug in values of predictors to the model equation\nCalculate the predicted value of the response variable, \\(\\hat{y}\\), either numerical or categorical.\n\n\n\n\n\n\nGetting it right is hard! No guarantee that\n\nthe model estimates are close to the truth\nyour model performs as well with new data as it did with your sample data\n\n\n\n\nBut Getting it right is hard!\n\nThere is no guarantee the model estimates you have are correct\nOr that your model will perform as well with new data as it did with your sample data\n\n\nTest data are the new data that are not used for training or fitting our model, but the data we are interested in predicting its value.\nSo we care about the prediction performance on the test data much more than the performance on the training data."
  },
  {
    "objectID": "slides/15-var-select.html#spending-our-data",
    "href": "slides/15-var-select.html#spending-our-data",
    "title": "Model Building, Selection and Validation",
    "section": "Spending Our Data",
    "text": "Spending Our Data\n\nSeveral steps to create a useful model:\n\nParameter estimation\nModel building and selection\nPerformance assessment, etc.\n\n\n\n\n\nDoing all of this on the entire data may lead to overfitting:\n\n\n\nThe model performs well on the current sample data, but awfully predicts the response on the new data we are interested.\n\n\n\n\nWhen we are doing modeling, we are doing several steps to create a useful model,\n\nparameter estimation\nmodel selection\nperformance assessment, etc.\n\n\nDoing all of this on the entire data we have available may lead to overfitting. In classification, it means that our model labels the training response variable almost perfectly with very high classification accuracy, but the model performs very bad when fitted to the test data or incoming future emails for example."
  },
  {
    "objectID": "slides/15-var-select.html#overfitting",
    "href": "slides/15-var-select.html#overfitting",
    "title": "Model Building, Selection and Validation",
    "section": "Overfitting",
    "text": "Overfitting\n\nThe model performs well on the current sample data, but awfully predicts the response on the new data we are interested.\n\n\nLow error rate on observed data, but high prediction error rate on future unobserved data!\n\n\nSource: https://i.pinimg.com/originals/72/e2/22/72e222c1542539754df1d914cb671bd7.pnghttps://i.pinimg.com/originals/72/e2/22/72e222c1542539754df1d914cb671bd7.png - Look at this illustration, and let‚Äôs focus on the overfitting and classification case. - the blue and red points are our training data representing two categories, and the green points are the new data to be classified. - the black curve is the classification boundary that separates the two categories. - Based on the boundary, you can see that the classification performance on the training data set is perfect, because all blue points and red points are perfectly separated. - However, such classification rule generated by the training data may not be good for the new data. - With this boundary, ‚Ä¶ - OK so, if we wanna make sure that our model is good at predicting things, we probably want to avoid overfitting. - But how?"
  },
  {
    "objectID": "slides/15-var-select.html#splitting-data",
    "href": "slides/15-var-select.html#splitting-data",
    "title": "Model Building, Selection and Validation",
    "section": "Splitting Data",
    "text": "Splitting Data\n\nOften, we don‚Äôt have another unused data to assess the performance of our model.\nSolution: Pretend we have new data by splitting our data into training set and test set (validation set)!\n\n\n\n\nTraining set:\n\nSandbox for model building/selection\nSpend most of your time using the training set to develop the model\nMajority of the original sample data (usually ~ 80%)\n\n\n\nTest set:\n\nHeld in reserve to determine efficacy of one or two chosen models\nCritical to look at it once only, otherwise it becomes part of the modeling process\nRemainder of the data (usually ~ 20%)\n\n\n\n\n\nAllocate specific subsets of data for different tasks, as opposed to allocating the largest possible amount to the model parameter estimation only (what we‚Äôve done so far).\nWell we do this by splitting our data. So we split our data into to sets, training set and testing set, or sometimes called validation set.\nYou can think about your training set as your sandbox for model building. You can do whatever you want, like data wrangling, data transformation, data tidying, and data visualization, all of which help you build an appropriate model.\nSo you Spend most of your time using the training set to develop the model\nAnd this is the Majority of the original sample data, which is usually about 75% - 80% of your data. So you basically take a random sample from the data that is about 80% of it.\nAnd you don‚Äôt touch the remaining 20% of the data until you are ready to test your model performance.\nSo the test set is held in reserve to determine efficacy of one or two chosen models\nCritical to look at it once, otherwise it becomes part of the modeling process\nand that is the Remainder of the data, usually 20% - 25%\nSo ideally, we hope to use our entire data as training data to train our model, right? And to test the model performance, we just collect another data set as test data to be used for testing performance. But in reality, it is not the usual case. In reality, we only have one single data set, and it is hard to collect another sample data as test data.\nSo under this situation, this type of splitting data becomes a must if we want to have both training and test data."
  },
  {
    "objectID": "slides/15-var-select.html#model-selection-criteria",
    "href": "slides/15-var-select.html#model-selection-criteria",
    "title": "Model Building, Selection and Validation",
    "section": "Model Selection Criteria",
    "text": "Model Selection Criteria\n\nThe full (largest) model has \\(M\\) candidate regressors.\nThere are \\(M \\choose p-1\\) possible subset models of size \\(p\\).\nThere are totally \\(2^M\\) possible subset models.\nAn evaluation metric should consider Goodness of Fit and Model Complexity:\n\n\nGoodness of Fit: The more regressors, the better\n\n\nComplexity Penalty: The less regressors, the better\n\n\n\nEvaluate subset models:\n\n\n\\(R_{adj}^2\\) \\(\\uparrow\\)\n\nMallow‚Äôs \\(C_p\\) \\(\\downarrow\\)\n\nInformation Criterion (AIC, BIC) \\(\\downarrow\\)\n\n\nPREdiction Sum of Squares (PRESS) \\(\\downarrow\\) (Allen, D.M. (1974))\n\n\n\n\n\n\\(R^2\\) \\(\\uparrow\\)\n\n\n\\(MS_{res}\\) \\(\\downarrow\\)\n\nThe idea of model selection is to apply some penalty on the number of parameters used in the model. On one hand, we want to model fitting to be as good as possible, i.e., the mean squared error is small. On the other hand, we also want to restrict on the number of variables. We know that as we keep adding variables into a linear regression, the R2 would usually increase. Hence, there is a trade-off between the two. In general, we consider a criterion in the form of"
  },
  {
    "objectID": "slides/15-var-select.html#selection-criteria-mallows-c_p-statistic-downarrow",
    "href": "slides/15-var-select.html#selection-criteria-mallows-c_p-statistic-downarrow",
    "title": "Model Building, Selection and Validation",
    "section": "Selection Criteria: Mallow‚Äôs \\(C_p\\) Statistic \\(\\downarrow\\)\n",
    "text": "Selection Criteria: Mallow‚Äôs \\(C_p\\) Statistic \\(\\downarrow\\)\n\nFor a model with \\(p\\) coefficients ( \\(k\\) predictors ),\n\\[\\begin{align} C_p &= \\frac{SS_{res}(p)}{\\hat{\\sigma}^2} - n + 2p \\\\ &= p + \\frac{(s^2 - \\hat{\\sigma}^2)(n-p)}{\\hat{\\sigma}^2} \\end{align}\\]\n\n\\(\\hat{\\sigma}^2\\) is the variance estimate from the full model, i.e., \\(\\hat{\\sigma}^2 = MS_{res}(M)\\).\n\\(s^2\\) is the variance estimate from the model with \\(p\\) coefficients, i.e., \\(s^2 = MS_{res}(p)\\).\nFavors the candidate model with the smallest \\(C_p\\).\n\nFor unbiased models that \\(E[\\hat{y}_i] = E[y_i]\\), \\(C_p = p\\).\n\nAll of the errors in \\(\\hat{y}_i\\) is variance, and the model is not underfitted.\n\n\n\n\n\n\\(C_p = M + 1\\) for the full model."
  },
  {
    "objectID": "slides/15-var-select.html#mallows-c_p-plot",
    "href": "slides/15-var-select.html#mallows-c_p-plot",
    "title": "Model Building, Selection and Validation",
    "section": "Mallow‚Äôs \\(C_p\\) Plot",
    "text": "Mallow‚Äôs \\(C_p\\) Plot\n\n\n\n\n\n\n\n\n\n\n\n\nModel A is a heavily biased model.\nModel D is the poorest performer.\nModel B and C are reasonable.\nModel C has \\(C_p < 3\\) which implies \\(MS_{res}(3) < MS_{res}(M)\\)"
  },
  {
    "objectID": "slides/15-var-select.html#selection-criteria-information-criterion-downarrow",
    "href": "slides/15-var-select.html#selection-criteria-information-criterion-downarrow",
    "title": "Model Building, Selection and Validation",
    "section": "Selection Criteria: Information Criterion \\(\\downarrow\\)\n",
    "text": "Selection Criteria: Information Criterion \\(\\downarrow\\)\n\nFor a model with \\(p\\) coefficients ( \\(k\\) predictors ),\n\nAkaike information criterion (AIC) is \\[\\text{AIC} = n \\ln \\left( \\frac{SS_{res}(p)}{n} \\right) + 2p\\]\n\nBayesian information criterion (BIC) is \\[\\text{BIC} = n \\ln \\left( \\frac{SS_{res}(p)}{n} \\right) + p \\ln (n)\\]\n\nBIC penalizes more when adding more variables as the sample size increases.\nBIC tends to choose models with less regressors."
  },
  {
    "objectID": "slides/15-var-select.html#selection-criteria-press-downarrow",
    "href": "slides/15-var-select.html#selection-criteria-press-downarrow",
    "title": "Model Building, Selection and Validation",
    "section": "Selection Criteria: PRESS \\(\\downarrow\\)\n",
    "text": "Selection Criteria: PRESS \\(\\downarrow\\)\n\n\nPredicted Residual Error Sum of Squares (PRESS)\n\\(\\text{PRESS}_p = \\sum_{i=1}^n[y_i - \\hat{y}_{(i)}]^2 = \\sum_{i=1}^n\\left( \\frac{e_i}{1-h_{ii}}\\right)^2\\) where \\(e_i = y_i - \\hat{y}_i\\).\n\\(R_{pred, p}^2 = 1 - \\frac{PRESS_p}{SS_T}\\)\n\\(\\text{Absolute PRESS}_p = \\sum_{i=1}^n|y_i - \\hat{y}_{(i)}|\\) can also be considered when some large prediction errors are too influential."
  },
  {
    "objectID": "slides/15-var-select.html#r-lab-criteria-computation",
    "href": "slides/15-var-select.html#r-lab-criteria-computation",
    "title": "Model Building, Selection and Validation",
    "section": "\nR Lab Criteria Computation",
    "text": "R Lab Criteria Computation\n\n\n\nmanpower <- read.csv(file = \"./data/manpower.csv\", header = TRUE)\nlm_full <- lm(y ~ x1 + x2 + x3 + x4 + x5, \n              data = manpower)\nsumm_full <- summary(lm_full)\n\n## Adjusted R sq.\nsumm_full$adj.r.squared  \n\n[1] 0.987\n\n# PRESS\nsum((lm_full$residual / \n       (1 - hatvalues(lm_full))) ^ 2) \n\n[1] 32195222\n\n\n\n\n\nols_mallows_cp() for Mallow‚Äôs \\(C_p\\) in olsrr package\n\n\n\n## AIC\nextractAIC(lm_full, k = 2)  \n\n[1]   6 224\n\n## BIC\nn <- length(manpower$y)\nextractAIC(lm_full, k = log(n)) \n\n[1]   6 229\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n'log Lik.' -130 (df=7)\n\n\n[1] 224\n\n\n[1] 229\n\n\n[1] 4.43"
  },
  {
    "objectID": "slides/15-var-select.html#selection-methods-best-subset-all-possible-selection",
    "href": "slides/15-var-select.html#selection-methods-best-subset-all-possible-selection",
    "title": "Model Building, Selection and Validation",
    "section": "Selection Methods: Best Subset (All Possible) Selection",
    "text": "Selection Methods: Best Subset (All Possible) Selection\n\nAssume the intercept is in all models.\nIf there are \\(M\\) possible regressors, we investigate all \\(2^M - 1\\) possible regression equations.\nUse the selection criteria to determine some candidate models and complete regression analysis on them.\nIf the estimates of a particular coefficient tends to ‚Äújump around‚Äù, this could be an indication of collinearity."
  },
  {
    "objectID": "slides/15-var-select.html#r-lab-best-subset-selection-ols_step_all_possible",
    "href": "slides/15-var-select.html#r-lab-best-subset-selection-ols_step_all_possible",
    "title": "Model Building, Selection and Validation",
    "section": "\nR Lab Best Subset Selection ols_step_all_possible()\n",
    "text": "R Lab Best Subset Selection ols_step_all_possible()\n\n\n\nolsrr_all <- olsrr::ols_step_all_possible(lm_full)\nnames(olsrr_all)\n\n [1] \"mindex\"     \"n\"          \"predictors\" \"rsquare\"    \"adjr\"      \n [6] \"predrsq\"    \"cp\"         \"aic\"        \"sbic\"       \"sbc\"       \n[11] \"msep\"       \"fpe\"        \"apc\"        \"hsp\"       \n\n\n\n\n\n\nn: number of predictors\n\npredictors: predictors in the model\n\nrsquare: R-square of the model\n\nadjr: adjusted R-square of the model\n\npredrsq: predicted R-square of the model\n\n\n\n\ncp: Mallow‚Äôs Cp\n\naic: AIC\n\nsbic: Sawa BIC\n\nsbc: Schwarz BIC (the one we defined)"
  },
  {
    "objectID": "slides/15-var-select.html#section-1",
    "href": "slides/15-var-select.html#section-1",
    "title": "Model Building, Selection and Validation",
    "section": "",
    "text": "Model (x2 x3 x5)\n\n\n   Index N     Predictors R-Square Adj. R-Square Mallow's Cp\n3      1 1             x3    0.972         0.970       20.38\n1      2 1             x1    0.971         0.970       21.20\n2      3 1             x2    0.893         0.886      114.97\n4      4 1             x4    0.884         0.877      125.87\n5      5 1             x5    0.335         0.290      785.26\n10     6 2          x2 x3    0.987         0.985        4.94\n6      7 2          x1 x2    0.986         0.984        5.66\n14     8 2          x3 x5    0.985         0.983        7.29\n9      9 2          x1 x5    0.984         0.982        8.16\n13    10 2          x3 x4    0.975         0.972       18.57\n8     11 2          x1 x4    0.974         0.970       20.04\n7     12 2          x1 x3    0.973         0.969       21.99\n11    13 2          x2 x4    0.931         0.921       72.29\n12    14 2          x2 x5    0.924         0.913       80.30\n15    15 2          x4 x5    0.910         0.898       96.50\n23    16 3       x2 x3 x5    0.990         0.988        2.92\n18    17 3       x1 x2 x5    0.989         0.987        3.71\n16    18 3       x1 x2 x3    0.987         0.984        6.21\n22    19 3       x2 x3 x4    0.987         0.984        6.90\n17    20 3       x1 x2 x4    0.986         0.983        7.66\n20    21 3       x1 x3 x5    0.985         0.982        8.97\n25    22 3       x3 x4 x5    0.985         0.982        9.00\n21    23 3       x1 x4 x5    0.985         0.981        9.41\n19    24 3       x1 x3 x4    0.978         0.974       16.80\n24    25 3       x2 x4 x5    0.952         0.941       48.28\n30    26 4    x2 x3 x4 x5    0.991         0.988        4.03\n28    27 4    x1 x2 x4 x5    0.991         0.987        4.26\n27    28 4    x1 x2 x3 x5    0.991         0.987        4.35\n26    29 4    x1 x2 x3 x4    0.988         0.984        7.54\n29    30 4    x1 x3 x4 x5    0.985         0.980       10.92\n31    31 5 x1 x2 x3 x4 x5    0.991         0.987        6.00\n\n\n\n\n\n[1] 275\n\n\n[1] 273\n\n\n[1] 224\n\n\n[1] 280\n\n\n[1] 278\n\n\n[1] 229\n\n\n[1] 278\n\n\n[1] 280\n\n\n[1] 232"
  },
  {
    "objectID": "slides/15-var-select.html#r-lab-ols_step_best_subset",
    "href": "slides/15-var-select.html#r-lab-ols_step_best_subset",
    "title": "Model Building, Selection and Validation",
    "section": "\nR Lab ols_step_best_subset()\n",
    "text": "R Lab ols_step_best_subset()\n\n\n\n# metric = c(\"rsquare\", \"adjr\", \"predrsq\", \"cp\", \"aic\", \"sbic\", \"sbc\", \"msep\", \"fpe\", \"apc\", \"hsp\")\nolsrr::ols_step_best_subset(lm_full, metric = \"predrsq\")\n\n   Best Subsets Regression   \n-----------------------------\nModel Index    Predictors\n-----------------------------\n     1         x3             \n     2         x2 x3          \n     3         x2 x3 x5       \n     4         x2 x3 x4 x5    \n     5         x1 x2 x3 x4 x5 \n-----------------------------\n\n                                                           Subsets Regression Summary                                                            \n-------------------------------------------------------------------------------------------------------------------------------------------------\n                       Adj.        Pred                                                                                                           \nModel    R-Square    R-Square    R-Square     C(p)        AIC         SBIC        SBC           MSEP             FPE            HSP         APC  \n-------------------------------------------------------------------------------------------------------------------------------------------------\n  1        0.9722      0.9703       0.956    20.3812    285.5158    234.8274    288.0155    15612703.3319    1025426.9349    65534.8041    0.0352 \n  2        0.9867      0.9848       0.964     4.9416    274.9519    227.0975    278.2847     8029607.6257     552301.0536    36111.9920    0.0190 \n  3        0.9901      0.9878       0.964     2.9177    272.0064    226.8104    276.1724     6503027.4301     466884.0206    31496.1442    0.0160 \n  4        0.9908      0.9877       0.942     4.0263    272.6849    229.2716    277.6841     6563621.8068     490245.8263    34438.7564    0.0168 \n  5        0.9908      0.9867       0.935     6.0000    274.6442    232.3507    280.4767     7202730.2305     557787.1896    41227.7488    0.0192 \n-------------------------------------------------------------------------------------------------------------------------------------------------\nAIC: Akaike Information Criteria \n SBIC: Sawa's Bayesian Information Criteria \n SBC: Schwarz Bayesian Criteria \n MSEP: Estimated error of prediction, assuming multivariate normality \n FPE: Final Prediction Error \n HSP: Hocking's Sp \n APC: Amemiya Prediction Criteria \n\n\n\n\n\nols_step_best_subset() identifies the best model of each size using \\(R^2\\) by default."
  },
  {
    "objectID": "slides/15-var-select.html#r-lab-best-subset-selection",
    "href": "slides/15-var-select.html#r-lab-best-subset-selection",
    "title": "Model Building, Selection and Validation",
    "section": "\nR Lab Best Subset Selection",
    "text": "R Lab Best Subset Selection\n\n\n\n\n\n\n\n\n\n\n\n\n\n k \n    1 \n    2 \n    3 \n    4 \n    5 \n    r2 \n    adj_r2 \n    cp \n    press \n    abs_press \n    r2_pred \n    aic \n    bic \n  \n\n\n 1 \n    0 \n    0 \n    1 \n    0 \n    0 \n    0.972 \n    0.970 \n    20.38 \n    2.18e+07 \n    13431 \n    0.956 \n    235 \n    237 \n  \n\n 1 \n    1 \n    0 \n    0 \n    0 \n    0 \n    0.971 \n    0.970 \n    21.20 \n    2.22e+07 \n    13666 \n    0.955 \n    236 \n    237 \n  \n\n 1 \n    0 \n    1 \n    0 \n    0 \n    0 \n    0.893 \n    0.886 \n    114.97 \n    1.58e+08 \n    28031 \n    0.680 \n    258 \n    260 \n  \n\n 1 \n    0 \n    0 \n    0 \n    1 \n    0 \n    0.884 \n    0.877 \n    125.87 \n    7.01e+07 \n    22890 \n    0.858 \n    260 \n    261 \n  \n\n 1 \n    0 \n    0 \n    0 \n    0 \n    1 \n    0.335 \n    0.290 \n    785.26 \n    4.56e+08 \n    61324 \n    0.079 \n    289 \n    291 \n  \n\n 2 \n    0 \n    1 \n    1 \n    0 \n    0 \n    0.987 \n    0.985 \n    4.94 \n    1.79e+07 \n    12742 \n    0.964 \n    225 \n    227 \n  \n\n 2 \n    1 \n    1 \n    0 \n    0 \n    0 \n    0.986 \n    0.984 \n    5.66 \n    1.80e+07 \n    12873 \n    0.964 \n    225 \n    228 \n  \n\n 2 \n    0 \n    0 \n    1 \n    0 \n    1 \n    0.985 \n    0.983 \n    7.29 \n    1.26e+07 \n    10915 \n    0.974 \n    227 \n    230 \n  \n\n 2 \n    1 \n    0 \n    0 \n    0 \n    1 \n    0.984 \n    0.982 \n    8.16 \n    1.30e+07 \n    11074 \n    0.974 \n    228 \n    230 \n  \n\n 2 \n    0 \n    0 \n    1 \n    1 \n    0 \n    0.975 \n    0.972 \n    18.57 \n    3.25e+07 \n    16825 \n    0.934 \n    235 \n    238 \n  \n\n 2 \n    1 \n    0 \n    0 \n    1 \n    0 \n    0.974 \n    0.970 \n    20.04 \n    3.58e+07 \n    17328 \n    0.928 \n    236 \n    239 \n  \n\n 2 \n    1 \n    0 \n    1 \n    0 \n    0 \n    0.973 \n    0.969 \n    21.99 \n    2.26e+07 \n    13957 \n    0.954 \n    237 \n    240 \n  \n\n 2 \n    0 \n    1 \n    0 \n    1 \n    0 \n    0.931 \n    0.921 \n    72.29 \n    1.41e+08 \n    28166 \n    0.716 \n    253 \n    255 \n  \n\n 2 \n    0 \n    1 \n    0 \n    0 \n    1 \n    0.924 \n    0.913 \n    80.30 \n    1.29e+08 \n    25960 \n    0.740 \n    254 \n    257 \n  \n\n 2 \n    0 \n    0 \n    0 \n    1 \n    1 \n    0.910 \n    0.898 \n    96.50 \n    7.12e+07 \n    25251 \n    0.856 \n    257 \n    260 \n  \n\n\n\n\n\n\n\n\n\n\n k \n    1 \n    2 \n    3 \n    4 \n    5 \n    r2 \n    adj_r2 \n    cp \n    press \n    abs_press \n    r2_pred \n    aic \n    bic \n  \n\n\n 3 \n    0 \n    1 \n    1 \n    0 \n    1 \n    0.990 \n    0.988 \n    2.92 \n    1.78e+07 \n    12019 \n    0.964 \n    222 \n    225 \n  \n\n 3 \n    1 \n    1 \n    0 \n    0 \n    1 \n    0.989 \n    0.987 \n    3.71 \n    1.81e+07 \n    12163 \n    0.964 \n    223 \n    226 \n  \n\n 3 \n    1 \n    1 \n    1 \n    0 \n    0 \n    0.987 \n    0.984 \n    6.21 \n    2.28e+07 \n    14243 \n    0.954 \n    226 \n    229 \n  \n\n 3 \n    0 \n    1 \n    1 \n    1 \n    0 \n    0.987 \n    0.984 \n    6.90 \n    3.01e+07 \n    15780 \n    0.939 \n    227 \n    230 \n  \n\n 3 \n    1 \n    1 \n    0 \n    1 \n    0 \n    0.986 \n    0.983 \n    7.66 \n    3.28e+07 \n    16227 \n    0.934 \n    227 \n    231 \n  \n\n 3 \n    1 \n    0 \n    1 \n    0 \n    1 \n    0.985 \n    0.982 \n    8.97 \n    1.30e+07 \n    11104 \n    0.974 \n    229 \n    232 \n  \n\n 3 \n    0 \n    0 \n    1 \n    1 \n    1 \n    0.985 \n    0.982 \n    9.00 \n    1.63e+07 \n    11801 \n    0.967 \n    229 \n    232 \n  \n\n 3 \n    1 \n    0 \n    0 \n    1 \n    1 \n    0.985 \n    0.981 \n    9.41 \n    1.78e+07 \n    12296 \n    0.964 \n    229 \n    232 \n  \n\n 3 \n    1 \n    0 \n    1 \n    1 \n    0 \n    0.978 \n    0.974 \n    16.80 \n    3.44e+07 \n    18046 \n    0.930 \n    235 \n    238 \n  \n\n 3 \n    0 \n    1 \n    0 \n    1 \n    1 \n    0.952 \n    0.941 \n    48.28 \n    1.07e+08 \n    26242 \n    0.784 \n    248 \n    252 \n  \n\n 4 \n    0 \n    1 \n    1 \n    1 \n    1 \n    0.991 \n    0.988 \n    4.03 \n    2.86e+07 \n    14606 \n    0.942 \n    222 \n    227 \n  \n\n 4 \n    1 \n    1 \n    0 \n    1 \n    1 \n    0.991 \n    0.987 \n    4.26 \n    3.03e+07 \n    14992 \n    0.939 \n    223 \n    227 \n  \n\n 4 \n    1 \n    1 \n    1 \n    0 \n    1 \n    0.991 \n    0.987 \n    4.35 \n    2.25e+07 \n    13364 \n    0.955 \n    223 \n    227 \n  \n\n 4 \n    1 \n    1 \n    1 \n    1 \n    0 \n    0.988 \n    0.984 \n    7.54 \n    3.77e+07 \n    17952 \n    0.924 \n    227 \n    231 \n  \n\n 4 \n    1 \n    0 \n    1 \n    1 \n    1 \n    0.985 \n    0.980 \n    10.92 \n    1.86e+07 \n    12773 \n    0.962 \n    231 \n    235 \n  \n\n 5 \n    1 \n    1 \n    1 \n    1 \n    1 \n    0.991 \n    0.987 \n    6.00 \n    3.22e+07 \n    16025 \n    0.935 \n    224 \n    229"
  },
  {
    "objectID": "slides/15-var-select.html#r-lab-best-subset-selection-1",
    "href": "slides/15-var-select.html#r-lab-best-subset-selection-1",
    "title": "Model Building, Selection and Validation",
    "section": "\nR Lab Best Subset Selection",
    "text": "R Lab Best Subset Selection\nScale Cp, AIC, BIC to \\([0, 1]\\)."
  },
  {
    "objectID": "slides/15-var-select.html#selection-methods-forward-selection",
    "href": "slides/15-var-select.html#selection-methods-forward-selection",
    "title": "Model Building, Selection and Validation",
    "section": "Selection Methods: Forward Selection",
    "text": "Selection Methods: Forward Selection\n\nBegins with no regressors.\nInsert regressors into the model one at a time.\n\n\n\nThe first regressor selected, \\(x_1\\), is the one producing the largest \\(R^2\\) of any single regressor. It is the one with\n\nthe highest correlation with the response.\nthe largest \\(F_{test}\\) and \\(F_{test} > F_{IN}(\\alpha, 1, n-2)\\), where \\(F_{IN}\\) is the pre-specified \\(F\\) threshold.\n\n\n\n\n\n\nThe second regressor \\(x_2\\) produces the largest increase in \\(R^2\\) in the presence of \\(x_1\\). It is the one with\n\nthe largest partial correlation with the response.\nthe largest partial \\(F_{test} = \\frac{SS_R(x_2|x_1)}{MS_{res}(x_1, x_2)}\\) and \\(F_{test} > F_{IN}(\\alpha, 1, n-3)\\)\n\n\n\n\n\n\n\nThe process terminates when\n\npartial \\(F_{test} <F_{IN}(\\alpha, 1, n-p)\\), or\nthe last candidate regressor is added to the model.\n\n\n\n\\(F_{test} > F_{\\alpha, 1, n-2}\\) \\(F_{test} = \\frac{SS_R(x_2|x_1)}{MS_{res}(x_1, x_2)}> F_{\\alpha, 1, n-2}\\) Once a regressor has been added, it cannot be removed at a later step. - Begins with no regressors in the model other than the intercept. - Insert regressors into the model one at a time. - The first regressor selected to be entered into the model, say \\(x_1\\), is the one that produces the largest \\(R^2\\) of any single regressor. + the one with the highest correlation with the response. + the one with the largest \\(F_{test}\\) and \\(F_{test} > F_{IN}\\) - The second regressor examined is the one, say \\(x_2\\), that produces the largest increase in \\(R^2\\) in the presence of \\(x_1\\). + the one with the largest partial correlation with the response. + the one with the largest \\(F_{test} = \\frac{SS_R(x_2|x_1)}{MS_{res}(x_1, x_2)}\\) and \\(F_{test} > F_{IN}\\) - The process terminates either when the partial \\(F\\) statistic at a particular step does not exceed \\(F_{IN}\\) or when the last candidate regressor is added to the model."
  },
  {
    "objectID": "slides/15-var-select.html#r-lab-forward-selection-ols_step_forward_p",
    "href": "slides/15-var-select.html#r-lab-forward-selection-ols_step_forward_p",
    "title": "Model Building, Selection and Validation",
    "section": "\nR Lab Forward Selection ols_step_forward_p()\n",
    "text": "R Lab Forward Selection ols_step_forward_p()\n\n\nThe default threshold compared with the \\(p\\)-value \\(=P(F_{1, n-p} > F_{test})\\) is 0.3.\nIf \\(p\\)-value < 0.3, the regressor is entered.\n\n\n(olsrr_for <- olsrr::ols_step_forward_p(lm_full, penter = 0.3))\n\n\n                             Selection Summary                              \n---------------------------------------------------------------------------\n        Variable                  Adj.                                         \nStep    Entered     R-Square    R-Square     C(p)        AIC         RMSE      \n---------------------------------------------------------------------------\n   1    x3            0.9722      0.9703    20.3812    285.5158    957.8556    \n   2    x2            0.9867      0.9848     4.9416    274.9519    685.1685    \n   3    x5            0.9901      0.9878     2.9177    272.0064    614.7794    \n---------------------------------------------------------------------------\n\n# names(olsrr_for)"
  },
  {
    "objectID": "slides/15-var-select.html#r-lab-forward-selection-ols_step_forward_p-1",
    "href": "slides/15-var-select.html#r-lab-forward-selection-ols_step_forward_p-1",
    "title": "Model Building, Selection and Validation",
    "section": "\nR Lab Forward Selection ols_step_forward_p()\n",
    "text": "R Lab Forward Selection ols_step_forward_p()\n\n\nolsrr_for$model\n\n\nCall:\nlm(formula = paste(response, \"~\", paste(preds, collapse = \" + \")), \n    data = l)\n\nCoefficients:\n(Intercept)           x3           x2           x5  \n   1523.389        0.978        0.053     -320.951  \n\n\n\n\nprogress = TRUE, details = TRUE for detailed selection progress.\n\n\nolsrr::ols_step_forward_p(lm_full, penter = 0.3, progress = TRUE, details = TRUE)"
  },
  {
    "objectID": "slides/15-var-select.html#selection-methods-stepwise-regression",
    "href": "slides/15-var-select.html#selection-methods-stepwise-regression",
    "title": "Model Building, Selection and Validation",
    "section": "Selection Methods: Stepwise Regression",
    "text": "Selection Methods: Stepwise Regression\n\nThis procedure is a modification of forward selection.\n\nAt each step, all regressors put into the model are reassessed via their partial \\(F\\) statistic.\nA regressor added at an earlier step may now be redundant because of the relationships between it and regressors now in the equation.\nIf the partial \\(F_{test} < F_{OUT}\\), the variable will be removed.\nThe method requires both an \\(F_{IN}\\) and \\(F_{OUT}\\).\n\nit becomes insignificant with the addition of other variables to the model."
  },
  {
    "objectID": "slides/15-var-select.html#r-lab-stepwise-regression-ols_step_both_p",
    "href": "slides/15-var-select.html#r-lab-stepwise-regression-ols_step_both_p",
    "title": "Model Building, Selection and Validation",
    "section": "\nR Lab Stepwise Regression ols_step_both_p()\n",
    "text": "R Lab Stepwise Regression ols_step_both_p()\n\n\nIf \\(p\\)-value < pent = 0.1, the regressor is entered.\nAfter refitting, the regressor is removed if \\(p\\)-value > prem = 0.3.\n\n\n(olsrr_both <- olsrr::ols_step_both_p(lm_full, pent = 0.1, prem = 0.3))\n\n\n                              Stepwise Selection Summary                                \n---------------------------------------------------------------------------------------\n                     Added/                   Adj.                                         \nStep    Variable    Removed     R-Square    R-Square     C(p)        AIC         RMSE      \n---------------------------------------------------------------------------------------\n   1       x3       addition       0.972       0.970    20.3810    285.5158    957.8556    \n   2       x2       addition       0.987       0.985     4.9420    274.9519    685.1685    \n   3       x5       addition       0.990       0.988     2.9180    272.0064    614.7794    \n---------------------------------------------------------------------------------------"
  },
  {
    "objectID": "slides/15-var-select.html#comments-on-stepwise-type-procedures",
    "href": "slides/15-var-select.html#comments-on-stepwise-type-procedures",
    "title": "Model Building, Selection and Validation",
    "section": "Comments on Stepwise-Type Procedures",
    "text": "Comments on Stepwise-Type Procedures\n\nThere is a backward elimination stepwise-type procedure\n\nBegin with the model with all \\(M\\) candidate regressors.\nRemove regressors from the model one at a time.\nolsrr::ols_step_backward_p()\nCheck Variable Selection Methods\n\n\n\n\n\n\nOne can select models/variables based on other metrics such as AIC\n\nolsrr::ols_step_forward_aic()\nolsrr::ols_step_both_aic()\nolsrr::ols_step_backward_aic()\n\n\n\n\n\n\nThe order in which the regressors enter or leave the model does not imply an order of importance to the regressors.\nNo one model may be the ‚Äúbest‚Äù.\nDifferent stepwise techniques could result in different models. \n\n\nThe order in which the regressors enter or leave the model does not imply an order of importance to the regressors.\nThis is in fact a general problem with the forward selection procedure. Once a regressor has been added, it cannot be removed at a later step.\nforward selection tends to agree with all possible regressions for small subset sizes but not for large ones\nbackward elimination tends to agree with all possible regressions for large subset sizes but not for small ones.\nthe most common being that none of the procedures generally guarantees that the best subset regression model of any size will be identified.\nthere is one best subset model, but that there are several equally good ones.\n\nset F IN = F OUT = 4, as this corresponds roughly to the upper 5% point of the F distribution Bendel and Afi fi [ 1974 ] recommend Œ± = 0.25 for forward selection. F IN of between 1.3 and 2.\nKennedy and Bancroft [ 1971] suggest Œ± = 0.25 for forward selection and recommend Œ± = 0.10 for backward elimi\n\nSTRATEGY FOR VARIABLE SELECTION AND MODEL BUILDING the PRESS statistic tends to recommend smaller models than Mallow‚Äôs Cp, which in turn tends to recommend smaller models than the adjusted R2 .\n\n\n\nmath4780-f23.github.io/website"
  },
  {
    "objectID": "slides/11-poly-reg.html#why-polynomial-regression",
    "href": "slides/11-poly-reg.html#why-polynomial-regression",
    "title": "Regression Diagnostics - Linearity ‚ñ∂Ô∏è",
    "section": "Why Polynomial Regression",
    "text": "Why Polynomial Regression\n\nPolynomials are widely used in situations where the response surface is curvilinear.\nMany complex nonlinear relationships can be adequately modeled by polynomials over reasonably small ranges of the \\(x\\)‚Äôs.\n\n\n\nThey are also useful as approximating functions to unknown and possibly very complex nonlinear relationships.\nIn this sense, the polynomial model is just the Taylor series expansion of the unknown function."
  },
  {
    "objectID": "slides/11-poly-reg.html#polynomial-regression-models",
    "href": "slides/11-poly-reg.html#polynomial-regression-models",
    "title": "Regression Diagnostics - Linearity ‚ñ∂Ô∏è",
    "section": "Polynomial Regression Models",
    "text": "Polynomial Regression Models\n\nA second-order (degree) polynomial in one variable or a quadratic model is \\[y = \\beta_0 + \\beta_1 x + \\beta_2 x^2 + \\epsilon\\]\nA second-order polynomial in two variables is \\[y = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\beta_{11}x_1^2 + \\beta_{22}x_2^2 + \\beta_{12}x_1x_2 + \\epsilon\\]\nThe \\(k\\)th-order polynomial model in one variable is \\[y = \\beta_0 + \\beta_1 x + \\beta_2 x^2 + \\cdots + \\beta_kx^k + \\epsilon\\]\nIf we set \\(x_j = x^j\\), this is just a multiple linear regression model with \\(k\\) predictors \\(x_1, x_2, \\dots, x_k\\)!"
  },
  {
    "objectID": "slides/11-poly-reg.html#important-considerations",
    "href": "slides/11-poly-reg.html#important-considerations",
    "title": "Regression Diagnostics - Linearity ‚ñ∂Ô∏è",
    "section": "Important Considerations",
    "text": "Important Considerations\n\nKeep the order of the model as low as possible.\n\n\n\n\nTransform data to keep the model 1st order.\nIf fails, try a 2nd order model.\nAvoid higher-order polynomials unless they can be justified for reasons outside the data.\n\nüëâ Occam‚Äôs Razor: among competing models that predict equally well, choose the ‚Äúsimplest‚Äù one, i.e., a parsimonious model.\n\nThis avoids overfitting that leads to nearly perfect fit to the data, but bad prediction performance.\n\n\n\n\n\n\n\n\nSource: Wikiversity\n\n\n\n\n\n\nThere are several important considerations that arise when fitting a polynomial in one variable. - A low-order model in a transformed variable is almost always preferable to a high-order model in the original metric. - Arbitrary fitting of high-order polynomials is a serious abuse of regression analysis. - You can always fit a polynomial model with very high order. It may give you a pretty good fit, but with very high chance, it will end up with very bad prediction, leading to overfitting. + Occam‚Äôs Razor: among competing models that predict equally well, choose the ‚Äúsimplest‚Äù one, i.e., parsimonious model. + This avoid overfitting that leads to bad prediction performance.\n\n\n‚ÄúBayesian Deep Learning and a Probabilistic Perspective of Generalization‚Äù Wilson and Izmailov (2020) for the rationale of choosing a super high-order polynomial as the regression model.\n\n\nthe ground truth explanation for the data is out of class for any of these choices, but there is some setting of the coefficients in choice (3) which provides a better description of reality than could be managed by choices (1) and (2).\nour beliefs about the generative processes for our observations, which are often very sophisticated, typically ought to be independent of how many data points we happen to observe.\nwe often use neural networks with millions of parameters to fit datasets with thousands of points"
  },
  {
    "objectID": "slides/11-poly-reg.html#important-considerations-1",
    "href": "slides/11-poly-reg.html#important-considerations-1",
    "title": "Regression Diagnostics - Linearity ‚ñ∂Ô∏è",
    "section": "Important Considerations",
    "text": "Important Considerations\n\nModel building strategy\n\n\nüëâ Forward selection: successively fit models of increasing order until the \\(t\\)-test for the highest order term is non-significant.\nüëâ Backward elimination: fit the highest order model and then delete terms one at a time until the highest order remaining term has a significant \\(t\\) statistic.\nüëâ They do not necessarily lead to the same model.\nüëâ Restrict our attention to low-order polynomials."
  },
  {
    "objectID": "slides/11-poly-reg.html#important-considerations-2",
    "href": "slides/11-poly-reg.html#important-considerations-2",
    "title": "Regression Diagnostics - Linearity ‚ñ∂Ô∏è",
    "section": "Important Considerations",
    "text": "Important Considerations\n\nExtrapolation\n\n\nCan be extremely dangerous when the model is higher-order polynomial.\nThe nature of the true underlying relationship may change or be completely different from the system that produced the data used to fit the model."
  },
  {
    "objectID": "slides/11-poly-reg.html#important-considerations-3",
    "href": "slides/11-poly-reg.html#important-considerations-3",
    "title": "Regression Diagnostics - Linearity ‚ñ∂Ô∏è",
    "section": "Important Considerations",
    "text": "Important Considerations\n\nIll-conditioning\n\n\n\nIll-conditioning: as the order of the model increases, \\({\\bf X'X}\\) matrix inversion will become inaccurate, and error may be introduced into the parameter estimates\nCentering the predictors may remove some ill conditioning but not all.\nOne solution is to use orthogonal polynomials (LRA Sec 7.5).\n\n\n\nIll-conditioning refers to the fact that as the order of the model increases, \\({\\bf X'X}\\) matrix inversion will become inaccurate, and error can be introduced into the parameter estimates\nCentering the predictors may remove some ill conditioning but not all."
  },
  {
    "objectID": "slides/11-poly-reg.html#example-7.1-hardwood-data-lra",
    "href": "slides/11-poly-reg.html#example-7.1-hardwood-data-lra",
    "title": "Regression Diagnostics - Linearity ‚ñ∂Ô∏è",
    "section": "Example 7.1: Hardwood Data (LRA)",
    "text": "Example 7.1: Hardwood Data (LRA)\n\nStrength of kraft paper vs.¬†the percentage of hardwood in the batch of pulp from which the paper was produced.\nA quadratic model may adequately describe the relationship between tensile strength and hardwood concentration.\n\n\n\n\nhardwood[1:14, ]\n\n   concentration strength\n1            1.0      6.3\n2            1.5     11.1\n3            2.0     20.0\n4            3.0     24.0\n5            4.0     26.1\n6            4.5     30.0\n7            5.0     33.8\n8            5.5     34.0\n9            6.0     38.1\n10           6.5     39.9\n11           7.0     42.0\n12           8.0     46.1\n13           9.0     53.1\n14          10.0     52.0"
  },
  {
    "objectID": "slides/11-poly-reg.html#r-lab-hardwood-data-model-fitting",
    "href": "slides/11-poly-reg.html#r-lab-hardwood-data-model-fitting",
    "title": "Regression Diagnostics - Linearity ‚ñ∂Ô∏è",
    "section": "\nR Lab Hardwood Data Model Fitting",
    "text": "R Lab Hardwood Data Model Fitting\n\nFollowing the suggestion that centering the data may remove nonessential ill-conditioning: \\[y = \\beta_0 + \\beta_1 (x - \\bar{x}) + \\beta_2 (x - \\bar{x}) ^ 2 + \\epsilon\\]\n\n\n\nconcen_cen <- hardwood$concentration - mean(hardwood$concentration)\n(hardwood_lm <- lm(strength ~ concen_cen + I(concen_cen ^ 2), \n                   data = hardwood))\n\n\nCall:\nlm(formula = strength ~ concen_cen + I(concen_cen^2), data = hardwood)\n\nCoefficients:\n    (Intercept)       concen_cen  I(concen_cen^2)  \n         45.295            2.546           -0.635  \n\n\n\n\\(y = 45.3 + 2.55 (x - 7.26) - 0.63 (x - 7.26) ^ 2 + \\epsilon\\)\nInference, prediction and residual diagnostics procedures are the same as multiple linear regression.\n\n\nWe don‚Äôt need to create an extra predictor concentration ^ 2 in the data set.\nWe can construct the x_sq term in the lm() function.\nWe use I() function to wrap up the operation concen_cen ^ 2, so that concen_cen ^ 2 is not interpreted as a part of formula, but actual arithmetic operation.\nTo avoid this confusion, the function I() can be used to bracket those portions of a model formula where the operators are used in their arithmetic sense. For example, in the formula y ~ a + I(b+c), the term b+c is to be interpreted as the sum of b and c."
  },
  {
    "objectID": "slides/11-poly-reg.html#piecewise-polynomial-regression",
    "href": "slides/11-poly-reg.html#piecewise-polynomial-regression",
    "title": "Regression Diagnostics - Linearity ‚ñ∂Ô∏è",
    "section": "Piecewise (Polynomial) Regression",
    "text": "Piecewise (Polynomial) Regression\n\nA polynomial regression may provide a poor fit, and increasing the order does not improve the situation.\nThis may happen when the regression function behaves differently in different parts of the range of \\(x\\).\n\n\n\nSOLUTION: üëâ piecewise polynomial regression that fits separate polynomials over different regions of \\(x\\).\nExample: \\[y=\\begin{cases}\n  \\beta_{01} + \\beta_{11}x+ \\beta_{21}x^2+\\beta_{31}x^3 +\\epsilon     & \\quad \\text{if } x < c\\\\\n  \\beta_{02} + \\beta_{12}x+ \\beta_{22}x^2+\\beta_{32}x^3+\\epsilon      & \\quad \\text{if } x \\ge c\n\\end{cases}\\]\nThe joint points of pieces are called knots.\n\n\n\n\nUsing more knots leads to a more flexible piecewise polynomial.\n\n\nWith \\(K\\) different knots, how many different polynomials do we have?\n\n\n\n\nThere might be tow different systems or schemes that govern the relationship between response and predictor variables.\nAny issue of fitting a piecewise polynomial regression?\nIn other words, we fit two different polynomial functions to the data, one on the subset of the observations with \\(xi < c\\), and one on the subset of the observations with \\(xi ‚â• c\\). The first polynomial function has coefficients Œ≤01, Œ≤11, Œ≤21, Œ≤31, and the second has coefficients Œ≤02, Œ≤12, Œ≤22, Œ≤32.\nEach of these polynomial functions can be fit using least squares applied to simple functions of the original predictor.\nthe function is discontinuous\nlocations of the knots are known, otherwise it is a nonlinear regression problem.\nWE could fit piecewise regression with order 1.\n(or more constrained?)"
  },
  {
    "objectID": "slides/11-poly-reg.html#u.s.-birth-rate-from-1917-to-2003",
    "href": "slides/11-poly-reg.html#u.s.-birth-rate-from-1917-to-2003",
    "title": "Regression Diagnostics - Linearity ‚ñ∂Ô∏è",
    "section": "U.S. Birth Rate from 1917 to 2003",
    "text": "U.S. Birth Rate from 1917 to 2003\n\n\n\n\n     Year Birthrate\n1917 1917       183\n1918 1918       184\n1919 1919       163\n1920 1920       180\n1921 1921       181\n1922 1922       173\n1923 1923       168\n1924 1924       177\n1925 1925       172\n1926 1926       170\n1927 1927       164\n1928 1928       152\n1929 1929       145\n1930 1930       145\n1931 1931       139\n1932 1932       132\n1933 1933       126\n1934 1934       130\n1935 1935       130\n1936 1936       130"
  },
  {
    "objectID": "slides/11-poly-reg.html#r-lab-a-polynomial-regression-provide-a-poor-fit",
    "href": "slides/11-poly-reg.html#r-lab-a-polynomial-regression-provide-a-poor-fit",
    "title": "Regression Diagnostics - Linearity ‚ñ∂Ô∏è",
    "section": "\nR Lab A Polynomial Regression Provide a Poor Fit",
    "text": "R Lab A Polynomial Regression Provide a Poor Fit\n\nlmfit3 <- lm(Birthrate ~ poly(Year - mean(Year), degree = 3, raw = TRUE),  \n             data = birthrates)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIt might be interesting to fit a linear regression with high order polynomials to approximate this curve. This can be carried out using the poly() function, which calculates all polynomials up to a certain power. Please note that this is a more stable method compared with writing out the powers such as I(Year^2), I(Year^3) etc because the Year variable is very large, and is numerically unstable."
  },
  {
    "objectID": "slides/11-poly-reg.html#r-lab-piecewise-polynomials-3-knots-at-1936-60-78",
    "href": "slides/11-poly-reg.html#r-lab-piecewise-polynomials-3-knots-at-1936-60-78",
    "title": "Regression Diagnostics - Linearity ‚ñ∂Ô∏è",
    "section": "\nR Lab Piecewise Polynomials: 3 knots at 1936, 60, 78",
    "text": "R Lab Piecewise Polynomials: 3 knots at 1936, 60, 78\n\n\nAny issue of piecewise polynomials?\n\nthese functions are not continuous. Hence we use a trick to construct continuous basis"
  },
  {
    "objectID": "slides/11-poly-reg.html#splines",
    "href": "slides/11-poly-reg.html#splines",
    "title": "Regression Diagnostics - Linearity ‚ñ∂Ô∏è",
    "section": "Splines",
    "text": "Splines\nSplines of degree \\(k\\) are piecewise polynomials of degree \\(k\\) with continuity in derivatives (smoothing) up to degree \\(k-1\\) at each knot.\n\nUse bs() function in the splines package.\n\n\nlin_sp <- lm(Birthrate ~ splines::bs(Year, degree = 1, knots = c(1936, 1960, 1978)), \n             data = birthrates)"
  },
  {
    "objectID": "slides/11-poly-reg.html#cubic-splines",
    "href": "slides/11-poly-reg.html#cubic-splines",
    "title": "Regression Diagnostics - Linearity ‚ñ∂Ô∏è",
    "section": "Cubic Splines",
    "text": "Cubic Splines\n\nThe cubic spline is a spline of degree 3 with first 2 derivatives are continuous at the knots.\n\n\ncub_sp <- lm(Birthrate ~ splines::bs(Year, degree = 3, knots = c(1936, 1960, 1978)), \n             data = birthrates)"
  },
  {
    "objectID": "slides/11-poly-reg.html#practical-issue",
    "href": "slides/11-poly-reg.html#practical-issue",
    "title": "Regression Diagnostics - Linearity ‚ñ∂Ô∏è",
    "section": "Practical Issue",
    "text": "Practical Issue\n\n\nHow many knots should be used\n\nAs few knots as possible\nAt least 5 data points per segment\n\n\n\nWhere to place the knots\n\nNo more than one extreme point per segment\nIf possible, the extreme points should be centered in the segment\n\n\n\nWhat is the degree of functions in each region\n\nCubic spline is popular\n\n\n\n\n\n\nmath4780-f23.github.io/website"
  },
  {
    "objectID": "slides/12-nonpara-reg.html#nonparametric-statistics",
    "href": "slides/12-nonpara-reg.html#nonparametric-statistics",
    "title": "Nonparametric Regression üõ†",
    "section": "Nonparametric Statistics",
    "text": "Nonparametric Statistics\n\nA general regression model \\(y = f(x) + \\epsilon\\).\n\nParametric model: make an assumption about the shape of \\(f\\), e.g., \\(f(x) = \\beta_0 + \\beta_1 x\\), then learn the parameters \\(\\beta_0\\) and \\(\\beta_1\\).\n\n\n\n\nNonparametric methods do NOT make assumptions about the form of \\(f\\).\n\nSeek an estimate of \\(f\\) that gets close to the data points without being too rough or wiggly.\nAvoid the possibility that the functional form used to estimate \\(f\\) is very different from the true \\(f\\).\nDo not reduce the problem of estimating \\(f\\) to a small number of parameters, so more data are required to obtain an accurate estimate of \\(f\\).\n\n\n\n\nSo far, with a general regression model \\(y = f(x) + \\epsilon\\), we make an assumption about the form or shape of \\(f\\), for example, \\(f(x) = \\beta_0 + \\beta_1 x\\), then learn the parameters \\(\\beta_0\\) and \\(\\beta_1\\) to understand the relationship between \\(y\\) and \\(x\\). This is a parametric model.\n\nNonparametric methods do not make assumptions about the form of \\(f\\).\n\nThey seek an estimate of \\(f\\) that gets close to the data points without being too rough or wiggly.\nThe methods avoid the possibility that the functional form used to estimate \\(f\\) is very different from the true \\(f\\).\nSince they do not reduce the problem of estimating \\(f\\) to a small number of parameters, more observations are required to obtain an accurate estimate for \\(f\\)."
  },
  {
    "objectID": "slides/12-nonpara-reg.html#parametric-vs.-nonparametric-models",
    "href": "slides/12-nonpara-reg.html#parametric-vs.-nonparametric-models",
    "title": "Nonparametric Regression üõ†",
    "section": "Parametric vs.¬†Nonparametric Models",
    "text": "Parametric vs.¬†Nonparametric Models\n\n\nParametric (Linear regression) \n\n\n\n\n\n\n\n\n\nNonparametric (Kernel smoother)"
  },
  {
    "objectID": "slides/12-nonpara-reg.html#nonparametric-regression-1",
    "href": "slides/12-nonpara-reg.html#nonparametric-regression-1",
    "title": "Nonparametric Regression üõ†",
    "section": "Nonparametric Regression",
    "text": "Nonparametric Regression\n\nIn (parametric) linear regression, \\(\\small \\hat{y}_i = \\sum_{j=1}^n h_{ij}y_j\\). \n\nNonparametric regression, with no assumption on \\(f\\), is trying to estimate \\(y_i\\) using the weighted average of the data: \\[\\small \\hat{y}_i = \\sum_{j=1}^n w_{ij}y_j\\] where \\(\\sum_{j=1}^nw_{ij} = 1\\).\n\n\n\\(w_{ij}\\) is larger when \\(x_i\\) and \\(x_j\\) are closer. \\(y_i\\) is affected more by its neighbors.\n\n\\[\\small {\\bf \\hat{y} = Xb = X(X'X)^{-1}X'y = Hy},\\] - \\(w_{ij}\\): the influence power of \\(y_j\\) on \\(y_i\\)."
  },
  {
    "objectID": "slides/12-nonpara-reg.html#kernel-smoother",
    "href": "slides/12-nonpara-reg.html#kernel-smoother",
    "title": "Nonparametric Regression üõ†",
    "section": "Kernel Smoother",
    "text": "Kernel Smoother\n\nIn nonparametric statistics, a kernel \\(K(t)\\) is used as a weighting function satisfying\n\n\n\\(K(t) \\ge 0\\) for all \\(t\\)\n\n\\(\\int_{-\\infty}^{\\infty} K(t) \\,dt = 1\\)\n\n\\(K(-t) = K(t)\\) for all \\(t\\)\n\n\n\n\n\nCan you give me an kernel function?"
  },
  {
    "objectID": "slides/12-nonpara-reg.html#kernel-smoother-1",
    "href": "slides/12-nonpara-reg.html#kernel-smoother-1",
    "title": "Nonparametric Regression üõ†",
    "section": "Kernel Smoother",
    "text": "Kernel Smoother\n\nIn nonparametric statistics, a kernel \\(K(t)\\) is used as a weighting function satisfying\n\n\n\\(K(t) \\ge 0\\) for all \\(t\\)\n\n\\(\\int_{-\\infty}^{\\infty} K(t) \\,dt = 1\\)\n\n\\(K(-t) = K(t)\\) for all \\(t\\)"
  },
  {
    "objectID": "slides/12-nonpara-reg.html#kernel-smoother-2",
    "href": "slides/12-nonpara-reg.html#kernel-smoother-2",
    "title": "Nonparametric Regression üõ†",
    "section": "Kernel Smoother",
    "text": "Kernel Smoother\n\nLet \\(\\tilde{y}_i\\) be the kernel smoother of the \\(i\\)th response. Then \\[\\small \\tilde{y}_i = \\sum_{j=1}^n w_{ij}y_j\\] where \\(\\sum_{j=1}^nw_{ij} = 1\\).\n\nThe Nadaraya‚ÄìWatson kernel regression uses the weights given by \\[\\small w_{ij} = \\frac{K \\left( \\frac{x_i - x_j}{b}\\right)}{\\sum_{k=1}^nK \\left( \\frac{x_i - x_k}{b}\\right)}\\]\n\nParameter \\(b\\) is the bandwidth that controls the smoothness of the fitted curve.\nCloser points are given higher weights: \\(w_{ij}\\) is larger if \\(x_i\\) and \\(x_j\\) are closer.\n\n\n\n\n\n\\(\\tilde{y}_i\\)s will not be as much variationed as \\(y_j\\).\nBy weighted averaging, the value \\(\\tilde{y}_i\\) is synthesized or integrated by other data points. The average value tends to wash out some unexplained noises, and look more like its other data points.\nThese kernel smoothers use a bandwidth, \\(b\\), to define this neighborhood of interest."
  },
  {
    "objectID": "slides/12-nonpara-reg.html#gaussian-kernel-smoother-example",
    "href": "slides/12-nonpara-reg.html#gaussian-kernel-smoother-example",
    "title": "Nonparametric Regression üõ†",
    "section": "Gaussian Kernel Smoother Example",
    "text": "Gaussian Kernel Smoother Example\n\nksmooth(x, y, bandwidth = 1, kernel = \"normal\")\nKernSmooth::locpoly(x, y, degree = 0, kernel = \"normal\", bandwidth = 1)\n\n\n\n\n\\(y = 2\\sin(x) + \\epsilon\\)\n\\(K_b(x_i, x_j) = \\frac{1}{b \\sqrt{2\\pi}}\\exp \\left( - \\frac{(x_i - x_j)^2}{2b^2}\\right)\\)\n\n\n\n\n\n\n\n\n\n\n\\[\\small w_{ij} = \\frac{K \\left( \\frac{x_i - x_j}{b}\\right)}{\\sum_{k=1}^nK \\left( \\frac{x_i - x_k}{b}\\right)}\\] Bandwidth \\(b\\) defines ‚Äúneighbors‚Äù of \\(x_i\\), and controls the smoothness of the estimated \\(f\\).\n\n\n\nLarge \\(b\\): More data points have large weights. The fitted curve becomes smoother\n\nSmall \\(b\\): Less of the data are used, and the resulting curve looks wiggly.\n\n\n\n\n\nBandwidth \\(b\\) defines ‚Äúneighbors‚Äù of the specific location of interest, and control the smoothness of the estimated function \\(f\\).\nWhen \\(b\\) is large, more data points with large weights are used to predict the response \\(y_i\\) at the specific \\(x_i\\). The fitted curve becomes smoother as \\(b\\) increases.\nAs \\(b\\) decreases, less of the data are used, and the resulting curve looks more wiggly.\nWhen \\(b\\) is large, more points having large weights to predict the response at the specific \\(x\\). The resulting plot of predicted values becomes smoother as \\(b\\) increases.\nThese kernel smoothers use a bandwidth, \\(b\\), to define this neighborhood of interest.\nA large value for \\(b\\) results in more of the data being used to predict the response at the specific location. Consequently, the resulting plot of predicted values becomes much smoother as \\(b\\) increases.\nConversely, as \\(b\\) decreases, less of the data are used to generate the prediction, and the resulting plot looks more ‚Äúwiggly‚Äù or bumpy."
  },
  {
    "objectID": "slides/12-nonpara-reg.html#gaussian-kernel-smoother-example-1",
    "href": "slides/12-nonpara-reg.html#gaussian-kernel-smoother-example-1",
    "title": "Nonparametric Regression üõ†",
    "section": "Gaussian Kernel Smoother Example",
    "text": "Gaussian Kernel Smoother Example\n\n\nThe kernel function shows the weights and how fast the weights decay.\nPoint sizes correspond to their kernel weight, or the influence on the fitted or predicted value of \\(y\\) at \\(x\\).\nTo get the estimated fitted curve, we can create a grid of \\(x\\) points. For each \\(x\\), find its response value by taking weighted average of the data points whose weight is determined by the kernel function.\nThe estimated regression function \\(f\\) is the result of connecting all the weighted average responses of the \\(x\\)s in the grid."
  },
  {
    "objectID": "slides/12-nonpara-reg.html#gaussian-kernel-smoother-example-2",
    "href": "slides/12-nonpara-reg.html#gaussian-kernel-smoother-example-2",
    "title": "Nonparametric Regression üõ†",
    "section": "Gaussian Kernel Smoother Example",
    "text": "Gaussian Kernel Smoother Example"
  },
  {
    "objectID": "slides/12-nonpara-reg.html#local-regression",
    "href": "slides/12-nonpara-reg.html#local-regression",
    "title": "Nonparametric Regression üõ†",
    "section": "Local Regression",
    "text": "Local Regression\n\n\nLocal regression is another nonparametric regression alternative.\n\n\n\nIn ordinary least squares, minimize \\(\\sum_{i=1}^n(y_i - \\beta_0 - \\beta_1x_i)^2\\)\n\nIn weighted least squares, minimize \\(\\sum_{i=1}^nw_i(y_i - \\beta_0 - \\beta_1x_i)^2\\)\n\n\n\n\nIn local weighted linear regression,\n\nUse a kernel as a weighting function to define neighborhoods and weights to perform weighted least squares.\nFind the estimates of \\(\\beta_0\\) and \\(\\beta_1\\) at \\(x_0\\) by minimizing \\[\\sum_{i=1}^nK_b(x_0, x_i)(y_i - \\beta_0 - \\beta_1x_i)^2\\]\n\n\n\nLocal regression (local polynomial regression, moving regression) is another nonparametric regression alternative.\nIdea: Use a kernel as a weighting function to define neighborhoods and weights to perform weighted least squares.\n\nLocal weighted linear regression\nLocal weighted polynomial regression"
  },
  {
    "objectID": "slides/12-nonpara-reg.html#local-regression-1",
    "href": "slides/12-nonpara-reg.html#local-regression-1",
    "title": "Nonparametric Regression üõ†",
    "section": "Local Regression",
    "text": "Local Regression\nIn locally weighted linear regression, we find the estimates of \\(\\beta_0\\) and \\(\\beta_1\\) at \\(x_0\\) by minimizing \\[\\sum_{i=1}^nK_b(x_0, x_i)(y_i - \\beta_0 - \\beta_1x_i)^2\\]\n\nPay more attention to the points that are closer to the target point \\(x_0\\).\n\n\n\nThe estimated (local) linear function and \\(\\hat{\\beta}_0\\) and \\(\\hat{\\beta}_1\\) are only valid at the local point \\(x_0\\).\n\n\n\n\nIf interested in a different target \\(x_0\\), we need to refit the model.\n\n\n\n\nIn locally weighted polynomial regression, minimize \\[\\sum_{i=1}^nK_b(x_0, x_i)(y_i - \\beta_0 - \\sum_{r=1}^d\\beta_rx_i^r)^2\\]  \n\n\n\n\n\\(\\hat{\\beta}_0\\) and \\(\\hat{\\beta}_1\\) are a function of \\(x_0\\) (of course a function of \\((x_i, y_i)_{i=1}^n\\))."
  },
  {
    "objectID": "slides/12-nonpara-reg.html#local-linear-regression-w-gaussian-kernel-weights",
    "href": "slides/12-nonpara-reg.html#local-linear-regression-w-gaussian-kernel-weights",
    "title": "Nonparametric Regression üõ†",
    "section": "Local Linear Regression w/ Gaussian Kernel Weights",
    "text": "Local Linear Regression w/ Gaussian Kernel Weights"
  },
  {
    "objectID": "slides/12-nonpara-reg.html#local-quadratic-regression-w-gaussian-weights",
    "href": "slides/12-nonpara-reg.html#local-quadratic-regression-w-gaussian-weights",
    "title": "Nonparametric Regression üõ†",
    "section": "Local Quadratic Regression w/ Gaussian Weights",
    "text": "Local Quadratic Regression w/ Gaussian Weights"
  },
  {
    "objectID": "slides/12-nonpara-reg.html#local-polynomial-regression-in-r",
    "href": "slides/12-nonpara-reg.html#local-polynomial-regression-in-r",
    "title": "Nonparametric Regression üõ†",
    "section": "Local Polynomial Regression in R",
    "text": "Local Polynomial Regression in R\nUse KernSmooth or locfit package. \n\n\n\nlibrary(KernSmooth)\nlocpoly(x, y, degree, \n        kernel = \"normal\", \n        bandwidth, ...)\n\n\n\ndegree = 1: local linear\n\ndegree = 2: local quadratic\n\ndegree = 0: kernel smoother\n\n\n\n\nlibrary(locfit)\nlocfit(y ~ lp(x, nn = 0.2, \n              h = 0.5, deg = 2), \n       weights = 1, subset, ...)\n# weights: Prior weights (or sample sizes) \n#          for individual observations.\n# subset: Subset observations in the \n#         data frame.\n# nn: Nearest neighbor component of \n#     the smoothing parameter. \n# h: The constant component of \n#    the smoothing parameter.\n# deg: Degree of polynomial to use.\n\n\nVarious ways to specify the bandwidth.\n\n\n\n\nHow does local regression determine the weights?\n\n\nBut why local regression?\n\nThe Nadaraya-Watson kernel is notorious for boundary effects.\nThere is a substantial bias at the boundaries.\n\nIntuition: all neighbors are smaller/larger than the boundary point.\n\nSolution: Locally weighted regression can (partially) correct it.\n\n\nIts most common methods are LOESS (locally estimated scatterplot smoothing) and LOWESS (locally weighted scatterplot smoothing).\nLike kernel regression, LOESS uses the data from a neighborhood around the specific location.\nThe neighborhood \\(N(x_0)\\) is defined by the span parameter \\(\\alpha\\), the fraction of the total points closest to \\(x_0\\).\nThe LOESS uses the points in \\(N(x_0)\\) to generate a weighted least-squares estimate of \\(y(x_0)\\)."
  },
  {
    "objectID": "slides/12-nonpara-reg.html#loess",
    "href": "slides/12-nonpara-reg.html#loess",
    "title": "Nonparametric Regression üõ†",
    "section": "LOESS",
    "text": "LOESS\n\n\nLOESS (LOcally Estimated Scatterplot Smoothing) uses the tricube kernel \\(K(x_0, x_i)\\) defined as \\[K\\left( \\frac{|x_0 - x_i|}{\\max_{k \\in N(x_0)} |x_0 - x_k|}\\right)\\] where \\[K(t) = \\begin{cases} (1-t^3)^3       & \\quad \\text{for } 0 \\le t \\le 1\\\\ 0 & \\quad \\text{otherwise } \\end{cases}\\]\n\n\n\n\n\n\n\n\n\n\n\n\nThe neighborhood \\(N(x_0)\\) is defined by the span parameter \\(\\alpha\\), the fraction of the total points closest to \\(x_0\\).\n\n\nloess(y ~ x, span = 0.75, degree = 2) ## Default setting\n\n\nLarger \\(\\alpha\\) means more neighbors and smoother fitting.\n\nLOESS is a special case of Local Polynomial Regression Fitting"
  },
  {
    "objectID": "slides/12-nonpara-reg.html#loess-example",
    "href": "slides/12-nonpara-reg.html#loess-example",
    "title": "Nonparametric Regression üõ†",
    "section": "LOESS Example",
    "text": "LOESS Example\n\nLOESS uses the points in \\(N(x_0)\\) to generate a WLS estimate of \\(y(x_0)\\).\n\n\n\nLOESS is a special case of local polynomial regression."
  },
  {
    "objectID": "slides/12-nonpara-reg.html#r-implementation",
    "href": "slides/12-nonpara-reg.html#r-implementation",
    "title": "Nonparametric Regression üõ†",
    "section": "R Implementation",
    "text": "R Implementation\n\nloess(), KernSmooth::locploy(), locfit::locfit(), ksmooth(). Not all of them uses the same definition of the bandwidth.\nksmooth: The kernels are scaled so that their quartiles are at \\(\\pm 0.25 * \\text{bandwidth}\\).\nKernSmooth::locpoly uses the raw value that we directly plug into the kernel.\n\n\n\n\n\n\n\n\n\n\nh=1.06œÉxn‚àí1/5\nksmooth: The kernels are scaled so that their quartiles (viewed as probability densities) are at ¬± 0.25*bandwidth.\nspan: In this case, the bandwidth is decided by first finding the closes k neighbors and then use a tri-cubic weighting on the range of these neighbors. the bandwidth essentially varies depending on the target point\nthe kknn() also utilize such a feature as default, so when you want to fit the standard KNN, you should specify method = ‚Äúrectangular‚Äù, otherwise, the neighboring points will not receive the same weight.\n\n\n\nmath4780-f23.github.io/website"
  },
  {
    "objectID": "slides/03-prob-stat.html#discrete-random-variables",
    "href": "slides/03-prob-stat.html#discrete-random-variables",
    "title": "Probability and Statistics üé≤",
    "section": "Discrete Random Variables",
    "text": "Discrete Random Variables\n\nA discrete variable \\(Y\\) has countable possible values, e.g.¬†\\(\\mathcal{Y} = \\{0, 1, 2\\}\\)\n\nProbability (mass) function (pf or pmf) \\[P(Y = y) = p(y), \\,\\, y \\in \\mathcal{Y}\\]\n\n\\(0 \\le p(y) \\le 1\\) for all \\(y \\in \\mathcal{Y}\\)\n\\(\\sum_{y \\in \\mathcal{Y}}p(y) = 1\\)\n\\(P(a < Y < b) = \\sum_{y: a<y<b}p(y)\\)\n\n\n\ngive an example.\n\nGive me an example of a discrete variable/distribution!"
  },
  {
    "objectID": "slides/03-prob-stat.html#binomial-probability-function",
    "href": "slides/03-prob-stat.html#binomial-probability-function",
    "title": "Probability and Statistics üé≤",
    "section": "Binomial Probability Function",
    "text": "Binomial Probability Function\n\\(P(Y = y; m, \\pi) = \\frac{m!}{y!(m-y)!}\\pi^y(1-\\pi)^{m-y}, \\quad y = 0, 1, 2, \\dots, m\\)"
  },
  {
    "objectID": "slides/03-prob-stat.html#continuous-random-variables",
    "href": "slides/03-prob-stat.html#continuous-random-variables",
    "title": "Probability and Statistics üé≤",
    "section": "Continuous Random Variables",
    "text": "Continuous Random Variables\n\nA continuous variable \\(Y\\) has infinite possible values, e.g.¬†\\(\\mathcal{Y} = [0, \\infty)\\)\n\nProbability density function (pdf) \\[f(y), \\,\\, y \\in \\mathcal{Y}\\]\n\n\\(f(y) \\ge 0\\) for all \\(y \\in \\mathcal{Y}\\)\n\\(\\int_{\\mathcal{Y}}f(y) \\, dy= 1\\)\n\\(P(a < Y < b) = \\int_{a}^bf(y)\\,dy\\)\n\n\n\n\n\nGive me an example of continuous variable/distribution!"
  },
  {
    "objectID": "slides/03-prob-stat.html#normal-gaussian-density-curve",
    "href": "slides/03-prob-stat.html#normal-gaussian-density-curve",
    "title": "Probability and Statistics üé≤",
    "section": "Normal (Gaussian) Density Curve",
    "text": "Normal (Gaussian) Density Curve\nFor continuous variables, \\(P(a < Y < b)\\) is the area under the density curve between \\(a\\) and \\(b\\)."
  },
  {
    "objectID": "slides/03-prob-stat.html#expected-value-and-variance",
    "href": "slides/03-prob-stat.html#expected-value-and-variance",
    "title": "Probability and Statistics üé≤",
    "section": "Expected Value and Variance",
    "text": "Expected Value and Variance\nFor a random variable \\(Y\\),\n\nThe expected value or mean: \\(E(Y)\\) or \\(\\mu\\).\nThe variance: \\(\\mathrm{Var}(Y)\\) or \\(\\sigma^2\\).\n\n\n\nThe mean measures the center of the distribution, or the balancing point of a seesaw.\nThe variance measures the mean squared distance from the mean, or dispersion of a distribution.\n\n\n\n\n\n\nDiscrete \\(Y\\):\n\n\\[E(Y) := \\sum_{y \\in \\mathcal{Y}}yP(Y = y)\\] \\[\\begin{align} \\mathrm{Var}(Y) &:= E\\left[(Y - E(Y))^2 \\right] \\\\&= \\sum_{y \\in \\mathcal{Y}}(y - \\mu)^2P(Y = y)\\end{align}\\]\n\n\nContinuous \\(Y\\):\n\n\\[E(Y) := \\int_{-\\infty}^{\\infty}yf(y)\\, dy\\] \\[\\begin{align} \\mathrm{Var}(Y) &:= E\\left[(Y - E(Y))^2 \\right] \\\\&= \\int_{-\\infty}^{\\infty}(y - \\mu)^2f(y)\\, dy \\end{align}\\]\n\n\n\n\nThe mean of a discrete random variable is the weighted average of possible values weighted by their corresponding probability.\nThe variance of a discrete random variable is the weighted sum of squared deviation from the mean weighted by probability values.\ngive an normal example\n\nThis is NOT the sample mean \\(\\overline{y}\\) or sample variance \\(s^2\\).\n\nIf \\(Y \\sim binomial(n, \\pi)\\), what are the mean and variance?"
  },
  {
    "objectID": "slides/03-prob-stat.html#r-lab-dpqr-functions",
    "href": "slides/03-prob-stat.html#r-lab-dpqr-functions",
    "title": "Probability and Statistics üé≤",
    "section": "\nR Lab dpqr Functions",
    "text": "R Lab dpqr Functions\nFor some distribution (dist),\n\n\nddist(x, ...): density value \\(f(x)\\) or probability value \\(P(X = x)\\).\n\npdist(q, ...): cdf \\(F(q) = P(X \\le q)\\).\n\nqdist(p, ...): quantile of probability \\(p\\).\n\nrdist(n, ...): generate \\(n\\) random numbers.\n\n\nIn practice, we are not gonna calculate those properties by hand. Instead, we use computing software.\nIn R we can use dpqr Functions to calculate probabilities or generate values from some distribution. Let‚Äôs see how.\n\n\n\n\n## 10 binomial variable values with m = 5\nrbinom(n = 10, size = 5, prob = 0.4)\n\n [1] 2 2 2 2 2 2 0 1 3 3\n\n## P(X = 3) of binom(5, 0.4)\ndbinom(x = 3, size = 5, prob = 0.4)\n\n[1] 0.23\n\n\n\n\n## P(X <= 2) of binom(5, 0.4)\npbinom(q = 2, size = 5, prob = 0.4)\n\n[1] 0.683"
  },
  {
    "objectID": "slides/03-prob-stat.html#r-lab-dpqr-functions-1",
    "href": "slides/03-prob-stat.html#r-lab-dpqr-functions-1",
    "title": "Probability and Statistics üé≤",
    "section": "\nR Lab dpqr Functions",
    "text": "R Lab dpqr Functions\n\n## the default mean = 0 and sd = 1 (standard normal)\nrnorm(5)\n\n[1] -0.151  0.259 -0.649  0.846 -0.660\n\n\n\n\n\n\\(100\\) random draws from \\(N(0, 1)\\)"
  },
  {
    "objectID": "slides/03-prob-stat.html#r-lab-dpqr-functions-2",
    "href": "slides/03-prob-stat.html#r-lab-dpqr-functions-2",
    "title": "Probability and Statistics üé≤",
    "section": "\nR Lab dpqr Functions",
    "text": "R Lab dpqr Functions\n\n# P(0.5 < Z < 1) where Z ~ N(0, 1)\npnorm(1) - pnorm(0.5)\n\n[1] 0.15"
  },
  {
    "objectID": "slides/03-prob-stat.html#r-lab-dpqr-functions-3",
    "href": "slides/03-prob-stat.html#r-lab-dpqr-functions-3",
    "title": "Probability and Statistics üé≤",
    "section": "\nR Lab dpqr Functions",
    "text": "R Lab dpqr Functions\n\nm <- 5\np <- 0.4\n## mean\n(mu <- sum(0:5 * dbinom(0:5, size = m, prob = p)))\n\n[1] 2\n\nm * p\n\n[1] 2\n\n## var\nsum((0:5 - mu) ^ 2 * dbinom(0:5, size = m, prob = p))\n\n[1] 1.2\n\nm * p * (1 - p)\n\n[1] 1.2"
  },
  {
    "objectID": "slides/03-prob-stat.html#section-1",
    "href": "slides/03-prob-stat.html#section-1",
    "title": "Probability and Statistics üé≤",
    "section": "",
    "text": "https://statisticsglobe.com/probability-distributions-in-r"
  },
  {
    "objectID": "slides/03-prob-stat.html#some-of-normals-is-normal",
    "href": "slides/03-prob-stat.html#some-of-normals-is-normal",
    "title": "Probability and Statistics üé≤",
    "section": "Some of Normals is Normal",
    "text": "Some of Normals is Normal\n\nIf \\(Y \\sim N(\\mu, \\sigma^2)\\), \\(Z = \\frac{Y - \\mu}{\\sigma} \\sim N(0, 1)\\).\n\n\n\nIf \\(X \\sim N(\\mu_X, \\sigma_X^2)\\) and \\(Y \\sim N(\\mu_Y, \\sigma_Y^2)\\) and \\(X\\) and \\(Y\\) are independent. Then for \\(a, b \\in \\mathbf{R}\\), \\[aX + bY \\sim N\\left(a\\mu_X+b\\mu_Y, \\color{red}{a^2} \\color{black} \\sigma_X^2 + \\color{red}{b^2} \\color{black} \\sigma_Y^2\\right)\\]\n\n\n\n\n\nWhat is the distribution of \\(a_1Y_1 + a_2Y_2 + \\cdots + a_nY_n\\) if \\(Y_i \\sim N(\\mu_i, \\sigma^2_i)\\) and \\(Y_i\\)s are independent?\n\n\nIf \\(Z \\sim N(0, 1)\\), then \\[Z^2 \\sim \\chi^2_1\\]\nIf \\(Z_i \\stackrel{iid}{\\sim} N(0, 1), i = 1, 2, \\dots, k\\), then \\[\\sum_{i=1}^k Z_i^2 \\sim \\chi^2_k\\]\n\\(iid\\) means  * independent identically distributed*.\nLet \\(Y_i \\stackrel{indep}{\\sim} N(\\mu_i, \\sigma_i^2), i = 1, 2, \\dots, n\\). Then the random variable \\(U = \\sum_{i=1}^n a_iY_i\\) has the distribution"
  },
  {
    "objectID": "slides/03-prob-stat.html#statistics-comes-in",
    "href": "slides/03-prob-stat.html#statistics-comes-in",
    "title": "Probability and Statistics üé≤",
    "section": "Statistics Comes In",
    "text": "Statistics Comes In\nSuppose each data point \\(Y_i\\) of the sample \\((Y_1, Y_2, \\dots, Y_n)\\) is a random variable from the same population whose distribution is \\(N(\\mu, \\sigma^2)\\), and \\(Y_i\\)s are independent each other: \\[Y_i \\stackrel{iid}{\\sim} N(\\mu, \\sigma^2), \\quad i = 1, 2, \\dots, n\\]"
  },
  {
    "objectID": "slides/03-prob-stat.html#statistics-comes-in-sampling-distribution",
    "href": "slides/03-prob-stat.html#statistics-comes-in-sampling-distribution",
    "title": "Probability and Statistics üé≤",
    "section": "Statistics Comes In: Sampling Distribution",
    "text": "Statistics Comes In: Sampling Distribution\nIf \\(Y_i \\stackrel{iid}{\\sim} N(\\mu, \\sigma^2), \\quad i = 1, 2, \\dots, n\\),\n\n\\(\\overline{Y} \\sim N\\left(\\mu,\\frac{\\sigma^2}{n} \\right)\\)\n\\(Z = \\frac{\\overline{Y} - \\mu}{\\sigma/\\sqrt{n}} \\sim N(0, 1)\\)\n\n\n\nLet the sample variance of \\(Y\\) be \\(S^2 = \\frac{\\sum_{i=1}^n(Y_i - \\overline{Y})^2}{n-1}\\).\n\\(\\frac{\\overline{Y} - \\mu}{S/\\sqrt{n}} \\sim t_{n-1}\\)\n\n\n\n\n\nInference: \\(\\mu\\) and \\(\\sigma^2\\) are unknown, and \\(\\overline{y}\\) and \\(s^2\\) are point estimates for \\(\\mu\\) and \\(\\sigma^2\\), respectively."
  },
  {
    "objectID": "slides/03-prob-stat.html#why-use-normal-central-limit-theorem-clt",
    "href": "slides/03-prob-stat.html#why-use-normal-central-limit-theorem-clt",
    "title": "Probability and Statistics üé≤",
    "section": "Why Use Normal? Central Limit Theorem (CLT)\n",
    "text": "Why Use Normal? Central Limit Theorem (CLT)\n\n\n\\(X_1, X_2, \\dots, X_n\\) are i.i.d. variables with mean \\(\\mu\\) and variance \\(\\sigma^2 < \\infty\\).\nAs \\(n\\) increases, the sampling distribution of \\(\\overline{X}_n = \\frac{\\sum_{i=1}^nX_i}{n}\\) looks more and more like \\(N(\\mu, \\frac{\\sigma^2}{n})\\), regardless of the distribution from which we are sampling \\(X_i\\)!\n\n\n\nAlright. We know why we want large sample. You will find that we use normal distribution quite often. It is not because it is called normal or more normal than other distributions. There is a reason why we use it.\nThe reason is Central Limit Theorem.\nThe CLT says that Suppose is \\(\\overline{X}_n\\) is from a random sample of size \\(n\\) and from a population distribution having mean \\(\\mu\\) and finite standard deviation \\(\\sigma\\). As \\(n\\) increases, the sampling distribution of \\(\\overline{X}_n\\) looks more and more like \\(N(\\mu, \\sigma^2/n)\\), regardless of the distribution from which we are sampling!\n\nLook at this figure. Your population distribution can be of any shape. As long as the distribution has mean and variance, its sampling distribution of sample mean will always look like a normal distribution as long as n is large."
  },
  {
    "objectID": "slides/03-prob-stat.html#section-2",
    "href": "slides/03-prob-stat.html#section-2",
    "title": "Probability and Statistics üé≤",
    "section": "",
    "text": "Nature Methods 10, 809‚Äì810 (2013)"
  },
  {
    "objectID": "slides/03-prob-stat.html#alpha100-confidence-interval-for-mu",
    "href": "slides/03-prob-stat.html#alpha100-confidence-interval-for-mu",
    "title": "Probability and Statistics üé≤",
    "section": "\n\\((1-\\alpha)100\\%\\) Confidence Interval for \\(\\mu\\)\n",
    "text": "\\((1-\\alpha)100\\%\\) Confidence Interval for \\(\\mu\\)\n\n\n\\(T = \\frac{\\overline{Y} - \\mu}{S/\\sqrt{n}} \\sim t_{n-1}\\)\n\n\\[\\small \\begin{align} & \\quad \\quad P(-t_{\\alpha/2, n-1} < T  < t_{\\alpha/2, n-1}) = 1 - \\alpha \\\\ & \\iff P(-t_{\\alpha/2, n-1} < \\frac{\\overline{Y} - \\mu}{S/\\sqrt{n}} < t_{\\alpha/2, n-1}) = 1 - \\alpha \\\\ & \\iff P(\\mu-t_{\\alpha/2, n-1}S/\\sqrt{n} < \\overline{Y} < \\mu + t_{\\alpha/2, n-1}S/\\sqrt{n}) = 1 - \\alpha \\end{align}\\]"
  },
  {
    "objectID": "slides/03-prob-stat.html#alpha100-confidence-interval-for-mu-probability",
    "href": "slides/03-prob-stat.html#alpha100-confidence-interval-for-mu-probability",
    "title": "Probability and Statistics üé≤",
    "section": "\n\\((1-\\alpha)100\\%\\) Confidence Interval for \\(\\mu\\): Probability",
    "text": "\\((1-\\alpha)100\\%\\) Confidence Interval for \\(\\mu\\): Probability\n\n\n\n\\[P\\left(\\mu-t_{\\alpha/2, n-1}\\frac{S}{\\sqrt{n}} < \\overline{Y} < \\mu + t_{\\alpha/2, n-1}\\frac{S}{\\sqrt{n}} \\right) = 1-\\alpha\\]\n\n\nIs the interval \\(\\left(\\mu-t_{\\alpha/2, n-1}\\frac{S}{\\sqrt{n}}, \\mu + t_{\\alpha/2, n-1}\\frac{S}{\\sqrt{n}} \\right)\\) our confidence interval?\n\n\n\n\n\n\n\n\n\n\n\n\n\nNo! We don‚Äôt know \\(\\mu\\), the quantity we‚Äôd like to estimate! But we almost there!"
  },
  {
    "objectID": "slides/03-prob-stat.html#alpha100-confidence-interval-for-mu-formula",
    "href": "slides/03-prob-stat.html#alpha100-confidence-interval-for-mu-formula",
    "title": "Probability and Statistics üé≤",
    "section": "\n\\((1-\\alpha)100\\%\\) Confidence Interval for \\(\\mu\\): Formula",
    "text": "\\((1-\\alpha)100\\%\\) Confidence Interval for \\(\\mu\\): Formula\n\n\n\n\\[\\begin{align}\n&P\\left(\\mu-t_{\\alpha/2, n-1}\\frac{S}{\\sqrt{n}} < \\overline{Y} < \\mu + t_{\\alpha/2, n-1}\\frac{S}{\\sqrt{n}} \\right) = 1-\\alpha\\\\\n&P\\left( \\boxed{\\overline{Y}- t_{\\alpha/2, n-1}\\frac{S}{\\sqrt{n}} < \\mu < \\overline{Y} + t_{\\alpha/2, n-1}\\frac{S}{\\sqrt{n}}} \\right) = 1-\\alpha\n\\end{align}\\]\n\n\n\n\n\n\n\n\n\n\n\n\n\n With sample data of size \\(n\\), \\(\\left( \\overline{y}- t_{\\alpha/2, n-1}\\frac{s}{\\sqrt{n}}, \\overline{y} + t_{\\alpha/2, n-1}\\frac{s}{\\sqrt{n}} \\right)\\) is our \\((1-\\alpha)100\\%\\) CI for \\(\\mu\\)."
  },
  {
    "objectID": "slides/03-prob-stat.html#hypothesis-testing",
    "href": "slides/03-prob-stat.html#hypothesis-testing",
    "title": "Probability and Statistics üé≤",
    "section": "Hypothesis Testing",
    "text": "Hypothesis Testing\n\n \\(H_0: \\mu = \\mu_0 \\text{   vs.   } H_1: \\mu > \\mu_0\\), or \\(\\mu < \\mu_0\\), or \\(\\mu \\ne \\mu_0\\) \nThe significant level \\(\\alpha = P(\\text{Reject } H_0 \\mid H_0 \\text{ is true}) = P(\\text{Type I error})\\)\n\nThe test statistic is \\(t_{test} = \\frac{\\overline{y} - \\color{blue}{\\mu_0}}{s/\\sqrt{n}}\\), a value from \\(T \\sim t_{n-1}\\).\nWhen calculating a test statistic, we assume \\(H_0\\) is true.\n\n\nReject \\(H_0\\) if\n\n\n\n\n\n\n\n\nMethod ¬† ¬†\n\nRight-tailed \\((H_1: \\mu > \\mu_0)\\)\n\n\nLeft-tailed \\((H_1: \\mu < \\mu_0)\\)\n\n\nTwo-tailed \\((H_1: \\mu \\ne \\mu_0)\\)\n\n\n\n\nCritical value\n\\(t_{test} > t_{\\alpha, n-1}\\)\n\\(t_{test} < -t_{\\alpha, n-1}\\)\n\\(\\mid t_{test}\\mid \\, > t_{\\alpha/2, n-1}\\)\n\n\n\n\\(p\\)-value\n\\(\\small P(T > t_{test} \\mid H_0) < \\alpha\\)\n\\(\\small P(T < t_{test} \\mid H_0) < \\alpha\\)\n\\(\\small 2P(T > \\,\\mid t_{test}\\mid) \\mid H_0) < \\alpha\\)"
  },
  {
    "objectID": "slides/03-prob-stat.html#both-methods-lead-to-the-same-conclusion",
    "href": "slides/03-prob-stat.html#both-methods-lead-to-the-same-conclusion",
    "title": "Probability and Statistics üé≤",
    "section": "Both Methods Lead to the Same Conclusion",
    "text": "Both Methods Lead to the Same Conclusion\n\n\n\n\n\n\n\n\n\n\n\nmath4780-f23.github.io/website"
  },
  {
    "objectID": "slides/09-diag-const-var.html#nonconstant-error-variance",
    "href": "slides/09-diag-const-var.html#nonconstant-error-variance",
    "title": "Regression Diagnostics - Constant Variance üìñ",
    "section": "Nonconstant Error Variance",
    "text": "Nonconstant Error Variance\nWithout constant variance,\n\nLSEs are still unbiased if linearity and independence of \\(x\\)s and errors hold.\nInference results are not convincing: distorted \\(p\\)-values and incorrect coverage of confidence interval.\nThe \\(b_j\\)s will have larger standard errors (no minimum-variance).\n\n\nIs it harder or easier to reject \\(H_0: \\beta_j = 0\\) when the standard error of \\(b_j\\) is larger?\n\n\nFor these consequences to occur, nonconstant error variance must be severe."
  },
  {
    "objectID": "slides/09-diag-const-var.html#detecting-nonconstant-error-variance",
    "href": "slides/09-diag-const-var.html#detecting-nonconstant-error-variance",
    "title": "Regression Diagnostics - Constant Variance üìñ",
    "section": "Detecting Nonconstant Error Variance",
    "text": "Detecting Nonconstant Error Variance\n\nCommon pattern: the conditional variation of \\(y\\) to increase with the level of \\(y\\).\n\nIt seems natural to plot \\(e_i\\) against \\(y\\)\n\n\n\\(e_i\\) have unequal variances \\((\\mathrm{Var}(e_i) = \\sigma^2(1-h_{ii}))\\) even when the errors \\(\\epsilon_i\\) have equal variances \\(\\mathrm{Var}(\\epsilon_i) = \\sigma^2\\).\n\n\\(e_i = y_i - \\hat{y}_i\\) are correlated with \\(y\\)\n\n\n\nUse R-student residual plot \\(t_i\\) vs.¬†\\(\\hat{y}_i\\) because R-student residuals have constant variance and they are uncorrelated with \\(\\hat{y}_i\\).\n\n\\(r_{ey} = \\sqrt{1-R^2}\\) https://stats.stackexchange.com/questions/5235/what-is-the-expected-correlation-between-residual-and-the-dependent-variable"
  },
  {
    "objectID": "slides/09-diag-const-var.html#residual-plots",
    "href": "slides/09-diag-const-var.html#residual-plots",
    "title": "Regression Diagnostics - Constant Variance üìñ",
    "section": "Residual Plots",
    "text": "Residual Plots\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIf the spread of the residuals increases with the level of the fitted values, we might be able to correct the problem by transforming \\(y\\) down the ladder of power and roots."
  },
  {
    "objectID": "slides/09-diag-const-var.html#variance-stablizing-transformation",
    "href": "slides/09-diag-const-var.html#variance-stablizing-transformation",
    "title": "Regression Diagnostics - Constant Variance üìñ",
    "section": "Variance Stablizing Transformation",
    "text": "Variance Stablizing Transformation\n\nConstant variance assumption is often violated when the response \\(y\\) follows a distribution whose variance is functionally related to mean. \n\n\n\nCan you provide a distribution whose variance is functionally related to its mean?\n\n\n\n\n\n\n\n\nRelationship of \\(\\sigma^2\\) to \\(E(y)\\)\n\nTransformation\n\n\n\n\n\\(\\sigma^2 \\propto\\) constant\n\n\\(y' = y\\) (no transformation)\n\n\n\\(\\sigma^2 \\propto E(y)\\)\n\n\\(y' = \\sqrt{y}\\) (square root; Poisson)\n\n\n\\(\\sigma^2 \\propto E(y)[1-E(y)]\\)\n\n\\(y' = \\sin^{-1}(\\sqrt{y})\\) (arcsin; binomial proportions \\(0 \\le y_i \\le 1\\))\n\n\n\\(\\sigma^2 \\propto [E(y)]^2\\)\n\n\\(y' = \\ln(y)\\) (natural log)\n\n\n\\(\\sigma^2 \\propto [E(y)]^3\\)\n\n\\(y' = y^{-1/2}\\) (reciprocal square root)\n\n\n\\(\\sigma^2 \\propto [E(y)]^4\\)\n\n\\(y' = y^{-1}\\) (reciprocal)\n\n\n\n\nThe strength of a transformation depends on the amount of curvature that it induces\na mild transformation applied over a relatively narrow range of values (e.g., y max / y min < 2, 3) has little effect. On the other hand, a strong transformation over a wide range of values will have a dramatic effect on the analysis."
  },
  {
    "objectID": "slides/09-diag-const-var.html#r-lab-nonconstant-variance-cia-example",
    "href": "slides/09-diag-const-var.html#r-lab-nonconstant-variance-cia-example",
    "title": "Regression Diagnostics - Constant Variance üìñ",
    "section": "\nR Lab Nonconstant Variance (CIA Example)",
    "text": "R Lab Nonconstant Variance (CIA Example)\n\n\n\n\n\n\n\n\n\n\n\nThe residuals increase with fitted values.\nNonlinear pattern.\nNonsense negative fitted values.\n\n\n\n\n\n\n\n\n\n\n\n\nVariation around the central smooth seems approximately constant.\nNonlinearity is still apparent. (Linearity and lack of fit next week)\n\n\n\n\n\nThe nonlinear pattern suggests there are serious problems of the linear model.\nThe residuals increases with fitted values.\nSeveral nonsense negative fitted values.\nresidual plot can reveal nonlinearity, but can;t determine where the problem lies and how to correct it."
  },
  {
    "objectID": "slides/09-diag-const-var.html#tukeys-spread-level-plot",
    "href": "slides/09-diag-const-var.html#tukeys-spread-level-plot",
    "title": "Regression Diagnostics - Constant Variance üìñ",
    "section": "Tukey‚Äôs Spread-Level Plot",
    "text": "Tukey‚Äôs Spread-Level Plot\n\nPlot log of the absolute R-Student residuals vs.¬†log of the (positive) fitted values.\nThe fitted line slope \\(b\\) suggests a variance-stablizing power transformation \\(\\lambda = 1 - b\\) for \\(y\\).\n\n\n\n\n\ncar::spreadLevelPlot(ciafit, smooth = FALSE)\n\n\n\n\n\n\n\n\nSuggested power transformation:  0.37 \n\n\n\n\nspreadLevelPlot(logciafit, smooth = FALSE)\n\n\n\n\n\n\n\n\nSuggested power transformation:  1.1 \n\n\n\n\n\n\nPlot log of the absolute R-Student residuals vs.¬†log of the (positive) fitted values\nFit by robust regression to generate fitted lines.\nThe slope \\(b\\) of the fitted line suggests a variance-stablizing power transformation \\(\\lambda = 1 - b\\) for \\(y\\)."
  },
  {
    "objectID": "slides/09-diag-const-var.html#weighted-least-squares-wls",
    "href": "slides/09-diag-const-var.html#weighted-least-squares-wls",
    "title": "Regression Diagnostics - Constant Variance üìñ",
    "section": "Weighted Least Squares (WLS)",
    "text": "Weighted Least Squares (WLS)\n\nSuppose that the other assumptions hold but that the error variances differ: \\(\\epsilon_i \\stackrel{indep}{\\sim} N(0, \\sigma_i^2)\\)\nThe resulting model isn‚Äôt estimable because we have more parameters than data points.\nIf we know the pattern of unequal error variances, so that \\(\\sigma_i^2 = \\sigma^2/w_i\\), for known values \\(w_i\\), we can compute the coefficient estimates that minimize the Weighted Residual Sum of Squares.\nIn simple linear regression, \\[S(\\beta_0, \\beta_1) = \\sum_{i=1}^nw_i\\epsilon_i^2 = \\sum_{i=1}^n w_i \\left(y_i - \\beta_0 - \\beta_1x_i \\right)^2\\]\n\n\nObservations with large variances will have smaller weights.\n\n\\[(b_0, b_1, \\dots, b_k) = \\underset{{\\beta_0, \\beta_1, \\dots, \\beta_k}}{\\mathrm{arg \\, min}}  S(\\beta_0, \\beta_1, \\dots, \\beta_k)\\]\n\\[S(\\beta_0, \\beta_1, \\dots, \\beta_k) = \\sum_{i=1}^nw_i\\epsilon_i^2 = \\sum_{i=1}^nw_i\\left(y_i - \\beta_0 - \\sum_{j=1}^k\\beta_j x_{ij}\\right)^2\\]"
  },
  {
    "objectID": "slides/09-diag-const-var.html#methods-of-choosing-weights",
    "href": "slides/09-diag-const-var.html#methods-of-choosing-weights",
    "title": "Regression Diagnostics - Constant Variance üìñ",
    "section": "Methods of Choosing Weights",
    "text": "Methods of Choosing Weights\n\nPrior knowledge or information\n\nResidual analysis\n\n\n\\(\\mathrm{Var}(\\epsilon_i) \\propto x_{i}\\), suggest \\(w_i = 1/x_{i}\\)\n\nWhen \\(y_i\\) is an average of \\(n_i\\) observations at \\(x_i\\), \\(\\mathrm{Var}(y_i) =\\sigma^2 / n_{i}\\) and suggest \\(w_i = n_i\\)\n\n\n\nChosen inversely proportional to variances of measurement error\n\n.alert[ If we have no idea of \\(\\bf W\\), consider feasible GLS estimator \\(\\hat{\\bsbeta}_{FGLS} = {\\bf(X'\\hat{W}X)^{-1}X'\\hat{W}y}\\), where \\(\\bf \\hat{W}\\) is an estimate of \\(\\bf W\\) from some method. ] - The weights, \\(w_i\\), or in general \\(\\bf V\\) must be known. - Prior knowledge or information - Residual analysis + \\(\\mathrm{Var}(\\epsilon_i) \\propto x_{i}\\), suggest \\(w_i = 1/x_{i}\\) + When \\(y_i\\) is an average of \\(n_i\\) observations at \\(x_i\\), \\(\\mathrm{Var}(y_i) =\\sigma^2 / n_{i}\\) and suggest \\(w_i = n_i\\) - Chosen inversely proportional to variances of measurement error - Guess at the weights and iteratively estimate them."
  },
  {
    "objectID": "slides/09-diag-const-var.html#r-lab-example-5.5-food-sales-in-lra",
    "href": "slides/09-diag-const-var.html#r-lab-example-5.5-food-sales-in-lra",
    "title": "Regression Diagnostics - Constant Variance üìñ",
    "section": "\nR Lab Example 5.5 Food Sales in LRA",
    "text": "R Lab Example 5.5 Food Sales in LRA\n\nThe average monthly revenue vs.¬†annual advertising expenses for 30 restaurants.\n\n\nCodex <- c(3000, 3150, 3085, 5225, 5350, 6090, 8925, 9015, 8885, 8950, 9000, 11345,\n       12275, 12400, 12525, 12310, 13700, 15000, 15175, 14995, 15050, 15200,\n       15150, 16800, 16500, 17830, 19500, 19200, 19000, 19350)\ny <- c(81464, 72661, 72344, 90743, 98588, 96507, 126574, 114133, 115814, 123181,\n       131434, 140564, 151352, 146926, 130963, 144630, 147041, 179021, 166200, 180732,\n       178187, 185304, 155931, 172579, 188851, 192424, 203112, 192482, 218715, 214317)\ngroup <- c(1, 1, 1, 2, 2, 3, 4, 4, 4, 4, 4, 5, 5, 5, 5, 5, 6, 7, 7, 7, 7, 7, 7, \n           8, 8, 9, 10, 10, 10, 10)\nex5_5 <- data.frame(\"expense\" = x, \"sales\" = y, \"group\" = group)\npar(mar = c(3.3, 3.3, 0, 0), mgp = c(2, 1, 0))\nplot(ex5_5$expense, ex5_5$sales, xlab = \"Advertising expense\", \n     ylab = \"Food sales\", col = group, pch = 16, cex = 2, cex.lab = 1.5)\n\n\n\nThe average monthly income from food sales vs.¬†annual advertising expenses for 30 restaurants.\nManagement is interested in the relationship between these variables."
  },
  {
    "objectID": "slides/09-diag-const-var.html#r-lab-non-constant-variance",
    "href": "slides/09-diag-const-var.html#r-lab-non-constant-variance",
    "title": "Regression Diagnostics - Constant Variance üìñ",
    "section": "\nR Lab Non-constant Variance",
    "text": "R Lab Non-constant Variance\n\n\\(\\hat{y} = 49443.384 + 8.048x\\)"
  },
  {
    "objectID": "slides/09-diag-const-var.html#r-lab-determine-weights",
    "href": "slides/09-diag-const-var.html#r-lab-determine-weights",
    "title": "Regression Diagnostics - Constant Variance üìñ",
    "section": "\nR Lab Determine Weights",
    "text": "R Lab Determine Weights\n\nThere are several sets of \\(x\\) values that are ‚Äúnear neighbors‚Äù.\n\n\nex5_5$expense\n\n [1]  3000  3150  3085  5225  5350  6090  8925  9015  8885  8950  9000 11345\n[13] 12275 12400 12525 12310 13700 15000 15175 14995 15050 15200 15150 16800\n[25] 16500 17830 19500 19200 19000 19350\n\nex5_5$group\n\n [1]  1  1  1  2  2  3  4  4  4  4  4  5  5  5  5  5  6  7  7  7  7  7  7  8  8\n[26]  9 10 10 10 10\n\n\n\nAssume that these near neighbors are close enough to be considered repeat points.\nUse the variance of the responses at those repeat points to investigate how \\(\\mathrm{Var}(y)\\) changes with \\(x.\\)"
  },
  {
    "objectID": "slides/09-diag-const-var.html#r-lab-determine-weights-1",
    "href": "slides/09-diag-const-var.html#r-lab-determine-weights-1",
    "title": "Regression Diagnostics - Constant Variance üìñ",
    "section": "\nR Lab Determine Weights",
    "text": "R Lab Determine Weights\n\nThe empirical variance of \\(y\\), \\(s_y^2\\), increases approximately linearly with \\(\\bar{x}\\)."
  },
  {
    "objectID": "slides/09-diag-const-var.html#r-lab-determine-weights-2",
    "href": "slides/09-diag-const-var.html#r-lab-determine-weights-2",
    "title": "Regression Diagnostics - Constant Variance üìñ",
    "section": "\nR Lab Determine Weights",
    "text": "R Lab Determine Weights\n\n\\(\\hat{s}^2_y = -9226002 + 7781.6\\bar{x}\\)\nSubstituting each \\(x_i\\) value into this equation will give an estimate of the variance of the corresponding observation \\(y_i\\), \\(\\hat{s}^2_i\\)\n\nThe inverse of these \\(\\hat{s}^2_i\\) will be reasonable estimates of the weights \\(w_i\\).\n\n\ns2_fit <- lm(s2_y~x_bar)\n(coef <- s2_fit$coef)\n\n(Intercept)       x_bar \n   -9226002        7782 \n\ns2_i <- coef[1] + coef[2] * ex5_5$expense\n(wt <- 1 / s2_i)\n\n [1] 7.1e-08 6.5e-08 6.8e-08 3.2e-08 3.1e-08 2.6e-08 1.7e-08 1.6e-08 1.7e-08\n[10] 1.7e-08 1.6e-08 1.3e-08 1.2e-08 1.1e-08 1.1e-08 1.2e-08 1.0e-08 9.3e-09\n[19] 9.2e-09 9.3e-09 9.3e-09 9.2e-09 9.2e-09 8.2e-09 8.4e-09 7.7e-09 7.0e-09\n[28] 7.1e-09 7.2e-09 7.1e-09"
  },
  {
    "objectID": "slides/09-diag-const-var.html#r-lab-wls-fit",
    "href": "slides/09-diag-const-var.html#r-lab-wls-fit",
    "title": "Regression Diagnostics - Constant Variance üìñ",
    "section": "\nR Lab WLS fit",
    "text": "R Lab WLS fit\n\n(wls_fit <- lm(ex5_5$sales ~ ex5_5$expense, \n               weights = wt))\n\n...\nCoefficients:\n  (Intercept)  ex5_5$expense  \n     51024.81           7.92  \n...\n\n\n\n\\(\\hat{y} = 51024.8 + 7.918x\\)\nWe must examine the residuals to determine if using WLS has improved the fit.\nPlot the weighted residuals \\(w_i^{1/2}(y_i - \\hat{y}_i^W)\\), where \\(\\hat{y}_i^W\\) comes form the WLS fit, against \\(w_i^{1/2}\\hat{y}_i^W\\)\n\n\n.alert[ The WLS residuals and the WLS fitted values when the model assumes \\(\\mathrm{Var}(\\boldsymbol \\epsilon) = \\sigma^2 {\\bf W}^{-1}\\) should behave as the OLS residuals and OLS fitted values when the model assumes \\(\\mathrm{Var}(\\boldsymbol \\epsilon) = \\sigma^2 {\\bf I}\\)."
  },
  {
    "objectID": "slides/09-diag-const-var.html#r-lab-wls-residual-plot",
    "href": "slides/09-diag-const-var.html#r-lab-wls-residual-plot",
    "title": "Regression Diagnostics - Constant Variance üìñ",
    "section": "\nR Lab WLS Residual Plot",
    "text": "R Lab WLS Residual Plot\n\nThe residual plot has no significant fanning.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nmath4780-f23.github.io/website"
  },
  {
    "objectID": "slides/11-bootstrap.html#why-simulation-based-inference",
    "href": "slides/11-bootstrap.html#why-simulation-based-inference",
    "title": "Simulation-based Inference üíª",
    "section": "Why Simulation-based Inference?",
    "text": "Why Simulation-based Inference?\n\nLet‚Äôs go back to MATH 4720. How do we do interval estimation for population mean \\(\\mu\\)?\n\n\n\n\n\\(z\\)-interval when \\(\\sigma\\) is known\nStudent‚Äôs \\(t\\)-interval when \\(\\sigma\\) is unknown\n\n\nWhat are the assumptions in orderto use \\(z\\) or \\(t\\) intervals?\n\n\n\n\nThe population is Gaussian. If not, the sample size is large enough so that the central limit theorem can be applied!\n\n\n\n\nWe never answer this question in MATH 4720. What if the population is not Gaussian and the sample size \\(n\\) is small, i.e., CLT becomes powerless?"
  },
  {
    "objectID": "slides/11-bootstrap.html#bootstrapping-1",
    "href": "slides/11-bootstrap.html#bootstrapping-1",
    "title": "Simulation-based Inference üíª",
    "section": "Bootstrapping üë¢",
    "text": "Bootstrapping üë¢\n\n\n\nThe term bootstrapping comes from the phrase\n\n\npulling oneself up by one‚Äôs bootstraps\n\nwhich is a metaphor for accomplishing an impossible task without any outside help.\n\n\nImpossible task: estimating a population parameter using data from only the given single sample dataset, without CI formula and replicates of data.\n\n\n\n\n\n\n\nSource: http://dailywhiteboard.blogspot.com/2014/04/day-251-pull-yourself-up-by-your.html\n\n\n\n\n\n\n\n\n\nNote: Notion of saying something about a population parameter using only information from an observed sample is the crux of statistical inference, it is not limited to bootstrapping.\nThe term bootstrapping comes from the phrase ‚Äúpulling oneself up by one‚Äôs bootstraps‚Äù, which is a metaphor for accomplishing an impossible task without any outside help.\nIn statistics scenario, it means that we are estimating a population parameter using data from only the given single sample dataset, without CI formula and/or replicates of data.\nI call it Impossible task, but that‚Äôs basically what statistics usually does.\nSo notion of saying something about a population parameter using only information from an observed sample is the crux of statistical inference, it is not limited to bootstrapping."
  },
  {
    "objectID": "slides/11-bootstrap.html#rent-in-manhattan",
    "href": "slides/11-bootstrap.html#rent-in-manhattan",
    "title": "Simulation-based Inference üíª",
    "section": "Rent in Manhattan",
    "text": "Rent in Manhattan\n\nHow much do you think it costs to rent a typical 1 bedroom apartment in Manhattan?\n\n\n\nHere is our example. How much do you think it costs to rent a typical 1 bedroom apartment in Manhattan NYC?\nWell if you have no idea, at least we can say we are 100% confident that the average one-bedroom rent in Manhattan is between 0 to one million dollars.\nit‚Äôs not helping though."
  },
  {
    "objectID": "slides/11-bootstrap.html#data",
    "href": "slides/11-bootstrap.html#data",
    "title": "Simulation-based Inference üíª",
    "section": "Data",
    "text": "Data\n\nConsider 20 one-bedroom apartments that were randomly selected on Craigslist Manhattan from apartments listed as ‚Äúby owner‚Äù.\n\n\nmanhattan <- readr::read_csv(\"./data/manhattan.csv\")\nrange(manhattan)\n\n[1] 1470 4195\n\nglimpse(manhattan)\n\nRows: 20\nColumns: 1\n$ rent <dbl> 3850, 3800, 2350, 3200, 2150, 3267, 2495, 2349, 3950, 1795, 2145,‚Ä¶\n\n\n\nThe minimum is 1470 and the rent can be as high as near $4200."
  },
  {
    "objectID": "slides/11-bootstrap.html#parameter-of-interest",
    "href": "slides/11-bootstrap.html#parameter-of-interest",
    "title": "Simulation-based Inference üíª",
    "section": "Parameter of Interest",
    "text": "Parameter of Interest\n\nCould focus on the median rent or mean rent depending on our research goal.\n\n\n\nWe could focus on the median rent or mean rent depending on our research goal. Sometimes we are interested in other quantiles, not just median."
  },
  {
    "objectID": "slides/11-bootstrap.html#observed-sample-vs.-bootstrap-population",
    "href": "slides/11-bootstrap.html#observed-sample-vs.-bootstrap-population",
    "title": "Simulation-based Inference üíª",
    "section": "Observed Sample vs.¬†Bootstrap Population",
    "text": "Observed Sample vs.¬†Bootstrap Population\n\n\n\n\n\n\n\n\n\n\nSample median = $2350 üò±\n\n\n\n\n\n\n\n\n\nPopulation median = ‚ùì\n\n\nIDEA: We think the sample is representative of the population, so create an artificial population by replicating the subjects from the observed ones.\n\nSo based on the sample, the sample median rent is $2350.\nThe question is What‚Äôs our BT population look like that we can sample from?\nWell the idea is that we assume that there are probably more apartments like the ones in our observed sample in the population as well.\nSo here, basically the BT artificial population is made from our sample data, and the population is like so many replicates of our given sample.\nAgain, this is an artificial population, not the real population. If I had the real population, no inference or estimation is needed. We know the truth. Right."
  },
  {
    "objectID": "slides/11-bootstrap.html#bootstrap-population",
    "href": "slides/11-bootstrap.html#bootstrap-population",
    "title": "Simulation-based Inference üíª",
    "section": "Bootstrap Population",
    "text": "Bootstrap Population\n\nSource: Figure 12.1 of Introduction to Modern Statistics"
  },
  {
    "objectID": "slides/11-bootstrap.html#bootstrap-sampling",
    "href": "slides/11-bootstrap.html#bootstrap-sampling",
    "title": "Simulation-based Inference üíª",
    "section": "Bootstrap Sampling",
    "text": "Bootstrap Sampling\n\nSource: Figure 12.2 of Introduction to Modern Statistics"
  },
  {
    "objectID": "slides/11-bootstrap.html#practical-bootstrap-sampling",
    "href": "slides/11-bootstrap.html#practical-bootstrap-sampling",
    "title": "Simulation-based Inference üíª",
    "section": "Practical Bootstrap Sampling",
    "text": "Practical Bootstrap Sampling\n\nSource: Figure 12.4 of Introduction to Modern StatisticsTaking repeated resamples from the sample data is the same process as creating an infinitely large estimate of the population. It is computationally more feasible to take resamples directly from the sample. Note that the resampling is now done with replacement (that is, the original sample does not ever change) so that the original sample and estimated hypothetical population are equivalent."
  },
  {
    "objectID": "slides/11-bootstrap.html#bootstrapping-algorithm",
    "href": "slides/11-bootstrap.html#bootstrapping-algorithm",
    "title": "Simulation-based Inference üíª",
    "section": "Bootstrapping Algorithm",
    "text": "Bootstrapping Algorithm\n\n\n\n[1] Take a bootstrap sample\n\na random sample taken with replacement from the original sample, of the same size as the original sample.\n\n\n\n[2] Calculate the bootstrap statistic\n\na statistic such as mean, median, proportion, slope, etc. computed on the bootstrap samples.\n\n\n\n[3] Repeat steps (1) and (2) many times to create a bootstrap distribution\n\na distribution of bootstrap statistics.\n\n\n\n[4] Calculate the bounds of the \\((1-\\alpha)100\\%\\) confidence interval\n\nthe middle \\((1-\\alpha)100\\%\\) of the bootstrap distribution.\n\n\n\n\n\nSo the BT scheme or algorithm is like this.\n\n[1] Take a bootstrap sample - a random sample taken with replacement from the original sample, of the same size as the original sample. In this case it‚Äôll be 20.\n\n[2] Calculate the bootstrap statistic - a statistic such as mean, median, proportion, slope, etc. computed on the bootstrap samples. Here we compute the median.\n\n[3] Repeat steps (1) and (2) many times to create a bootstrap distribution - a distribution of bootstrap statistics.\nFinally we can make a histogram of those BT statistics, and [4] Calculate the bounds of the \\((1-\\alpha)100\\%\\) confidence interval as the middle \\((1-\\alpha)100\\%\\) of the bootstrap distribution."
  },
  {
    "objectID": "slides/11-bootstrap.html#r-lab-infer-in-tidymodels",
    "href": "slides/11-bootstrap.html#r-lab-infer-in-tidymodels",
    "title": "Simulation-based Inference üíª",
    "section": "\nR Lab infer üì¶ in tidymodels\n",
    "text": "R Lab infer üì¶ in tidymodels\n\n\n\n\nThe objective of this package is to perform statistical inference using an expressive statistical grammar that coheres with the tidyverse framework.\nhttps://infer.netlify.app/\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSo now we get the idea of BT. Let‚Äôs see how to do it in R.\nYou can write your own code to implement the BT.\nBut here, we use the infer package to do so. It is one of the tidymodels package.\nThe infer package performs statistical inference using an expressive statistical grammar that coheres with the tidyverse design framework.\nSo in R we have tidyverse for data wrangling and visualization, and we have tidymodels for inference and modeling.\nThe tidymodels framework is pretty young. It‚Äôs website was released last year. It is not very mature at this time, but I believe it will get more and more functionality and will be getting popular because its syntax is similar to tidyverse syntax.\nThe idea of using the infer package is basically shown in this workflow.\nWe start with specifying a hypothesis or response and predictors in general. In our case, we just specify the response variable of interest. Here our variable of interest is the rent.\nAnd once the variable of interest is specified, we can start generating data and calculating the sample statistic we want. So in our case, we generate our data using BT, and we calculate the median.\nAnd finally we can visualise the BT distribution and do the inference."
  },
  {
    "objectID": "slides/11-bootstrap.html#r-lab-generate-bootstrap-samples-of-medians",
    "href": "slides/11-bootstrap.html#r-lab-generate-bootstrap-samples-of-medians",
    "title": "Simulation-based Inference üíª",
    "section": "\nR Lab Generate Bootstrap Samples of Medians",
    "text": "R Lab Generate Bootstrap Samples of Medians\n\n\nspecify() the variable of interest.\n\n\nmanhattan |>\n    # specify the variable of interest\n    specify(response = rent) #<<\n\n\nAll right, let‚Äôs do it step by step.\nFirst we specify our response variable using specify(response = rent)"
  },
  {
    "objectID": "slides/11-bootstrap.html#r-lab-generate-bootstrap-samples-of-medians-1",
    "href": "slides/11-bootstrap.html#r-lab-generate-bootstrap-samples-of-medians-1",
    "title": "Simulation-based Inference üíª",
    "section": "\nR Lab Generate Bootstrap Samples of Medians",
    "text": "R Lab Generate Bootstrap Samples of Medians\n\n\nspecify() the variable of interest.\n\ngenerate() a fixed number of bootstrap samples.\n\n\nmanhattan |> \n    # specify the variable of interest\n    specify(response = rent) |>\n    # generate 15000 bootstrap samples\n    generate(reps = 15000, type = \"bootstrap\")  #<<\n\n\nAnd then we generate our BT samples using generate() function, reps 15000 times, and specify type = ‚Äúbootstrap‚Äù."
  },
  {
    "objectID": "slides/11-bootstrap.html#r-lab-generate-bootstrap-samples-of-medians-2",
    "href": "slides/11-bootstrap.html#r-lab-generate-bootstrap-samples-of-medians-2",
    "title": "Simulation-based Inference üíª",
    "section": "\nR Lab Generate Bootstrap Samples of Medians",
    "text": "R Lab Generate Bootstrap Samples of Medians\n\n\nspecify() the variable of interest.\n\ngenerate() a fixed number of bootstrap samples.\n\ncalculate() the bootstrapped statistic(s).\n\n\nmanhattan |> \n    # specify the variable of interest\n    specify(response = rent) |>\n    # generate 15000 bootstrap samples\n    generate(reps = 15000, type = \"bootstrap\") |>\n    # calculate the median of each bootstrap sample\n    calculate(stat = \"median\")  #<<\n\n\nOnce we collect or BT samples, we calculate the bootstrapped statistic(s) which is median using calculate() function.\nThe stat argument is ‚Äúmedian‚Äù"
  },
  {
    "objectID": "slides/11-bootstrap.html#r-lab-generate-bootstrap-samples-of-medians-3",
    "href": "slides/11-bootstrap.html#r-lab-generate-bootstrap-samples-of-medians-3",
    "title": "Simulation-based Inference üíª",
    "section": "\nR Lab Generate Bootstrap Samples of Medians",
    "text": "R Lab Generate Bootstrap Samples of Medians\n\n\nspecify() the variable of interest.\n\ngenerate() a fixed number of bootstrap samples.\n\ncalculate() the bootstrapped statistic(s).\nsave bootstrapping distribution for analysis.\n\n\n# save resulting bootstrap distribution\nboot_sample <- manhattan |> \n    # specify the variable of interest\n    specify(response = rent) |>  \n    # generate 15000 bootstrap samples\n    generate(reps = 15000, type = \"bootstrap\") |>  \n    # calculate the median of each bootstrap sample\n    calculate(stat = \"median\")\n\n\nThen we are done! We have the BT samples or distribution of median rent for analysis.\nI just save it to the object called boot_sample."
  },
  {
    "objectID": "slides/11-bootstrap.html#r-lab-the-bootstrap-sample",
    "href": "slides/11-bootstrap.html#r-lab-the-bootstrap-sample",
    "title": "Simulation-based Inference üíª",
    "section": "\nR Lab The bootstrap Sample",
    "text": "R Lab The bootstrap Sample\n\nüë§ How many observations are there in boot_sample? What does each observation represent?\n\n\ndplyr::glimpse(boot_sample)\n\nRows: 15,000\nColumns: 2\n$ replicate <int> 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 1‚Ä¶\n$ stat      <dbl> 2350, 2350, 2349, 2495, 2325, 2495, 2000, 2350, 2422, 2350, ‚Ä¶\n\n\n\nEach replicate median rent is calculated from a bootstrapped sample of size 20 from our original sample data.\n\n\nSo now you can see that we have 15000 median rents\nEach replicate median rent is calculated from a bootstrapped sample of size 20 from our original sample data."
  },
  {
    "objectID": "slides/11-bootstrap.html#r-lab-visualize-the-bootstrap-distribution",
    "href": "slides/11-bootstrap.html#r-lab-visualize-the-bootstrap-distribution",
    "title": "Simulation-based Inference üíª",
    "section": "\nR Lab Visualize the Bootstrap Distribution",
    "text": "R Lab Visualize the Bootstrap Distribution\n\nbt_dist <- ggplot(data = boot_sample, mapping = aes(x = stat)) +\n    geom_histogram(binwidth = 50) +\n    labs(title = \"Bootstrap distribution of medians\") + theme_bw()\nbt_dist\n\n\n\nAnd this is the bootstrap distribution of medians that is generated purely by our sample data, and it is generated without using normal assumptions or CLT."
  },
  {
    "objectID": "slides/11-bootstrap.html#r-lab-calculate-the-ci",
    "href": "slides/11-bootstrap.html#r-lab-calculate-the-ci",
    "title": "Simulation-based Inference üíª",
    "section": "\nR Lab Calculate the CI",
    "text": "R Lab Calculate the CI\n\n\n\nbt_ci <- infer::get_ci(\n  boot_sample, level = .95)\nbt_ci\n\n# A tibble: 1 √ó 2\n  lower_ci upper_ci\n     <dbl>    <dbl>\n1    2162.     2875\n\n\n\n\nCodebt_dist + geom_vline(xintercept = c(bt_ci$lower_ci, bt_ci$upper_ci), \n                     color = \"blue\") +\n    labs(subtitle = \"and 95% CI\") + theme_bw()\n\n\n\n\n\n\n\n\n\nA 95% confidence interval is bounded by the middle 95% of the bootstrap distribution."
  },
  {
    "objectID": "slides/11-bootstrap.html#quantify-the-variability-of-the-slope",
    "href": "slides/11-bootstrap.html#quantify-the-variability-of-the-slope",
    "title": "Simulation-based Inference üíª",
    "section": "Quantify the Variability of the Slope",
    "text": "Quantify the Variability of the Slope\nIn simple linear regression, we\n\nBootstrap new samples \\(\\{x^b_j, y^b_j\\}_{j=1}^n\\) from the original sample \\(\\{x_i, y_i\\}_{i=1}^n\\).\nFit models to each of the bootstrapped samples and estimate the slope.\nUse the distribution of the bootstrapped slopes to construct a confidence interval."
  },
  {
    "objectID": "slides/11-bootstrap.html#bootstrap-sample-1",
    "href": "slides/11-bootstrap.html#bootstrap-sample-1",
    "title": "Simulation-based Inference üíª",
    "section": "Bootstrap Sample 1",
    "text": "Bootstrap Sample 1"
  },
  {
    "objectID": "slides/11-bootstrap.html#bootstrap-sample-2",
    "href": "slides/11-bootstrap.html#bootstrap-sample-2",
    "title": "Simulation-based Inference üíª",
    "section": "Bootstrap Sample 2",
    "text": "Bootstrap Sample 2"
  },
  {
    "objectID": "slides/11-bootstrap.html#bootstrap-sample-3",
    "href": "slides/11-bootstrap.html#bootstrap-sample-3",
    "title": "Simulation-based Inference üíª",
    "section": "Bootstrap Sample 3",
    "text": "Bootstrap Sample 3"
  },
  {
    "objectID": "slides/11-bootstrap.html#bootstrap-sample-4",
    "href": "slides/11-bootstrap.html#bootstrap-sample-4",
    "title": "Simulation-based Inference üíª",
    "section": "Bootstrap Sample 4",
    "text": "Bootstrap Sample 4"
  },
  {
    "objectID": "slides/11-bootstrap.html#bootstrap-sample-5",
    "href": "slides/11-bootstrap.html#bootstrap-sample-5",
    "title": "Simulation-based Inference üíª",
    "section": "Bootstrap Sample 5",
    "text": "Bootstrap Sample 5\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nso on and so forth‚Ä¶"
  },
  {
    "objectID": "slides/11-bootstrap.html#bootstrap-samples-1---5",
    "href": "slides/11-bootstrap.html#bootstrap-samples-1---5",
    "title": "Simulation-based Inference üíª",
    "section": "Bootstrap Samples 1 - 5",
    "text": "Bootstrap Samples 1 - 5"
  },
  {
    "objectID": "slides/11-bootstrap.html#bootstrap-samples-1---100",
    "href": "slides/11-bootstrap.html#bootstrap-samples-1---100",
    "title": "Simulation-based Inference üíª",
    "section": "Bootstrap Samples 1 - 100",
    "text": "Bootstrap Samples 1 - 100"
  },
  {
    "objectID": "slides/11-bootstrap.html#slopes-of-bootstrap-samples",
    "href": "slides/11-bootstrap.html#slopes-of-bootstrap-samples",
    "title": "Simulation-based Inference üíª",
    "section": "Slopes of bootstrap samples",
    "text": "Slopes of bootstrap samples\n\n\n\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\n‚Ñπ Please use `linewidth` instead."
  },
  {
    "objectID": "slides/11-bootstrap.html#bootstrapped-ci",
    "href": "slides/11-bootstrap.html#bootstrapped-ci",
    "title": "Simulation-based Inference üíª",
    "section": "Bootstrapped CI",
    "text": "Bootstrapped CI\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nA 95% confidence interval is bounded by the middle 95% of the bootstrap distribution."
  },
  {
    "objectID": "slides/11-bootstrap.html#r-lab-fit-regression-to-bootstrap-samples",
    "href": "slides/11-bootstrap.html#r-lab-fit-regression-to-bootstrap-samples",
    "title": "Simulation-based Inference üíª",
    "section": "\nR Lab Fit Regression to Bootstrap Samples",
    "text": "R Lab Fit Regression to Bootstrap Samples\n\n\n\n\nset.seed(2023)\nboot_fits <- delivery_data |> \n  specify(time ~ cases) |> \n  generate(reps = 100, type = \"bootstrap\") |> \n  fit()\nboot_fits\n\n\n\n\n# A tibble: 200 √ó 3\n# Groups:   replicate [100]\n  replicate term      estimate\n      <int> <chr>        <dbl>\n1         1 intercept    0.738\n2         1 cases        2.54 \n3         2 intercept    2.24 \n4         2 cases        2.33 \n5         3 intercept    4.16 \n6         3 cases        2.14 \n# ‚Ñπ 194 more rows"
  },
  {
    "objectID": "slides/11-bootstrap.html#r-lab-bootstrap-ci",
    "href": "slides/11-bootstrap.html#r-lab-bootstrap-ci",
    "title": "Simulation-based Inference üíª",
    "section": "\nR Lab Bootstrap CI",
    "text": "R Lab Bootstrap CI\n\n\nPercentile method: Compute the 95% CI as the middle 95% of the bootstrap distribution:\n\n\nobserved_fit <- delivery_data |> \n  specify(time ~ cases) |> \n  fit()\nboot_fits |> get_ci(point_estimate = observed_fit, type = \"percentile\")\n\n# A tibble: 2 √ó 3\n  term      lower_ci upper_ci\n  <chr>        <dbl>    <dbl>\n1 cases        1.73      2.48\n2 intercept    0.485     6.81\n\n\n\n\n\nStandard error method: Compute the 95% CI as the point estimate \\(\\pm\\) ~2 standard deviations of the bootstrap distribution:\n\n\nboot_fits |> get_ci(point_estimate = observed_fit, type = \"se\")\n\n# A tibble: 2 √ó 3\n  term      lower_ci upper_ci\n  <chr>        <dbl>    <dbl>\n1 cases        1.68      2.68\n2 intercept   -0.341     6.98\n\n\nconfint(lm(time~cases, data = delivery_data))"
  },
  {
    "objectID": "slides/11-bootstrap.html#r-lab-carboot",
    "href": "slides/11-bootstrap.html#r-lab-carboot",
    "title": "Simulation-based Inference üíª",
    "section": "\nR Lab car::Boot()\n",
    "text": "R Lab car::Boot()\n\n\n\ncar::Boot() is a simple interface to the boot::boot().\n\n\nlm_fit <- lm(time ~ cases, data = delivery_data)\ncar_boot <- car::Boot(lm_fit, R = 100)\nbrief(car_boot$t)\n\n100 x 2 matrix (95 rows omitted)\n       (Intercept) cases\n  [1,]       1.159  2.40\n  [2,]       3.537  2.11\n  [3,]       0.961  2.35\n. . .                        \n [99,]       2.757  2.24\n[100,]       2.415  2.35"
  },
  {
    "objectID": "slides/11-bootstrap.html#r-lab-histogram-of-bootstrap-samples",
    "href": "slides/11-bootstrap.html#r-lab-histogram-of-bootstrap-samples",
    "title": "Simulation-based Inference üíª",
    "section": "\nR Lab Histogram of Bootstrap Samples",
    "text": "R Lab Histogram of Bootstrap Samples\n\nBC\\(_a\\) is the accelerated bias-corrected percentile interval.\n\n\nhist(car_boot)"
  },
  {
    "objectID": "slides/11-bootstrap.html#r-lab-carconfint",
    "href": "slides/11-bootstrap.html#r-lab-carconfint",
    "title": "Simulation-based Inference üíª",
    "section": "\nR Lab car::Confint()\n",
    "text": "R Lab car::Confint()\n\n\ncar::Confint(lm_fit, vcov. = vcov(car_boot))\n\nStandard errors computed by vcov(car_boot) \n\n\n            Estimate  2.5 % 97.5 %\n(Intercept)     3.32 -0.611   7.25\ncases           2.18  1.613   2.74\n\n\ncheck boot::boot.ci\n\n\n\nmath4780-f23.github.io/website"
  },
  {
    "objectID": "slides/06-mat.html#matrix",
    "href": "slides/06-mat.html#matrix",
    "title": "Matrix Algebra üë®‚Äçüíª",
    "section": "Matrix",
    "text": "Matrix\n\nA matrix \\({\\bf A}\\) that has \\(n\\) rows and \\(m\\) columns is defined as \\[{\\bf A} = (a_{ij})_{n \\times m}\\begin{bmatrix} a_{11} & a_{12} & \\cdots & a_{1m} \\\\ a_{21} & a_{22} & \\cdots & a_{2m} \\\\ \\vdots  & \\vdots  & \\ddots & \\vdots  \\\\ a_{n1} & a_{n2} & \\cdots & a_{nm} \\end{bmatrix}_{n \\times m}\\]\n\n\n\n\n\n\n(A <- matrix(data = 1:6, \n             nrow = 2, ncol = 3))\n\n     [,1] [,2] [,3]\n[1,]    1    3    5\n[2,]    2    4    6\n\nclass(A)\n\n[1] \"matrix\" \"array\" \n\n\n\n\nattributes(A)\n\n$dim\n[1] 2 3\n\n\n\n\n\n\n\nBut what is the geometrical meaning of matrices?\n\n\nFirst index is for row and the second index is for column\nUse command matrix() to create a matrix.\nA matrix is a two-dimensional analog of a vector.\nLike all elements of a matrix must be of the same type."
  },
  {
    "objectID": "slides/06-mat.html#section-1",
    "href": "slides/06-mat.html#section-1",
    "title": "Matrix Algebra üë®‚Äçüíª",
    "section": "",
    "text": "https://gfycat.com/cooperativequerulousfirefly (3Blue1Brown)\nA matrix is actually a numerical representation of a linear transformation in a linear space which is a function that takes an vector as an input and produces another vector as an output.\nIf we use the grid lines to represent the entire coordinate system, we will see that when we use (1, 0) and (0, 1) as the two bases, all lines are vertical and horizontal, showing, for example, the integer values of each coordinate.\nLinear transformation means we stretch or rotate the lines, but at the same time, all lines remains lines and origin remain fixed.\nGrids lines are parallel and evenly spaced after linear transformation.\n(-1, 3) is the transformed vector of (-1, 2) when the two new transformed basis are (3, 1) and (1, 2)"
  },
  {
    "objectID": "slides/06-mat.html#column-vector",
    "href": "slides/06-mat.html#column-vector",
    "title": "Matrix Algebra üë®‚Äçüíª",
    "section": "Column Vector",
    "text": "Column Vector\n\nIf \\(m = 1\\), it becomes a \\(n\\) by 1 matrix or a column vector of size \\(n\\). \\[\\small {\\bf y} = \\begin{bmatrix} y_{1}  \\\\ y_{2}  \\\\ \\vdots  \\\\  y_{n} \\end{bmatrix}_{n \\times 1} \\quad {\\bf 1} = \\begin{bmatrix} 1  \\\\ 1  \\\\ \\vdots  \\\\  1 \\end{bmatrix}_{n \\times 1}\\]\n\n\n\n\n\n\nA\n\n     [,1] [,2] [,3]\n[1,]    1    3    5\n[2,]    2    4    6\n\n## 2nd column\nA[, 2]\n\n[1] 3 4\n\n## becomes a numeric vector\nclass(A[, 2]) \n\n[1] \"integer\"\n\n\n\n\n## keep its matrix class\nA[, 2, drop = FALSE] \n\n     [,1]\n[1,]    3\n[2,]    4\n\nclass(A[, 2, drop = FALSE])  \n\n[1] \"matrix\" \"array\""
  },
  {
    "objectID": "slides/06-mat.html#row-vector",
    "href": "slides/06-mat.html#row-vector",
    "title": "Matrix Algebra üë®‚Äçüíª",
    "section": "Row Vector",
    "text": "Row Vector\n\nIf \\(n = 1\\), it becomes a \\(1\\) by \\(m\\) matrix or a row vector of size \\(m\\). \\[{\\bf x} = \\begin{bmatrix} x_{1} &  x_{2}  & \\dots  &  x_{m} \\end{bmatrix}_{1 \\times m}\\]\n\nBy default, a vector means a column vector.\n\n\n## 2nd row\nA[2, , drop = FALSE]\n\n     [,1] [,2] [,3]\n[1,]    2    4    6\n\n## keep its matrix class\nclass(A[2, , drop = FALSE]) \n\n[1] \"matrix\" \"array\""
  },
  {
    "objectID": "slides/06-mat.html#transpose",
    "href": "slides/06-mat.html#transpose",
    "title": "Matrix Algebra üë®‚Äçüíª",
    "section": "Transpose",
    "text": "Transpose\n\nLet \\({\\bf A}\\) be an \\(n \\times m\\) matrix. The transpose of \\({\\bf A}\\), denoted by \\({\\bf A}'\\) or \\({\\bf A}^T\\), is a \\(m \\times n\\) matrix whose columns are the rows of \\({\\bf A}\\). \\[{\\bf A} = \\begin{bmatrix} 1 & 2 & 3 \\\\ 4 & 5 & 6 \\\\ \\end{bmatrix} \\quad {\\bf A}' =\\begin{bmatrix} 1 & 4 \\\\ 2 & 5 \\\\ 3 & 6\\end{bmatrix}\\] \\[{\\bf X}_{n \\times 2}\\begin{bmatrix} 1 & x_1 \\\\ 1 & x_{2} \\\\ \\vdots  & \\vdots  \\\\ 1 & x_n \\end{bmatrix} = \\begin{bmatrix} {\\bf 1} & {\\bf x} \\end{bmatrix}\\] \\[{\\bf X}'_{2 \\times n} = \\begin{bmatrix} 1 & 1 & \\cdots & 1 \\\\ x_1 & x_2 & \\cdots & x_n \\end{bmatrix} = \\begin{bmatrix} {\\bf 1}' \\\\ {\\bf x}' \\end{bmatrix}\\]"
  },
  {
    "objectID": "slides/06-mat.html#transpose-in-r",
    "href": "slides/06-mat.html#transpose-in-r",
    "title": "Matrix Algebra üë®‚Äçüíª",
    "section": "Transpose in R",
    "text": "Transpose in R\n\nA\n\n     [,1] [,2] [,3]\n[1,]    1    3    5\n[2,]    2    4    6\n\nt(A)\n\n     [,1] [,2]\n[1,]    1    2\n[2,]    3    4\n[3,]    5    6"
  },
  {
    "objectID": "slides/06-mat.html#linear-independence",
    "href": "slides/06-mat.html#linear-independence",
    "title": "Matrix Algebra üë®‚Äçüíª",
    "section": "Linear Independence",
    "text": "Linear Independence\n\n\n\nVectors \\({\\bf v}_1, {\\bf v}_2, \\dots, {\\bf v}_k\\) is said to be linearly independent if for scalars \\(c_{1},c_{2},\\dots,c_{k} \\in \\mathbf{R},\\) \\[c_{1}{\\bf v}_1 + c_{2}{\\bf v}_2 + \\cdots + c_{k}{\\bf v}_k = \\bf 0\\] can only be satisfied by \\(c_i = 0\\) for all \\(i = 1, 2, \\dots, k\\).\n\n\nAre \\({\\bf v}_1 = (1, 1)'\\) and \\({\\bf v}_2 = (-3, 2)'\\) linearly independent?\n\n\nWays to check linear independence:\n\nsolve the homogeneous linear system \\(\\begin{bmatrix} 1 & -3 \\\\ 1 & 2 \\end{bmatrix}\\begin{bmatrix} c_1 \\\\ c_2 \\end{bmatrix}=\\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix}\\) for \\(c_1\\) and \\(c_2\\).\ncheck if \\(\\text{det} \\left( \\begin{bmatrix} 1 & -3 \\\\ 1 & 2 \\end{bmatrix} \\right)\\) is non-zero."
  },
  {
    "objectID": "slides/06-mat.html#linear-independence-1",
    "href": "slides/06-mat.html#linear-independence-1",
    "title": "Matrix Algebra üë®‚Äçüíª",
    "section": "Linear Independence",
    "text": "Linear Independence\n\nV <- matrix(c(1, 1, -3, 2), 2, 2)\nsolve(V, b = rep(0, 2))\n\n[1] 0 0\n\ndet(V)\n\n[1] 5\n\n\n\nA is invertible, that is, A has an inverse, is nonsingular, and is nondegenerate.\nA is row-equivalent to the n-by-n identity matrix In.\nA is column-equivalent to the n-by-n identity matrix In.\nA has n pivot positions.\nA has full rank; that is, rank A = n.\nBased on the rank A=n, the equation Ax = 0 has only the trivial solution x = 0. and the equation Ax = b has exactly one solution for each b in Kn.\nThe kernel (null space) of A is trivial\nThe columns of A are linearly independent.\nThe columns of A span Kn.\nThe columns of A form a basis of Kn.\ndet A ‚â† 0.\nThe number 0 is not an eigenvalue of A.\nThe transpose AT is an invertible matrix"
  },
  {
    "objectID": "slides/06-mat.html#rank",
    "href": "slides/06-mat.html#rank",
    "title": "Matrix Algebra üë®‚Äçüíª",
    "section": "Rank",
    "text": "Rank\n\nThe rank of a matrix \\({\\bf A} = \\begin{bmatrix} {\\bf a}_1 & {\\bf a}_2 & \\cdots & {\\bf a}_m \\end{bmatrix}\\) is the number of linearly independent columns (dimension of the column space).\nIf \\(k\\) of the \\(m\\) column vectors \\({\\bf a}_1, {\\bf a}_2, \\dots, {\\bf a}_m\\) are linearly independent, the rank of \\({\\bf A}\\) is \\(k\\).\nThe remaining \\(m-k\\) columns of \\({\\bf A}\\) can be written as a linear combination of the \\(k\\) linearly independent columns.\n\n\nIf \\(k = m\\), it is full rank\nIf \\(k\\) of the \\(n\\) column vectors \\({\\bf a}_1, {\\bf a}_2, \\dots, {\\bf a}_n\\) are linearly independent, the rank of \\({\\bf A}\\) is \\(k\\). The remaining \\(n-k\\) columns can be written as a linear combination of the \\(k\\) columns.\nThe vectors \\({\\bf v}_1, {\\bf v}_2, \\dots, {\\bf v}_k\\) is said to be linearly dependent if there exist scalars \\(c_{1},c_{2},\\dots,c_{k},\\) not all zero, such that \\[c_{1}{\\bf v}_1 + c_{2}{\\bf v}_2 + \\cdots + c_{k}{\\bf v}_k = \\bf 0\\]\n\nThe column vectors \\({\\bf v}_1, {\\bf v}_2, \\dots, {\\bf v}_k\\) is said to be linearly independent if it is not linearly dependent, that is, if \\[c_{1}{\\bf v}_1 + c_{2}{\\bf v}_2 + \\cdots + c_{k}{\\bf v}_k = \\bf 0\\] can only be satisfied by \\(c_i = 0, i = 1, 2, \\dots, k.\\)"
  },
  {
    "objectID": "slides/06-mat.html#rank-1",
    "href": "slides/06-mat.html#rank-1",
    "title": "Matrix Algebra üë®‚Äçüíª",
    "section": "Rank",
    "text": "Rank\n\n(M <- matrix(1:9, nrow = 3, ncol = 3))\n\n     [,1] [,2] [,3]\n[1,]    1    4    7\n[2,]    2    5    8\n[3,]    3    6    9\n\n# install.packages(\"Matrix\")\nlibrary(Matrix)\nrankMatrix(M)[1]\n\n[1] 2\n\n\n\nCan you see why the rank is 2, meaning that one column can be written as a linear combo of the other two?\n\n\n\n\\({\\bf a}_3 = 2{\\bf a_2}-{\\bf a}_1\\)\n\n\n2 * M[, 2] - 1 * M[, 1]\n\n[1] 7 8 9\n\n\n\nFind the row or column basis of A using reduced row echelon form\nthe dimension of the vector space generated (or spanned) by its columns\ndimension: number of basis in the vector space."
  },
  {
    "objectID": "slides/06-mat.html#operations-addition",
    "href": "slides/06-mat.html#operations-addition",
    "title": "Matrix Algebra üë®‚Äçüíª",
    "section": "Operations: Addition",
    "text": "Operations: Addition\n\nAddition: \\({\\bf A + B}\\) is adding the corresponding elements together \\(a_{ij} + b_{ij}\\).\n\\({\\bf A}\\) and \\({\\bf B}\\) must have an equal number of rows and columns.\n\n\n\n\nA\n\n     [,1] [,2] [,3]\n[1,]    1    3    5\n[2,]    2    4    6\n\n(B <- matrix(6:1, nrow = 2, ncol = 3))\n\n     [,1] [,2] [,3]\n[1,]    6    4    2\n[2,]    5    3    1\n\nA + B\n\n     [,1] [,2] [,3]\n[1,]    7    7    7\n[2,]    7    7    7\n\n\n\n\n\n(B <- matrix(1:4, nrow = 2, ncol = 2))\n\n     [,1] [,2]\n[1,]    1    3\n[2,]    2    4\n\nA + B\n\nError in A + B: non-conformable arrays"
  },
  {
    "objectID": "slides/06-mat.html#operations-multiplication",
    "href": "slides/06-mat.html#operations-multiplication",
    "title": "Matrix Algebra üë®‚Äçüíª",
    "section": "Operations: Multiplication",
    "text": "Operations: Multiplication\n\n\nMultiplication: The product of matrices \\({\\bf A}\\) and \\({\\bf B}\\) is denoted as \\({\\bf AB}\\).\n\n\n\nThe number of columns in \\({\\bf A}\\) must be equal to the number of rows \\({\\bf B}\\).\nThe result matrix \\({\\bf C}\\) has the number of rows of \\({\\bf A}\\) and the number of columns of the \\({\\bf B}\\).\n\n\nThe number of columns in the first matrix \\({\\bf A}\\) must be equal to the number of rows in the second matrix of \\({\\bf B}\\).\nThe resulting matrix \\({\\bf C}\\) has dimension of the number of rows of the first and the number of columns of the second matrix."
  },
  {
    "objectID": "slides/06-mat.html#operations-multiplication-1",
    "href": "slides/06-mat.html#operations-multiplication-1",
    "title": "Matrix Algebra üë®‚Äçüíª",
    "section": "Operations: Multiplication",
    "text": "Operations: Multiplication\n\n\n\nA\n\n     [,1] [,2] [,3]\n[1,]    1    3    5\n[2,]    2    4    6\n\n(B <- matrix(1:12, nrow = 3, ncol = 4))\n\n     [,1] [,2] [,3] [,4]\n[1,]    1    4    7   10\n[2,]    2    5    8   11\n[3,]    3    6    9   12\n\n\n\n\n\n(C <- A %*% B)\n\n     [,1] [,2] [,3] [,4]\n[1,]   22   49   76  103\n[2,]   28   64  100  136\n\n(B <- matrix(1:8, nrow = 2, ncol = 4))\n\n     [,1] [,2] [,3] [,4]\n[1,]    1    3    5    7\n[2,]    2    4    6    8\n\nA %*% B\n\nError in A %*% B: non-conformable arguments"
  },
  {
    "objectID": "slides/06-mat.html#operations-multiplication-2",
    "href": "slides/06-mat.html#operations-multiplication-2",
    "title": "Matrix Algebra üë®‚Äçüíª",
    "section": "Operations: Multiplication",
    "text": "Operations: Multiplication\n\n\\({\\bf A}{\\bf B} \\ne {\\bf B}{\\bf A}\\) in general.\n\\(({\\bf A}{\\bf B})' = {\\bf B}'{\\bf A}'\\).\n\n\n\n\n\n(C <- matrix(1:4, 2, 2))\n\n     [,1] [,2]\n[1,]    1    3\n[2,]    2    4\n\n(D <- matrix(2:5, 2, 2))\n\n     [,1] [,2]\n[1,]    2    4\n[2,]    3    5\n\nC %*% D\n\n     [,1] [,2]\n[1,]   11   19\n[2,]   16   28\n\n\n\n\n\nD %*% C\n\n     [,1] [,2]\n[1,]   10   22\n[2,]   13   29\n\nt(C %*% D)\n\n     [,1] [,2]\n[1,]   11   16\n[2,]   19   28\n\nt(D) %*% t(C)\n\n     [,1] [,2]\n[1,]   11   16\n[2,]   19   28"
  },
  {
    "objectID": "slides/06-mat.html#special-matrices-symmetric-and-identity",
    "href": "slides/06-mat.html#special-matrices-symmetric-and-identity",
    "title": "Matrix Algebra üë®‚Äçüíª",
    "section": "Special Matrices: Symmetric and Identity",
    "text": "Special Matrices: Symmetric and Identity\n\nA square matrix \\((m = n)\\) \\({\\bf A}_{n \\times n}\\) is a symmetric matrix if \\({\\bf A = A}'\\).\n\n\\[{\\bf A} = \\begin{bmatrix} 1 & 2  \\\\ 2 & 5 \\\\ \\end{bmatrix} \\quad {\\bf A}' =\\begin{bmatrix} 1 & 2 \\\\ 2 & 5 \\end{bmatrix}\\]\n\nA square matrix \\({\\bf I}_{n \\times n}\\) whose diagonal elements are 1‚Äôs and off diagonal elements are 0‚Äôs is called an identity matrix of order \\(n\\). \\[{\\bf I} = \\begin{bmatrix} 1 & 0 & \\cdots & 0 \\\\ 0 & 1 & \\cdots & 0 \\\\ \\vdots  & \\vdots  & \\ddots & \\vdots  \\\\ 0 & 0 & \\cdots & 1 \\end{bmatrix}\\]\n\n\n\nIf the inverse exists, it is unique."
  },
  {
    "objectID": "slides/06-mat.html#identity-and-diagonal-matrix",
    "href": "slides/06-mat.html#identity-and-diagonal-matrix",
    "title": "Matrix Algebra üë®‚Äçüíª",
    "section": "Identity and Diagonal Matrix",
    "text": "Identity and Diagonal Matrix\n\n\n\n## Identity matrix\nI <- diag(5)\nI\n\n     [,1] [,2] [,3] [,4] [,5]\n[1,]    1    0    0    0    0\n[2,]    0    1    0    0    0\n[3,]    0    0    1    0    0\n[4,]    0    0    0    1    0\n[5,]    0    0    0    0    1\n\n## A diagonal matrix\ndiag(c(4, 2, 5))\n\n     [,1] [,2] [,3]\n[1,]    4    0    0\n[2,]    0    2    0\n[3,]    0    0    5\n\n\n\n\n## Extract diagonal elements\nD <- matrix(1:4, 2, 2)\nD\n\n     [,1] [,2]\n[1,]    1    3\n[2,]    2    4\n\ndiag(D)\n\n[1] 1 4"
  },
  {
    "objectID": "slides/06-mat.html#inverse-matrix",
    "href": "slides/06-mat.html#inverse-matrix",
    "title": "Matrix Algebra üë®‚Äçüíª",
    "section": "Inverse Matrix",
    "text": "Inverse Matrix\n\nThe inverse of a square matrix \\({\\bf A}\\), denoted by \\({\\bf A}^{-1}\\), is a square matrix such that \\[{\\bf A}^{-1}{\\bf A} = {\\bf A}{\\bf A}^{-1} = {\\bf I}\\]\n\n\n\n\n\nD\n\n     [,1] [,2]\n[1,]    1    3\n[2,]    2    4\n\nD_inv <- solve(D)\nD_inv\n\n     [,1] [,2]\n[1,]   -2  1.5\n[2,]    1 -0.5\n\n\n\n\nD_inv %*% D\n\n     [,1] [,2]\n[1,]    1    0\n[2,]    0    1\n\nD %*% D_inv\n\n     [,1] [,2]\n[1,]    1    0\n[2,]    0    1"
  },
  {
    "objectID": "slides/06-mat.html#idempotent-orthogonal-positive-definite",
    "href": "slides/06-mat.html#idempotent-orthogonal-positive-definite",
    "title": "Matrix Algebra üë®‚Äçüíª",
    "section": "Idempotent, Orthogonal, Positive Definite",
    "text": "Idempotent, Orthogonal, Positive Definite\n\nA square matrix \\({\\bf A}\\) is idempotent if \\({\\bf A}{\\bf A} = {\\bf A}\\).\nA square matrix \\({\\bf A}\\) is orthogonal if \\({\\bf A}^{-1} = {\\bf A}'\\) and hence \\({\\bf A}'{\\bf A} = {\\bf I}\\).\n\n\n\nLet \\({\\bf A}\\) be a \\(n \\times n\\) matrix and \\({\\bf y}\\) be a \\(n \\times 1\\) vector. \\[{\\bf y}'{\\bf A} {\\bf y} = \\sum_{i=1}^n\\sum_{j=1}^na_{ij}y_iy_j\\] is called a quadratic form of \\({\\bf y}\\).\n\n\n\n\nA symmetric matrix \\({\\bf A}\\) is said to be \n\n\npositive definite if \\({\\bf y}'{\\bf A} {\\bf y} > 0\\) for all \\({\\bf y \\ne 0}\\).\n\npositive semi-definite if \\({\\bf y}'{\\bf A} {\\bf y} \\ge 0\\) for all \\({\\bf y \\ne 0}\\) and \\({\\bf y}'{\\bf A} {\\bf y} = 0\\) for some \\({\\bf y \\ne 0}\\).\n\n\nIf \\({\\bf A}\\) is symmetric and idempotent, then \\({\\bf A}\\) is positive semi-definite.\n\n\nHow about negative definite"
  },
  {
    "objectID": "slides/06-mat.html#special-matrices-idempotent-orthogonal",
    "href": "slides/06-mat.html#special-matrices-idempotent-orthogonal",
    "title": "Matrix Algebra üë®‚Äçüíª",
    "section": "Special Matrices: Idempotent, Orthogonal",
    "text": "Special Matrices: Idempotent, Orthogonal\n\n\n\n## Idempotent and symmetric matrix\n(A <- matrix(1, 3, 3)/3)\n\n     [,1] [,2] [,3]\n[1,] 0.33 0.33 0.33\n[2,] 0.33 0.33 0.33\n[3,] 0.33 0.33 0.33\n\nA %*% A\n\n     [,1] [,2] [,3]\n[1,] 0.33 0.33 0.33\n[2,] 0.33 0.33 0.33\n[3,] 0.33 0.33 0.33\n\n## positive semi-definite\ny <- c(-2, 5, 2)\nt(y) %*% A %*% y\n\n     [,1]\n[1,]  8.3\n\n\n\n\n## Orthogonal matrix\n(E <- matrix(c(1, -1, 1, 1)/sqrt(2), 2, 2))\n\n      [,1] [,2]\n[1,]  0.71 0.71\n[2,] -0.71 0.71\n\nE %*% t(E)\n\n        [,1]    [,2]\n[1,] 1.0e+00 2.2e-17\n[2,] 2.2e-17 1.0e+00\n\nt(E) %*% E\n\n         [,1]     [,2]\n[1,]  1.0e+00 -2.2e-17\n[2,] -2.2e-17  1.0e+00"
  },
  {
    "objectID": "slides/06-mat.html#trace",
    "href": "slides/06-mat.html#trace",
    "title": "Matrix Algebra üë®‚Äçüíª",
    "section": "Trace",
    "text": "Trace\n\nThe trace of \\({\\bf A}_{n \\times n}\\), denoted by \\(\\text{tr}({\\bf A})\\), is defined as \\[\\text{tr}({\\bf A}) = a_{11} + a_{22} + \\dots + a_{nn}\\]\n\\(\\text{tr}({\\bf A}{\\bf B}) = \\text{tr}({\\bf B}{\\bf A})\\)\n\n\n(G <- matrix(1:9, 3, 3))\n\n     [,1] [,2] [,3]\n[1,]    1    4    7\n[2,]    2    5    8\n[3,]    3    6    9\n\nsum(diag(G))\n\n[1] 15"
  },
  {
    "objectID": "slides/06-mat.html#eigenvalues-and-eigenvectors",
    "href": "slides/06-mat.html#eigenvalues-and-eigenvectors",
    "title": "Matrix Algebra üë®‚Äçüíª",
    "section": "Eigenvalues and Eigenvectors",
    "text": "Eigenvalues and Eigenvectors\n\nFor a square matrix \\({\\bf A}\\), if we can find a scalar \\(\\lambda\\) and a non-zero vector \\({\\bf x}\\) such that \\[{\\bf Ax} = \\lambda {\\bf x},\\]\n\n\n\\(\\lambda\\) is an eigenvalue of \\({\\bf A}\\)\n\n\n\\({\\bf x}\\) is a corresponding eigenvector of \\({\\bf A}\\).\n\n\nA \\(n \\times n\\) matrix \\({\\bf A}\\) will have \\(n\\) eigenvalues and \\(n\\) corresponding eigenvectors."
  },
  {
    "objectID": "slides/06-mat.html#section-2",
    "href": "slides/06-mat.html#section-2",
    "title": "Matrix Algebra üë®‚Äçüíª",
    "section": "",
    "text": "https://gfycat.com/ifr/WickedSaltyAnemonecrab (3Blue1Brown)"
  },
  {
    "objectID": "slides/06-mat.html#eigen-decomposition",
    "href": "slides/06-mat.html#eigen-decomposition",
    "title": "Matrix Algebra üë®‚Äçüíª",
    "section": "Eigen-decomposition",
    "text": "Eigen-decomposition\n\nEigen-decomposition: Any symmetric matrix \\({\\bf A}_{n \\times n}\\) can be decomposed as \\[{\\bf A} = {\\bf V\\boldsymbol \\Lambda V'}\\]\n\\(\\boldsymbol \\Lambda\\) is a \\(n \\times n\\) diagnonal matrix whose elements are eigenvalues \\(\\lambda_j\\) of \\({\\bf A}\\) \\[\\boldsymbol \\Lambda= \\begin{bmatrix} \\lambda_1 & 0 & \\cdots & 0 \\\\ 0 & \\lambda_2 & \\cdots & 0 \\\\ \\vdots  & \\vdots  & \\ddots & \\vdots  \\\\ 0 & 0 & \\cdots & \\lambda_n \\end{bmatrix}\\]\n\\({\\bf V} = [{\\bf v}_1 \\quad {\\bf v}_2 \\quad \\dots \\quad {\\bf v}_n]\\) is a \\(n \\times n\\) orthogonal matrix whose columns are the eigenvectors of \\({\\bf A}.\\)"
  },
  {
    "objectID": "slides/06-mat.html#eigenvalues-and-eigenvectors-1",
    "href": "slides/06-mat.html#eigenvalues-and-eigenvectors-1",
    "title": "Matrix Algebra üë®‚Äçüíª",
    "section": "Eigenvalues and Eigenvectors",
    "text": "Eigenvalues and Eigenvectors\n\n\\(\\text{tr}({\\bf A}) = \\sum_{i=1}^n\\lambda_i\\)\n\\(|{\\bf A}| = \\prod_{i=1}^n \\lambda_i\\)\n\n\\(\\text{rank}({\\bf A}) =\\) the number of non-zero \\(\\lambda_i\\)\n\nIf \\({\\bf A}\\) is symmetric, all \\(\\lambda_i \\in \\mathbf{R}\\)"
  },
  {
    "objectID": "slides/06-mat.html#eigenvalues-and-eigenvectors-2",
    "href": "slides/06-mat.html#eigenvalues-and-eigenvectors-2",
    "title": "Matrix Algebra üë®‚Äçüíª",
    "section": "Eigenvalues and Eigenvectors",
    "text": "Eigenvalues and Eigenvectors\n\n\n\n(A <- matrix(c(3, 1, 1, 2), 2, 2))\n\n     [,1] [,2]\n[1,]    3    1\n[2,]    1    2\n\neigen_decomp <- eigen(A)\n(lam <- eigen_decomp$values)\n\n[1] 3.6 1.4\n\n(V <- eigen_decomp$vectors)\n\n      [,1]  [,2]\n[1,] -0.85  0.53\n[2,] -0.53 -0.85\n\n\n\n\nV %*% diag(lam) %*% t(V)\n\n     [,1] [,2]\n[1,]    3    1\n[2,]    1    2\n\nsum(diag(A))\n\n[1] 5\n\nsum(lam)\n\n[1] 5\n\ndet(A)\n\n[1] 5\n\nprod(lam)\n\n[1] 5\n\n\n\n\n\n\n\nmath4780-f23.github.io/website"
  },
  {
    "objectID": "slides/10-diag-linearity.html#assumptions-of-linear-regression",
    "href": "slides/10-diag-linearity.html#assumptions-of-linear-regression",
    "title": "Regression Diagnostics - Linearity ‚ñ∂Ô∏è",
    "section": "Assumptions of Linear Regression",
    "text": "Assumptions of Linear Regression\n\\(Y_i= \\beta_0 + \\beta_1X_{i1} + \\beta_2X_{i2} + \\dots + \\beta_kX_{ik} + \\epsilon_i\\)\n\n\n\n\n\\(E(Y \\mid X)\\) and \\(X\\) are linearly related.\n\\(\\small E(\\epsilon_i) = 0\\)\n\\(\\small \\mathrm{Var}(\\epsilon_i) = \\sigma^2\\)\n\n\\(\\small \\mathrm{Cov}(\\epsilon_i, \\epsilon_j) = 0\\) for all \\(i \\ne j\\).\n\n\\(\\small \\epsilon_i \\stackrel{iid}{\\sim} N(0, \\sigma^2)\\) (for statistical inference)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAssuming \\(E(\\epsilon) = 0\\) implies that the regression surface captures the dependency of the conditional mean of \\(y\\) on the \\(x\\)s.\nViolating linearity implies that the model fails to represent the relationship between the mean response and the regressors. (Lack of fit)"
  },
  {
    "objectID": "slides/10-diag-linearity.html#detecting-nonlinearity-cia-example",
    "href": "slides/10-diag-linearity.html#detecting-nonlinearity-cia-example",
    "title": "Regression Diagnostics - Linearity ‚ñ∂Ô∏è",
    "section": "Detecting Nonlinearity (CIA Example)",
    "text": "Detecting Nonlinearity (CIA Example)\n\nScatterplot \\(y\\) against each \\(x\\) can be misleading! It shows the marginal relationship between \\(y\\) and each \\(x\\), without controlling the level of other regressors."
  },
  {
    "objectID": "slides/10-diag-linearity.html#detecting-nonlinearity-residual-plots",
    "href": "slides/10-diag-linearity.html#detecting-nonlinearity-residual-plots",
    "title": "Regression Diagnostics - Linearity ‚ñ∂Ô∏è",
    "section": "Detecting Nonlinearity: Residual Plots",
    "text": "Detecting Nonlinearity: Residual Plots\n\nCare about the partial relationship between \\(y\\) and each \\(x\\) with impact of other \\(x\\)s controlled.\nResidual-based plots are more relevant in detecting the departure of linearity.\n\n\n\n\n\nResidual plots cannot distinguish between monotone and non-monotone nonlinearity.\nThe distinction:\n\nMonotone: just transform \\(x\\) to \\(x^2\\)\n\nNon-monotone: need quadratic form\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe OLS ensures that the residuals and fitted values are uncorrelated."
  },
  {
    "objectID": "slides/10-diag-linearity.html#detecting-nonlinearity-partial-residual-plots",
    "href": "slides/10-diag-linearity.html#detecting-nonlinearity-partial-residual-plots",
    "title": "Regression Diagnostics - Linearity ‚ñ∂Ô∏è",
    "section": "Detecting Nonlinearity: Partial Residual Plots",
    "text": "Detecting Nonlinearity: Partial Residual Plots\n\nPartial residual plots (Component-plus-Residual Plot) are for diagnosing nonlinearity.\n\nThe partial residuals for \\(x_j\\): \\[e_i^{(j)} = b_jx_{ij} + e_i\\]\n\n\n\\(b_j\\) is the coefficient of \\(x_j\\) in the full multiple regression\n\n\\(e_i\\)s are the residuals from the full multiple regression\n\n\nPartial residual plot \\(e_i^{(j)}\\) vs.¬†\\(x_{ij}\\) for \\(x_j\\)\n\n\nThe nonlinear pattern can be highlighted or emphasized\nAdded-variable plots, for detecting influential data, are partial plots, but they don‚Äôt work well for detecting nonlinearity because they are biased towards linearity\nCook (1993): if the regressions of x_j on the other xs are approximately linear, then the regression function in the crPlot provides a visualization of transformation.\nIf the regressions among the predictors are strongly nonlinear and not well described by polynomials, then cvPlot may not be effective in recovering nonlinear partial relationships. => Use CERES plots (ceresPlots())\nAnd cvPlots can appear nonlinear even when the true partial regression is linear => called leakage."
  },
  {
    "objectID": "slides/10-diag-linearity.html#r-lab-partial-residual-plots",
    "href": "slides/10-diag-linearity.html#r-lab-partial-residual-plots",
    "title": "Regression Diagnostics - Linearity ‚ñ∂Ô∏è",
    "section": "\nR Lab Partial Residual Plots",
    "text": "R Lab Partial Residual Plots\n\nlogciafit <- lm(log(infant) ~ gdp + health + gini, data = CIA)\n# Component-plus-Residual Plot \ncar::crPlots(logciafit, ylab = \"partial residual\", layout = c(1, 3), grid = FALSE, main = \"\")"
  },
  {
    "objectID": "slides/10-diag-linearity.html#transformation-for-linearity",
    "href": "slides/10-diag-linearity.html#transformation-for-linearity",
    "title": "Regression Diagnostics - Linearity ‚ñ∂Ô∏è",
    "section": "Transformation for Linearity",
    "text": "Transformation for Linearity\n\n\nMonotone, simple: Power transformation on \\(x\\) and/or \\(y\\)\n\n\nMonotone, not simple: Polynomial regression (next week) or regression splines (MSSC 6250)\n\nNon-Monotone, simple: Quadratic regression \\(y = \\beta_0 + \\beta_1 x + \\beta_2 x^2 + \\epsilon\\)\n\n\n\n\nnot simple: direction of curvature changes\nTransform an \\(x\\) rather than \\(y\\), unless we see a common pattern of nonlinearity in the partial relationships of \\(y\\) to many \\(x\\)s.\nTransforming \\(y\\) changes the shape of its relationship to all of the \\(x\\)s, and also changes the shape of the residual distribution."
  },
  {
    "objectID": "slides/10-diag-linearity.html#bulging-rule-for-simple-monotone-nonlinearity",
    "href": "slides/10-diag-linearity.html#bulging-rule-for-simple-monotone-nonlinearity",
    "title": "Regression Diagnostics - Linearity ‚ñ∂Ô∏è",
    "section": "Bulging Rule for Simple Monotone Nonlinearity",
    "text": "Bulging Rule for Simple Monotone Nonlinearity\n\n\n\n\nThe bulge points\nTransform\nLadder of powers/roots\n\n\n\nleft\n\\(x\\)\n\ndown, e.g., \\(\\log(x)\\)\n\n\n\nright\n\\(x\\)\nup\n\n\ndown\n\\(y\\)\ndown\n\n\nup\n\\(y\\)\nup\n\n\n\n\n\nPrefer to transform an \\(x\\) rather than \\(y\\), unless we see a common pattern of nonlinearity in the partial relationships of \\(y\\) to many \\(x\\)s.\n\n\n\n\n\n\n\n\n\n\n\n\n\nMosteller and Tukey‚Äôs (1977) - Transform an \\(x\\) rather than \\(y\\), unless we see a common pattern of nonlinearity in the partial relationships of \\(y\\) to many \\(x\\)s. - Transforming \\(y\\) changes the shape of its relationship to all of the \\(x\\)s, and also changes the shape of the residual distribution."
  },
  {
    "objectID": "slides/10-diag-linearity.html#transformation-on-xs",
    "href": "slides/10-diag-linearity.html#transformation-on-xs",
    "title": "Regression Diagnostics - Linearity ‚ñ∂Ô∏è",
    "section": "Transformation on \\(x\\)s",
    "text": "Transformation on \\(x\\)s\n\n\ngdp to log(gdp)\n\n\nhealth to health + health^2"
  },
  {
    "objectID": "slides/10-diag-linearity.html#r-lab-partial-residual-plots-1",
    "href": "slides/10-diag-linearity.html#r-lab-partial-residual-plots-1",
    "title": "Regression Diagnostics - Linearity ‚ñ∂Ô∏è",
    "section": "\nR Lab Partial Residual Plots",
    "text": "R Lab Partial Residual Plots\n\n\nlogciafit2 <- update(logciafit, . ~ log(gdp) + poly(health, degree = 2, raw = TRUE) + gini)\ncar::crPlots(logciafit2, ylab = \"partial residual\", layout = c(1, 3), grid = FALSE, main = \"\") \n\n\n\n\n\n\n\n\nif true, use raw and not orthogonal polynomials. - If the partial relationship of log(infant) to health expenditure is quadratic, its partial residual plot should be linear."
  },
  {
    "objectID": "slides/10-diag-linearity.html#r-lab-improving-model-performance",
    "href": "slides/10-diag-linearity.html#r-lab-improving-model-performance",
    "title": "Regression Diagnostics - Linearity ‚ñ∂Ô∏è",
    "section": "\nR Lab Improving Model Performance",
    "text": "R Lab Improving Model Performance\n\ncar::brief(logciafit, digits = 2)\n\n           (Intercept)     gdp health   gini\nEstimate          3.02 -0.0439 -0.055 0.0216\nStd. Error        0.29  0.0037  0.022 0.0061\n\n Residual SD = 0.59 on 130 df, R-squared = 0.71 \n\ncar::brief(logciafit2, digits = 2)\n\n           (Intercept) log(gdp) poly(health, degree = 2, raw = TRUE)1\nEstimate          4.65   -0.720                                -0.221\nStd. Error        0.32    0.038                                 0.058\n           poly(health, degree = 2, raw = TRUE)2   gini\nEstimate                                  0.0096 0.0191\nStd. Error                                0.0034 0.0044\n\n Residual SD = 0.44 on 129 df, R-squared = 0.84"
  },
  {
    "objectID": "slides/10-diag-linearity.html#r-lab-plotting-against-original-untransformed-x",
    "href": "slides/10-diag-linearity.html#r-lab-plotting-against-original-untransformed-x",
    "title": "Regression Diagnostics - Linearity ‚ñ∂Ô∏è",
    "section": "\nR Lab Plotting against Original Untransformed \\(x\\)\n",
    "text": "R Lab Plotting against Original Untransformed \\(x\\)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nplot(Effect(‚Äúgdp‚Äù, logciafit2, residuals = TRUE))\n\\(\\epsilon^{(1)}_i = f_1(x_{i1}) + e_i\\) either if the partial-regression function \\(f_1(x_1)\\) is linear after all or if the other \\(x\\)s are each linearly related to \\(x_1\\).\nIn practice, it‚Äôs only strongly nonlinearly related \\(x\\)s that seriously threaten the validity of component-plus-residuals plots.\nCorrelation between x1 and x2 can induce spurious nonlinearity in the CR plot for x1.\ntrying to correct nonlinearity for one x at a time, but in my experience, it‚Äôs rarely necessary to proceed sequentially."
  },
  {
    "objectID": "slides/10-diag-linearity.html#transforming-xs-analytically-box-and-tidwell-1962",
    "href": "slides/10-diag-linearity.html#transforming-xs-analytically-box-and-tidwell-1962",
    "title": "Regression Diagnostics - Linearity ‚ñ∂Ô∏è",
    "section": "Transforming \\(x\\)s Analytically: Box and Tidwell (1962)",
    "text": "Transforming \\(x\\)s Analytically: Box and Tidwell (1962)\n\nBox and Tidwell (1962) proposed a procedure for estimating \\(\\lambda_1, \\lambda_2, \\dots, \\lambda_k\\) in the model \\[y = \\beta_0 + \\beta_1x_1^{\\lambda_1} + \\cdots + \\beta_kx_k^{\\lambda_k}+ \\epsilon\\]\nAll \\(x_j\\)s are positive.\n\\(\\beta_0, \\beta_1, \\dots, \\beta_k\\) are estimated after and conditional on the transformations.\n\\(x_j^{\\lambda_j} = \\log_e(x_j)\\) if \\(\\lambda_j = 0\\).\n\n\nIf some of the xs (for instance, dummy regressors) aren‚Äôt candidates for transformation, then these xs can simply enter the model linearly.\nall positive xs"
  },
  {
    "objectID": "slides/10-diag-linearity.html#r-lab-box-and-tidwell-1962",
    "href": "slides/10-diag-linearity.html#r-lab-box-and-tidwell-1962",
    "title": "Regression Diagnostics - Linearity ‚ñ∂Ô∏è",
    "section": "\nR Lab Box and Tidwell (1962)",
    "text": "R Lab Box and Tidwell (1962)\nConsider the model \\[\\log(Infant) = \\beta_0 + \\beta_1 GDP^{\\lambda_1} + \\beta_2Gini^{\\lambda_2} + \\beta_3 Health + \\beta_4 Health ^ 2 + \\epsilon\\]\n\ncar::boxTidwell(log(infant) ~ gdp + gini, \n                other.x = ~poly(health, 2, raw = TRUE), data = CIA)\n\n...\n     MLE of lambda Score Statistic (t) Pr(>|t|)    \ngdp            0.2                10.6   <2e-16 ***\ngini          -0.5                -0.4      0.7    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n...\n\n\n\nThe point estimate of \\(\\lambda\\) is \\(\\hat{\\lambda}_1 = 0.2\\) and \\(\\hat{\\lambda}_2 = -0.5\\)\n\nThe test is for \\(H_0:\\) No transformation is needed \\((\\lambda = 1)\\).\n\nStrong evidence to transform \\(GDP\\)\n\nLittle evidence of the need to transform the Gini coefficient"
  },
  {
    "objectID": "slides/10-diag-linearity.html#other-methods-for-dealing-with-nonlinearity",
    "href": "slides/10-diag-linearity.html#other-methods-for-dealing-with-nonlinearity",
    "title": "Regression Diagnostics - Linearity ‚ñ∂Ô∏è",
    "section": "Other Methods for Dealing with Nonlinearity",
    "text": "Other Methods for Dealing with Nonlinearity\n\nLack-of-fit test (LRA Sec 4.5, CMR Sec. 3.6): Need repeated observations\n\n\n\nTransform a nonlinear function into a linear one (LRA Sec 5.3)\n\n\nCan the nonlinear model \\(y = \\beta_0e^{\\beta_1x}\\epsilon\\) be transformed into a linear one (intrinsically linear)?\n\n\n\n\nPolynomial Regression, Regression Splines or other nonparametric regression (MSSC 6250)\n\n\n\n\nA (pure) nonlinear model may be needed if the model assumptions cannot be satisfied.\n\n\n\n\nmath4780-f23.github.io/website"
  },
  {
    "objectID": "slides/04-slr.html#simple-linear-regression-model-population",
    "href": "slides/04-slr.html#simple-linear-regression-model-population",
    "title": "Simple Linear Regression \n",
    "section": "Simple Linear Regression Model (Population)",
    "text": "Simple Linear Regression Model (Population)\n\n\nSimple: Only one predictor \\(X\\).\n\nLinear: the regression function is linear, i.e., \\(f(X) = \\beta_0 + \\beta_1 X\\).\n\n\nFor the \\(i\\)-th measurement in the target population, \\[Y_i = \\beta_0 + \\beta_1X_i + \\epsilon_i\\]\n\n\n\\(Y_i\\): the \\(i\\)-th value of the response (random) variable.\n\n\\(X_i\\): the \\(i\\)-th known fixed value of the predictor.\n\n\\(\\epsilon_i\\): the \\(i\\)-th random error with assumption \\(\\epsilon_i \\stackrel{iid}{\\sim} N(0, \\sigma^2)\\). \n\n\n\\(\\beta_0\\), \\(\\beta_1\\) and \\(\\sigma^2\\) are fixed unknown parameters to be estimated from the training sample after we collect them.\n\n\nHere is the formal Simple Linear Regression Model that describes my words mathematically.\n\n\\(\\epsilon_i\\): the \\(i\\)-th random error\n\nEach \\(\\epsilon_i\\) has an identical normal distribution with mean 0 and constant variance \\(\\sigma^2\\).\nAny \\(\\epsilon_i\\) and \\(\\epsilon_j\\), \\(i \\ne j\\), are independent or uncorrelated.\n\n\nAnd this is the population Simple Linear Regression Model. Because we assume \\(y\\) is drawn from a population distribution with some population parameters \\(\\beta_0\\), \\(\\beta_1\\) and \\(\\sigma^2\\).\nJust like we assume \\(Y_i \\sim N(\\mu, \\sigma^2)\\) in intro stats. The only difference is that now \\(Y_i\\) and its mean depend on the value of \\(X\\)."
  },
  {
    "objectID": "slides/04-slr.html#section-1",
    "href": "slides/04-slr.html#section-1",
    "title": "Simple Linear Regression \n",
    "section": "",
    "text": "When we collect data \\(\\{ (x_1, y_1), (x_2, y_2), \\dots, (x_n, y_n)\\},\\) \\(y\\) is assumed drawn from a normal distribution. Its value varies around its mean \\(\\mu_y\\).\n\nLet‚Äôs see what the assumptions of the model mean, and the ideas of behind them."
  },
  {
    "objectID": "slides/04-slr.html#section-2",
    "href": "slides/04-slr.html#section-2",
    "title": "Simple Linear Regression \n",
    "section": "",
    "text": "When we collect data \\(\\{ (x_1, y_1), (x_2, y_2), \\dots, (x_n, y_n)\\},\\) \\(y\\) is assumed drawn from a normal distribution. Its value varies around its mean \\(\\mu_y\\)."
  },
  {
    "objectID": "slides/04-slr.html#conditional-mean-of-y_i-given-a-value-of-x_i",
    "href": "slides/04-slr.html#conditional-mean-of-y_i-given-a-value-of-x_i",
    "title": "Simple Linear Regression \n",
    "section": "Conditional Mean of \\(Y_i\\) given a value of \\(X_i\\)\n",
    "text": "Conditional Mean of \\(Y_i\\) given a value of \\(X_i\\)\n\n\n\n\\(Y_i = \\beta_0 + \\beta_1X_i + \\epsilon_i\\) \\(\\quad \\epsilon_i \\stackrel{iid}{\\sim} N(0, \\sigma^2)\\)\n\nFor a random variable \\(Z\\) and a constant \\(c \\in \\mathbf{R}\\), \\(E(c+Z) = E(c) + E(Z) = c + E(Z)\\).\n\n\n\n\n\n\n\n\n\n\n\n\n\\[\\begin{align*}\n\\mu_{Y_i \\mid X_i} = E(Y_i \\mid X_i) &= E(\\beta_0 + \\beta_1X_i + \\epsilon_i) \\\\\n&= \\beta_0 + \\beta_1X_i + E(\\epsilon_i) \\\\\n&= \\beta_0 + \\beta_1X_i\n\\end{align*}\\]\nThe mean response of \\(Y\\), \\(\\mu_{Y\\mid X} = E(Y\\mid X)\\), has a straight-line relationship with \\(X\\) given by the population regression line \\[\\mu_{Y\\mid X} = \\beta_0 + \\beta_1X\\]"
  },
  {
    "objectID": "slides/04-slr.html#conditional-variance-of-y_i-given-a-value-of-x_i",
    "href": "slides/04-slr.html#conditional-variance-of-y_i-given-a-value-of-x_i",
    "title": "Simple Linear Regression \n",
    "section": "Conditional Variance of \\(Y_i\\) given a value of \\(X_i\\)\n",
    "text": "Conditional Variance of \\(Y_i\\) given a value of \\(X_i\\)\n\n\n\n\\(Y_i = \\beta_0 + \\beta_1X_i + \\epsilon_i\\) \\(\\quad \\epsilon_i \\stackrel{iid}{\\sim} N(0, \\sigma^2)\\)\n\nFor a random variable \\(Z\\) and a constant \\(c \\in \\mathbf{R}\\), \\(\\mathrm{Var}(c+Z) = \\mathrm{Var}(Z)\\).\n\n\n\n\n\n\n\n\n\n\n\n\n\\[\\begin{align*}\n\\mathrm{Var}(Y_i \\mid X_i) &= \\mathrm{Var}(\\beta_0 + \\beta_1X_i + \\epsilon_i) \\\\\n&= \\mathrm{Var}(\\epsilon_i) = \\sigma^2\n\\end{align*}\\] The variance of \\(Y\\) does not depend on \\(X\\).\n\nThe variation of Y is the same no matter what value of x is."
  },
  {
    "objectID": "slides/04-slr.html#conditional-distribution-of-y_i-mid-x-x_i",
    "href": "slides/04-slr.html#conditional-distribution-of-y_i-mid-x-x_i",
    "title": "Simple Linear Regression \n",
    "section": "Conditional Distribution of \\(Y_i \\mid X = x_i\\)\n",
    "text": "Conditional Distribution of \\(Y_i \\mid X = x_i\\)\n\n\n\n\\(Y_i = \\beta_0 + \\beta_1X_i + \\epsilon_i\\); \\(\\quad \\epsilon_i \\stackrel{iid}{\\sim} N(0, \\sigma^2)\\)\n\nFor a random variable \\(Z \\sim N(\\mu, \\sigma^2)\\) and a constant \\(c \\in \\mathbf{R}\\), \\(c+Z \\sim N(c + \\mu, \\sigma^2)\\).\n\n\n\n\n\n\n\n\n\n\n\n\n\\[\\begin{align*}\nY_i \\mid X_i \\stackrel{indep}{\\sim} N(\\beta_0 + \\beta_1X_i, \\sigma^2)\n\\end{align*}\\] For any fixed value of \\(X_i = x\\), the response \\(Y_i\\) varies according to \\(N(\\mu_{Y_i\\mid X_i = x}, \\sigma^2)\\).\n\n\nJob: Collect data and estimate the unknown \\(\\beta_0\\), \\(\\beta_1\\) and \\(\\sigma^2\\)!\nWith the model, our first job is to collect data and estimate the unknown parameters \\(\\beta_0\\), \\(\\beta_1\\) and \\(\\sigma^2\\)!"
  },
  {
    "objectID": "slides/04-slr.html#idea-of-fitting",
    "href": "slides/04-slr.html#idea-of-fitting",
    "title": "Simple Linear Regression \n",
    "section": "Idea of Fitting",
    "text": "Idea of Fitting\n\nInterested in \\(\\beta_0\\) and \\(\\beta_1\\) in the sample regression model:\n\n\\[\\begin{align*}\ny_i &= f(x_i) +  \\epsilon_i \\\\\n    &= \\beta_0 + \\beta_1~x_{i} + \\epsilon_i,\n\\end{align*}\\] or\n\\[E(y_i \\mid x_i) = \\mu_{y|x_i} = \\beta_0 + \\beta_1~x_{i}\\]\n\n\nUse sample statistics \\(b_0\\) and \\(b_1\\) computed from our sample data to estimate \\(\\beta_0\\) and \\(\\beta_1\\).\n\\(\\hat{y}_{i} = b_0 + b_1~x_{i}\\) is called fitted value of \\(y_i\\), a point estimate of the mean \\(\\mu_{y|x_i}\\) and \\(y_i\\) itself.\n\n\nOK. once we collect the data, we have a sample regression model.\nHere I use small x and y to represent the collected data.\nGiven this model, we‚Äôre interested in \\(\\beta_0\\) (population parameter for the intercept) and \\(\\beta_1\\) (population parameter for the slope) because once we know \\(\\beta_0\\) and \\(\\beta_1\\), we know the exact shape of \\(f\\) and we know the relationship of \\(y\\) and \\(x\\), and given any value of \\(x\\), we can predict its corresponding value of \\(y\\) using the regression line \\(\\hat{y}_{i} = \\beta_0 + \\beta_1~x_{i}\\).\nBut again the population parameters are unknown to us.\n\\(\\hat{y}_i = E(Y|X_i=x_i)\\)\n\n\\(b_0\\): intercept of the sample regression line\n\n\\(b_1\\): slope of the sample regression line"
  },
  {
    "objectID": "slides/04-slr.html#fitting-a-regression-line-haty-b_0-b_1x",
    "href": "slides/04-slr.html#fitting-a-regression-line-haty-b_0-b_1x",
    "title": "Simple Linear Regression \n",
    "section": "Fitting a Regression Line \\(\\hat{Y} = b_0 + b_1X\\)\n",
    "text": "Fitting a Regression Line \\(\\hat{Y} = b_0 + b_1X\\)\n\nGiven the training sample \\(\\{ (x_1, y_1), (x_2, y_2), \\dots, (x_n, y_n)\\},\\)\n\nWhich sample regression line is the best?\nWhat are the best estimators \\(b_0\\) and \\(b_1\\) for \\(\\beta_0\\) and \\(\\beta_1\\)?\n\n\n\nNow suppose we already get our sample, and now we are trying to use the sample to get a sample regression line, and hopefully, the sample regression line and the population regression line are alike. look similarly.\nSo people usually ask\nWhich sample regression line is the best?\nWhat are the best estimators \\(b_0\\) and \\(b_1\\) for \\(\\beta_0\\) and \\(\\beta_1\\)?\nAfter all, given the data, we can generate so many different straight lines, and we need a criterion to help us determine which line is the best in some sense. Right!"
  },
  {
    "objectID": "slides/04-slr.html#what-does-best-mean-ordinary-least-squares-ols",
    "href": "slides/04-slr.html#what-does-best-mean-ordinary-least-squares-ols",
    "title": "Simple Linear Regression \n",
    "section": "What does ‚Äúbest‚Äù mean? Ordinary Least Squares (OLS)",
    "text": "What does ‚Äúbest‚Äù mean? Ordinary Least Squares (OLS)\n\n\n\n\nChoose the best \\(b_0\\) and \\(b_1\\) or the sample regression line minimizing the sum of squared residuals \\[SS_{res} = e_1^2 + e_2^2 + \\dots + e_n^2 = \\sum_{i = 1}^n e_i^2.\\]\n\nThe residual \\(e_i = y_i - \\hat{y}_i = y_i - (b_0 + b_1x_i)\\) is a point estimate of \\(\\epsilon_i\\).\n\n\n\nIf \\(b_0\\) and \\(b_1\\) are the best estimators, plug \\(e_i = y_i - (b_0 + b_1x_i)\\) into \\(SS_{res}\\), we have\n\n\\[\\begin{align} SS_{res} &= (y_1 - b_0 - b_1x_1)^2 + (y_2 - b_0 - b_1x_2)^2 + \\dots + (y_n - b_0 - b_1x_n)^2\\\\ &= \\sum_{i=1}^n(y_i - b_0 - b_1x_i)^2 \\end{align}\\] that is the smallest comparing to any other \\(SS_{res} = \\sum_{i=1}^n(y_i - a_0 - a_1x_i)^2\\) that uses another pair of estimators \\((a_0, a_1) \\ne (b_0, b_1)\\).\n\nNow the question is How do we get \\(b_0\\) and \\(b_1\\) that well estimate \\(\\beta_0\\) and \\(\\beta_1\\)?\nWe choose \\(b_0\\) and \\(b_1\\), or regression line \\(b_0 + b_1x\\) that minimizes the sum of squared residuals.\nIf we define residual as \\(e_i = y_i - \\hat{y}_i\\), then the sum of squared residuals is \\(\\sum_{i = 1}^n e_i^2\\).\nAnd this approach that estimates the population parameters \\(\\beta_0\\) and \\(\\beta_1\\) or the population regression line is called Ordinary Least Squares method."
  },
  {
    "objectID": "slides/04-slr.html#visualizing-residuals",
    "href": "slides/04-slr.html#visualizing-residuals",
    "title": "Simple Linear Regression \n",
    "section": "Visualizing Residuals",
    "text": "Visualizing Residuals\n\nThat‚Äôs see the idea of Ordinary Least Squares visually. Here just showed the data. - Later we will work on the data set together."
  },
  {
    "objectID": "slides/04-slr.html#visualizing-residuals-cont.",
    "href": "slides/04-slr.html#visualizing-residuals-cont.",
    "title": "Simple Linear Regression \n",
    "section": "Visualizing Residuals (cont.)",
    "text": "Visualizing Residuals (cont.)\n\n\nAll right, with the data, this figure also shows the least squares regression line, and the fitted value of \\(y\\) for each \\(x\\) in the training data, which are those red points.\nThe fitted values of y are right on the regression line.\nNow the question is, how do we find this line?\nGiven a line, we can have predicted values of y, right?\nThen what is residual on the plot? The residual will be the difference between the true observation y and the fitted value of y given any value of x.\nSo a residual in the plot will be a vertical bar at the value of x with two ends of the bar \\(y\\) and \\(\\hat{y}\\), right?\n(Show on board)\n(add \\(y_i = b_0+b_1x_i\\) and residual line)"
  },
  {
    "objectID": "slides/04-slr.html#visualizing-residuals-cont.-1",
    "href": "slides/04-slr.html#visualizing-residuals-cont.-1",
    "title": "Simple Linear Regression \n",
    "section": "Visualizing Residuals (cont.)",
    "text": "Visualizing Residuals (cont.)\n\n\nHere shows all the residuals in vertical bars.\nleast squares line is the line such that the sum of all the squared residuals is minimized.\nWhy we square the residuals?\nIt‚Äôs mathematically more convenient.\nSquaring emphasizes larger differences"
  },
  {
    "objectID": "slides/04-slr.html#least-squares-estimates-lse",
    "href": "slides/04-slr.html#least-squares-estimates-lse",
    "title": "Simple Linear Regression \n",
    "section": "Least Squares Estimates (LSE)",
    "text": "Least Squares Estimates (LSE)\n\nThe least squares approach choose \\(b_0\\) and \\(b_1\\) that minimize the \\(SS_{res}\\), i.e., \\[(b_0, b_1) = \\arg\\min_{\\alpha_0, \\alpha_1} \\sum_{i=1}^n(y_i - \\alpha_0 - \\alpha_1x_i)^2\\]\n\n\n\nTake derivative w.r.t. \\(\\alpha_0\\) and \\(\\alpha_1\\), setting both equal to zero: \\[\\left.\\frac{\\partial SS_{res}}{\\partial\\alpha_0}\\right\\vert_{b_0, b_1} = \\left.\\sum_{i=1}^n\\frac{\\partial (y_i - \\alpha_0 - \\alpha_1x_i)^2}{\\partial\\alpha_0}\\right\\vert_{b_0, b_1} = -2 \\sum_{i=1}^n(y_i - b_0 - b_1x_i) = 0\\] \\[\\left. \\frac{\\partial SS_{res}}{\\partial\\alpha_1}\\right\\vert_{b_0, b_1} = \\left.\\sum_{i=1}^n\\frac{\\partial (y_i - \\alpha_0 - \\alpha_1x_i)^2}{\\partial\\alpha_1}\\right\\vert_{b_0, b_1} = -2 \\sum_{i=1}^nx_i(y_i - b_0 - b_1x_i) = 0\\] The two equations are called the normal equations."
  },
  {
    "objectID": "slides/04-slr.html#least-squares-estimates-solve-for-alpha_0-and-alpha_1",
    "href": "slides/04-slr.html#least-squares-estimates-solve-for-alpha_0-and-alpha_1",
    "title": "Simple Linear Regression \n",
    "section": "Least Squares Estimates: Solve for \\(\\alpha_0\\) and \\(\\alpha_1\\)\n",
    "text": "Least Squares Estimates: Solve for \\(\\alpha_0\\) and \\(\\alpha_1\\)\n\n\nSolve for \\(\\alpha_0\\) given \\(b_1\\):\n\n\\[ \\color{red}{b_0 = \\overline{y} - b_1\\overline{x}}\\]\n\n\nSolve for \\(\\alpha_1\\) given \\(\\color{red}{b_0 = \\overline{y} - b_1\\overline{x}}\\):\n\n\\[\\color{red}{b_1 = \\frac{\\sum_{i=1}^n(x_i - \\overline{x})(y_i - \\overline{y})}{\\sum_{i=1}^n(x_i - \\overline{x})^2} = \\frac{S_{xy}}{S_{xx}} = r \\frac{\\sqrt{S_{yy}}}{\\sqrt{S_{xx}}}},\\] where \\(S_{xx} = \\sum_{i=1}^n(x_i - \\overline{x})^2\\), \\(S_{yy} = \\sum_{i=1}^n(y_i - \\overline{y})^2\\), \\(S_{xy} = \\sum_{i=1}^n(x_i - \\overline{x})(y_i - \\overline{y})\\), and \\(r\\) is the sample correlation coefficient between \\(x\\) and \\(y\\).\n\n\n\nWhat can we learn from the formula of \\(b_0\\) and \\(b_1\\)?\n\n\nThe LS regression line passes through the centroid.\n\n\\(b_1\\) is kinda like a scaled covariance of X and Y.\n\n\\(b_0\\) and \\(b_1\\) are correlated."
  },
  {
    "objectID": "slides/04-slr.html#r-lab-highway-mpg-hwy-vs.-displacement-displ",
    "href": "slides/04-slr.html#r-lab-highway-mpg-hwy-vs.-displacement-displ",
    "title": "Simple Linear Regression \n",
    "section": "\nR Lab Highway MPG hwy vs.¬†Displacement displ\n",
    "text": "R Lab Highway MPG hwy vs.¬†Displacement displ\n\n\nData set: mpg in ggplot2 package.\n\n\n## install.packages(\"ggplot2\")\nlibrary(ggplot2)\nmpg\n\n# A tibble: 234 √ó 11\n  manufacturer model displ  year   cyl trans      drv     cty   hwy fl    class \n  <chr>        <chr> <dbl> <int> <int> <chr>      <chr> <int> <int> <chr> <chr> \n1 audi         a4      1.8  1999     4 auto(l5)   f        18    29 p     compa‚Ä¶\n2 audi         a4      1.8  1999     4 manual(m5) f        21    29 p     compa‚Ä¶\n3 audi         a4      2    2008     4 manual(m6) f        20    31 p     compa‚Ä¶\n4 audi         a4      2    2008     4 auto(av)   f        21    30 p     compa‚Ä¶\n5 audi         a4      2.8  1999     6 auto(l5)   f        16    26 p     compa‚Ä¶\n6 audi         a4      2.8  1999     6 manual(m5) f        18    26 p     compa‚Ä¶\n# ‚Ñπ 228 more rows\n\n\n\nOK. I think we all learn what a linear regression is, and it‚Äôs time to see how to do a real data analysis using R.\nThe data set we use is stored in the ggplot2 package.\nIn order to get access to the data, you have to install and load the package into your R session.\nYou can use the command install.packages(‚Äúggplot2‚Äù) and library(ggplot2) to do so.\nOnce you are done. Simply type the data set‚Äôs name mpg on your R console, it will print the data set.\nIt is of type tibble, basically a new version of data frame.\nThere are 234 different cars with 11 variables.\nIn this example, we are going to use the two variables, hwy which is highway miles per gallon and displ, the size of engine displacement."
  },
  {
    "objectID": "slides/04-slr.html#r-lab-scatter-plot",
    "href": "slides/04-slr.html#r-lab-scatter-plot",
    "title": "Simple Linear Regression \n",
    "section": "\nR Lab Scatter Plot",
    "text": "R Lab Scatter Plot\n\nplot(x = mpg$displ, y = mpg$hwy, las = 1, pch = 19, col = \"navy\", cex = 0.5,\n     xlab = \"Displacement (litres)\", ylab = \"Highway MPG\",\n     main = \"Highway MPG vs. Engine Displacement (litres)\")\n\n\n\nUsually, when we get data, the first thing is plotting your data, doing some exploratory data analysis, and see if there is useful information out there that may help us build an appropriate model.\nTo make a scatter plot, we can simply use the plot() function, and put displ in x axis and hwy in the y axis.\nTo grab a variable or a column of a data frame, we can use the dollar sign, the same way as a list extracting an element.\nThe rest of arguments are optional, they are just used to decorate your plot. You can generate a plot without specifying any of them.\nAnd because it seems to a linear trend downwards. We could fit a simple linear regression to the data. Right"
  },
  {
    "objectID": "slides/04-slr.html#r-lab-fit-simple-linear-regression",
    "href": "slides/04-slr.html#r-lab-fit-simple-linear-regression",
    "title": "Simple Linear Regression \n",
    "section": "\nR Lab Fit Simple Linear Regression",
    "text": "R Lab Fit Simple Linear Regression\n\n(reg_fit <- lm(formula = hwy ~ displ, data = mpg))\n\n\nCall:\nlm(formula = hwy ~ displ, data = mpg)\n\nCoefficients:\n(Intercept)        displ  \n      35.70        -3.53  \n\ntypeof(reg_fit)\n\n[1] \"list\"\n\n## use $ to extract an element of a list\nreg_fit$coefficients\n\n(Intercept)       displ \n      35.70       -3.53 \n\n\n\n\n\\(\\widehat{hwy}_{i} = b_0 + b_1 \\times displ_{i} = 35.698 -3.531 \\times displ_{i}\\)\n\\(b_1\\): For one unit (litre) increase of the displacement, we expect the highway MPG to be decreased, on average, by 3.5 miles.\n\n\nIn R, to fit a linear regression model, it cannot be easier.\nWe just need to use the command lm(). We put a formula in the function, y ~ x, and let R know which data set you are considering.\nThat‚Äôs it. And I save the fitted result in an object called reg_fit.\nYou can see lm() function returns a list.\nWe can grab the coefficient estimates this way."
  },
  {
    "objectID": "slides/04-slr.html#r-lab-fitted-values",
    "href": "slides/04-slr.html#r-lab-fitted-values",
    "title": "Simple Linear Regression \n",
    "section": "\nR Lab Fitted Values",
    "text": "R Lab Fitted Values\n\n## all elements in reg_fit\nnames(reg_fit)\n\n [1] \"coefficients\"  \"residuals\"     \"effects\"       \"rank\"         \n [5] \"fitted.values\" \"assign\"        \"qr\"            \"df.residual\"  \n [9] \"xlevels\"       \"call\"          \"terms\"         \"model\"        \n\nmpg$hwy[1:5]\n\n[1] 29 29 31 30 26\n\n## the first 20 fitted value y_hat\nhead(reg_fit$fitted.values, 5)\n\n   1    2    3    4    5 \n29.3 29.3 28.6 28.6 25.8 \n\nmpg$displ[1:5]\n\n[1] 1.8 1.8 2.0 2.0 2.8\n\nlength(reg_fit$fitted.values)\n\n[1] 234\n\n\n\nIf you wanna see what information is contained in the fitted result, you can check all of its elements by names because each element here has a name.\nTo get the fitted value of \\(y\\), simply type reg_fit$fitted.values\nIt will show all 234 fitted values of y for each given value of x.\nHere I just show the first 20 fitted values, and their corresponding value of x."
  },
  {
    "objectID": "slides/04-slr.html#r-lab-add-a-regression-line",
    "href": "slides/04-slr.html#r-lab-add-a-regression-line",
    "title": "Simple Linear Regression \n",
    "section": "\nR Lab Add a Regression Line",
    "text": "R Lab Add a Regression Line\n\nplot(x = mpg$displ, y = mpg$hwy, las = 1, pch = 19, col = \"navy\", cex = 0.5,\n     xlab = \"Displacement (litres)\", ylab = \"Highway MPG\",\n     main = \"Highway MPG vs. Engine Displacement (litres)\")\nabline(reg_fit, col = \"#FFCC00\", lwd = 3)  #<<\n\n\n\nHow do we add a regression line on a scatter plot, very simple in R.\nwe just use the abline() function right after the scatter plot we created.\nWe can put the entire fitted result in the abline() function. The function will look for the intercept and slope itself and add the line onto the scatter plot."
  },
  {
    "objectID": "slides/04-slr.html#properties-of-least-squares-fit",
    "href": "slides/04-slr.html#properties-of-least-squares-fit",
    "title": "Simple Linear Regression \n",
    "section": "Properties of Least Squares Fit",
    "text": "Properties of Least Squares Fit\n Both \\(b_0\\) and \\(b_1\\) are linear combinations of \\(y_1, \\dots, y_n\\). \n\n\n\\(S_{xy} = \\sum_{i=1}^n(x_i - \\overline{x})(y_i - \\overline{y}) = \\sum_{i=1}^n(x_i - \\overline{x})y_i\\) \\[b_1 = \\frac{S_{xy}}{S_{xx}} = \\frac{\\sum_{i=1}^n(x_i - \\overline{x})y_i}{\\sum_{i=1}^n(x_i - \\overline{x})^2} = \\sum_{i=1}^nc_iy_i,\\] where \\(c_i = \\frac{(x_i - \\overline{x})}{S_{xx}}\\).\n\n\\(b_0 = \\overline{y} - b_1\\overline{x}\\) \\(b_1 = \\frac{\\sum_{i=1}^n(x_i - \\overline{x})(y_i - \\overline{y})}{\\sum_{i=1}^n(x_i - \\overline{x})^2} := \\frac{S_{xy}}{S_{xx}}\\) - if \\(x_i = \\overline{x}\\), its \\(y_i\\) has no contribution on \\(b_1\\). - the sample regression line always pass through the center of x and y. - It is points away from the center determine the slope\n\n\n\n\\(\\sum_{i=1}^n c_i = ?\\)\n\\(\\sum_{i=1}^n c_ix_i = ?\\)"
  },
  {
    "objectID": "slides/04-slr.html#properties-of-least-squares-fit-1",
    "href": "slides/04-slr.html#properties-of-least-squares-fit-1",
    "title": "Simple Linear Regression \n",
    "section": "Properties of Least Squares Fit",
    "text": "Properties of Least Squares Fit\n Therefore, \\(\\hat{y}_i\\) is a linear combination of \\(y_1, \\dots, y_n\\)!\n\n\n\\(b_0 = \\overline{y} - b_1~\\overline{x}\\) and \\(b_1 = \\sum_{j=1}^n c_j y_j\\)\n\n\n\\[\\begin{align} \\hat{y}_i &= b_0 + b_1 ~x_i \\\\&= \\overline{y} - b_1~\\overline{x} + x_i\\sum_{j=1}^n c_j y_j \\\\&= \\cdots \\\\&= \\sum_{j=1}^nh_{ij}y_j \\\\&= h_{i1}y_1 + h_{i2}y_2 + \\cdots + h_{in}y_n\\end{align}\\] where \\(h_{ij} = \\frac{1}{n} + \\frac{(x_i-\\overline{x})(x_j-\\overline{x})}{S_{xx}}.\\)\n\nIf \\(h_{ij}\\) is large, the \\(j\\)th case can have substantial impact on the \\(i\\)th fitted value.\n\n\\(h_{ij}\\) captures the extent to which \\(y_j\\) can affect \\(\\hat{y}_i\\): - if x_i = x_bar, y_hat = y_bar - hat value plays a important role in regression - For every fitted value, we use info from the all data points, and their influence is determined by their location comparing to the \\(\\bar{x}\\)."
  },
  {
    "objectID": "slides/04-slr.html#properties-of-least-squares-fit-2",
    "href": "slides/04-slr.html#properties-of-least-squares-fit-2",
    "title": "Simple Linear Regression \n",
    "section": "Properties of Least Squares Fit",
    "text": "Properties of Least Squares Fit\n\nResponses \\(\\{y_i\\}_{i=1}^n\\) are random variables before we actually collect them. So are \\(b_0\\) and \\(b_1\\).\n\n\n \\(b_0\\) and \\(b_1\\) are unbiased estimators for \\(\\beta_0\\) and \\(\\beta_1\\), respectively, i.e., \\(E(b_0) = \\beta_0\\), \\(E(b_1) = \\beta_1\\). \n\n\n\nFor random variables \\(Z_1, \\dots, Z_n\\) and constants \\(c_1, \\dots, c_n \\in \\mathbf{R}\\), \\(E(c_1Z_1 + \\dots +c_nZ_n) = c_1E(Z_1) + \\dots + c_nE(Z_n)\\)\n\n\n\n\\[\\begin{align*}\nE(b_1) &= E\\left( \\sum_{i=1}^n c_iy_i\\right) = \\sum_{i=1}^n c_i E(y_i) = \\sum_{i=1}^n c_i (\\beta_0+\\beta_1x_i) \\\\\n&= \\beta_0 \\sum_{i=1}^n c_i+ \\beta_1\\sum_{i=1}^n c_ix_i = \\beta_1\n\\end{align*}\\]\n\nYou can do \\(E(b_0)\\)\n\nWe don‚Äôt know the true value of \\(\\beta_0\\) and \\(\\beta_1\\), but if we were able to collect our data set many times, and get lots of \\(b_0\\) and \\(b_1\\) estimates, then the average of those \\(b_0\\) and \\(b_1\\) will be very close to the unknown \\(\\beta_0\\) and \\(\\beta_1\\), which is quite nice. Right."
  },
  {
    "objectID": "slides/04-slr.html#properties-of-least-squares-fit-3",
    "href": "slides/04-slr.html#properties-of-least-squares-fit-3",
    "title": "Simple Linear Regression \n",
    "section": "Properties of Least Squares Fit",
    "text": "Properties of Least Squares Fit\n\n\nGauss-Markov Theorem: \\(b_0\\) and \\(b_1\\) are best linear unbiased estimators (BLUEs).\n\n Linear : linear combinations of \\(y_i\\)\n\n\n Unbiased : \\(E(b_0) = \\beta_0\\), \\(E(b_1) = \\beta_1\\)\n\n\n Best : minimum variance ‚ÄºÔ∏è\n\n\n\nFor independent random variables \\(Z_1, \\dots, Z_n\\) and constants \\(c_1, \\dots, c_n \\in \\mathbf{R}\\), \\(\\mathrm{Var}(c_1Z_1 + \\dots +c_nZ_n) = c_1^2\\mathrm{Var}(Z_1) + \\dots + c_n^2\\mathrm{Var}(Z_n)\\)\n\n\n\n\\[\\begin{align*}\n\\mathrm{Var}(b_1) &= \\mathrm{Var}\\left( \\sum_{i=1}^n c_iy_i\\right) = \\sum_{i=1}^n c_i^2 \\mathrm{Var}(y_i) = \\sigma^2\\sum_{i=1}^n c_i^2 = \\frac{\\sigma^2\\sum_{i=1}^n (x_i - \\overline{x})^2}{S_{xx}^2} = \\frac{\\sigma^2}{S_{xx}}\n\\end{align*}\\]\n\n\n\nüòé LSEs are unbiased and have minimum variance when compared with all other unbiased estimators that are linear combo of \\(y_i\\). üëç üëç\n\n\nFinally, we have this famous Gauss-Markov Theorem saying \\(b_0\\) and \\(b_1\\) are best linear unbiased estimators (BLUEs)\n\nMinimum variance is a excellent property.\nEvery time we collect a new data set, we will get the new \\(b_0\\) and \\(b_1\\), right?\nWe know \\(b_0\\) and \\(b_1\\) vary with data. But the theorem tells us that their variation is minimal comparing to other estimators.\nSo this minimizes our uncertainty about \\(\\beta_0\\) and \\(\\beta_1\\)"
  },
  {
    "objectID": "slides/04-slr.html#properties-of-least-squares-fit-4",
    "href": "slides/04-slr.html#properties-of-least-squares-fit-4",
    "title": "Simple Linear Regression \n",
    "section": "Properties of Least Squares Fit",
    "text": "Properties of Least Squares Fit\nüëâ Sum of residuals is zero.\n\\[\\scriptstyle \\sum_{i=1}^n(y_i - \\hat{y}_i) = \\sum_{i=1}^ne_i = 0\\]\n\nüëâ The sum of observations \\(y_i\\) equals the sum of the fitted values \\(\\hat{y}_i\\).\n\\[\\scriptstyle  \\sum_{i=1}^ny_i = \\sum_{i=1}^n\\hat{y}_i\\]\n\n\nüëâ The LS regression line passes through the centroid of data \\((\\overline{x}, \\overline{y})\\).\n\n\nüëâ Inner product of residual and predictor is zero. \\[\\scriptstyle \\sum_{i=1}^nx_ie_i = 0\\]\n\n\nüëâ Inner product of residual and fitted value is zero. \\[\\scriptstyle  \\sum_{i=1}^n\\hat{y}_ie_i = 0\\]\n\nSum of residuals weighted by the corresponding predictor value is zero.\nThis actually implies that the correlation between x and e is 0.\nthe residuals are everything that is not related to \\(x\\), after fitting the regression model.\nresiduals contain information that is NOT related to \\(x\\) or cannot be explained by \\(x\\).\nSum of residuals weighted by the corresponding fitted value is zero.\nthe correlation or covariance between and e is 0 as well."
  },
  {
    "objectID": "slides/04-slr.html#estimation-for-sigma2",
    "href": "slides/04-slr.html#estimation-for-sigma2",
    "title": "Simple Linear Regression \n",
    "section": "Estimation for \\(\\sigma^2\\)\n",
    "text": "Estimation for \\(\\sigma^2\\)\n\n\nThink of \\(\\sigma^2\\) as variance around the line or the mean squared error.\nThe estimate of \\(\\sigma^2\\), denoted as \\(s^2\\) computed from the sample data is \\[s^2 = \\frac{SS_{res}}{n-2} = \\frac{\\sum_{i=1}^n(y_i - \\hat{y}_i)^2}{n-2} = MS_{res}\\]\n\nThe estimated variance \\(MS_{res}\\), called mean squared residual, is often shown in computer output as \\(\\texttt{MS(Error)}\\) or \\(\\texttt{MS(Residual)}\\).\n\n\\(E(MS_{res}) = \\sigma^2\\), i.e., \\(s^2\\) is an unbiased estimator for \\(\\sigma^2\\). üëç\n\n\nSo we are done estimation for \\(beta\\). Let‚Äôs talk about the estimation of \\(\\sigma^2\\).\nThe estimate of \\(\\sigma^2\\), denoted as \\(s^2\\) or \\(s_{\\epsilon}^2\\), based on the sample data is residual sum of squares divided by \\(n-2\\), the degrees of freedom\nIt can be shown that \\(E(SS_{res}) = (n-2)\\sigma^2\\). That is, \\(s^2\\) is an unbiased estimator for \\(\\sigma^2\\). üëç"
  },
  {
    "objectID": "slides/04-slr.html#model-dependent-estimate-of-sigma2",
    "href": "slides/04-slr.html#model-dependent-estimate-of-sigma2",
    "title": "Simple Linear Regression \n",
    "section": "Model-dependent estimate of \\(\\sigma^2\\)\n",
    "text": "Model-dependent estimate of \\(\\sigma^2\\)\n\n\n\n\\(s\\): the residual standard error or standard error of regression, is a measure of the lack of fit of the regression model to the data.\nIf \\(\\hat{y}_i \\approx y_i\\), then \\(s\\) will be small, and the model fits the data well.\nIf \\(\\hat{y}_i\\) is far away from \\(y_i\\), \\(s\\) may be large, indicating the model does not fit the data well.\n\n\n\nIf the fitted value using the model is close to the true value, \\(\\hat{y}_i \\approx y_i\\), then \\(s\\) will be small, and the model fits the data well.\nIf \\(\\hat{y}_i\\) is far away from \\(y_i\\), \\(s\\) may be large, indicating that the model does not fit the data well.\nLet‚Äôs see the example shown below. If X and Y have a linear relationship and the relationship is pretty tight, we should see the fitted values on the fitted line are close to the observations, and the \\(s\\) will be small.\nBut if X and Y have a quadratic relationship, but we fit a linear regression, the fitted values will be distant from the observations, and \\(s\\) may be large.\nThe case on the right shows the violation of constant variance. In this case, the fitted values are away from the observations too, resulting in large \\(s\\), and indicating the model does not fit the data well. OK."
  },
  {
    "objectID": "slides/04-slr.html#r-lab-standard-error-of-regression",
    "href": "slides/04-slr.html#r-lab-standard-error-of-regression",
    "title": "Simple Linear Regression \n",
    "section": "\nR Lab Standard Error of Regression",
    "text": "R Lab Standard Error of Regression\n\n(summ_reg_fit <- summary(reg_fit))\n\n\nCall:\nlm(formula = hwy ~ displ, data = mpg)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-7.104 -2.165 -0.224  2.059 15.010 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)   35.698      0.720    49.5   <2e-16 ***\ndispl         -3.531      0.195   -18.1   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.84 on 232 degrees of freedom\nMultiple R-squared:  0.587, Adjusted R-squared:  0.585 \nF-statistic:  329 on 1 and 232 DF,  p-value: <2e-16\n\n\n\nHow do we get sigma_hat in R?\nWell you could grab residuals and df from lm fit result reg_fit, and use the formula to calculate the sigma_hat. sqrt(sum(reg_fit\\(residuals^2) / reg_fit\\)df.residual)\nIf you want R to do the calculation for you, you can get the summary of the fitted result reg_fit.\nThen the sigma hat is right here."
  },
  {
    "objectID": "slides/04-slr.html#r-lab-standard-error-of-regression-1",
    "href": "slides/04-slr.html#r-lab-standard-error-of-regression-1",
    "title": "Simple Linear Regression \n",
    "section": "\nR Lab Standard Error of Regression",
    "text": "R Lab Standard Error of Regression\n\n# lots of fitted information saved in summary(reg_fit)!\nnames(summ_reg_fit)\n\n [1] \"call\"          \"terms\"         \"residuals\"     \"coefficients\" \n [5] \"aliased\"       \"sigma\"         \"df\"            \"r.squared\"    \n [9] \"adj.r.squared\" \"fstatistic\"    \"cov.unscaled\" \n\n# residual standard error (sigma_hat)\nsumm_reg_fit$sigma\n\n[1] 3.84\n\n# from reg_fit\nsqrt(sum(reg_fit$residuals ^ 2) / reg_fit$df.residual)\n\n[1] 3.84\n\n\n\nHere shows the fitted information saved in summary(reg_fit)\nYou see sigma is right here. So you just extract that value if you need."
  },
  {
    "objectID": "slides/04-slr.html#sampling-distribution-of-b_0-and-b_1",
    "href": "slides/04-slr.html#sampling-distribution-of-b_0-and-b_1",
    "title": "Simple Linear Regression \n",
    "section": "Sampling Distribution of \\(b_0\\) and \\(b_1\\)\n",
    "text": "Sampling Distribution of \\(b_0\\) and \\(b_1\\)\n\n\nIf \\(y_i\\) ( given \\(x_i\\) ) is normally distributed, do \\(b_0\\) and \\(b_1\\) follow normal distribution too?\n\n\nIf \\(Z_1, \\dots, Z_n\\) are normal variables and constants \\(c_1, \\dots, c_n \\in \\mathbf{R}\\), then \\(c_1Z_1 + \\dots + c_nZ_n\\) is also a normal variable.\n\n\n\n\n\\(b_1 \\sim N\\left(\\beta_1, \\frac{\\sigma^2}{S_{xx}} \\right)\\); \\(\\quad b_0 \\sim N\\left(\\beta_0, \\sigma^2\\left(\\frac{1}{n} + \\frac{\\overline{x}^2}{S_{xx}} \\right) \\right)\\)\n\n\n\nWe skip the proof here due to time limit.\nMaybe I will ask you to prove it in your homework or exam.\nThis looks pretty similar to what we reviewed last week, right?\n\n\n\n\n\n\\(\\small \\frac{b_1 - \\beta_1}{\\sqrt{\\sigma^2/S_{xx}}} \\sim N\\left(0, 1 \\right)\\); \\(\\small \\quad \\frac{b_0 - \\beta_0}{\\sqrt{\\sigma^2 \\left(\\frac{1}{n} + \\frac{\\overline{x}^2}{S_{xx}}\\right)}} \\sim N\\left(0, 1 \\right)\\)\n\n\n\n\n\n\n\\(\\small \\frac{b_1 - \\beta_1}{\\sqrt{s^2/S_{xx}}} \\sim t_{n-2}\\); \\(\\small \\quad \\frac{b_0 - \\beta_0}{\\sqrt{s^2\\left(\\frac{1}{n} + \\frac{\\overline{x}^2}{S_{xx}}\\right)}} \\sim t_{n-2}\\)\n\n\n\n\n\n\n\\((1-\\alpha)100\\%\\) CI for \\(\\beta_1\\) is \\(\\small b_1 \\pm t_{\\alpha/2, n-2}\\sqrt{s^2/S_{xx}}\\)\n\n\n\\((1-\\alpha)100\\%\\) CI for \\(\\beta_0\\) is \\(\\small b_0 \\pm t_{\\alpha/2, n-2}\\sqrt{s^2\\left(\\frac{1}{n} + \\frac{\\overline{x}^2}{S_{xx}}\\right)}\\)"
  },
  {
    "objectID": "slides/04-slr.html#sampling-distribution-of-s2",
    "href": "slides/04-slr.html#sampling-distribution-of-s2",
    "title": "Simple Linear Regression \n",
    "section": "Sampling Distribution of \\(S^2\\)\n",
    "text": "Sampling Distribution of \\(S^2\\)\n\n\n\\(\\frac{SS_{res}}{\\sigma^2} \\sim \\chi^2_{n-2}\\)\n\\(\\frac{SS_{res}}{n-2} = MS_{res} = S^2 \\sim \\sigma^2 \\frac{\\chi^2_{n-2}}{n-2}\\)\n\n\nWe skip the proof here because the proof is not easy as one imagines.\nYou just need to know this fact, and know how to use it. That‚Äôs enough.\nWe will prove this result after we learn liner algebra and multiple regression.\n\n\n\n\\((1-\\alpha)100\\%\\) CI for \\(\\sigma^2\\) is \\(\\left(\\frac{SS_{res}}{\\chi^2_{\\alpha/2, n-2}}, \\frac{SS_{res}}{\\chi^2_{1-\\alpha/2, n-2}}\\right)\\)"
  },
  {
    "objectID": "slides/04-slr.html#r-lab-confidence-interval",
    "href": "slides/04-slr.html#r-lab-confidence-interval",
    "title": "Simple Linear Regression \n",
    "section": "\nR Lab Confidence Interval",
    "text": "R Lab Confidence Interval\n\n## Confidence interval for beta_0 and beta_1\nconfint(reg_fit, level = 0.95)\n\n            2.5 % 97.5 %\n(Intercept) 34.28  37.12\ndispl       -3.91  -3.15\n\n\n\n\n## Confidence interval for sigma^2 (no built-in function)\nalpha <- 0.05\nSS_res <- sum(reg_fit$residuals ^ 2)\nlow_bd <- SS_res / qchisq(alpha / 2, df = reg_fit$df, lower.tail = FALSE)\nupp_bd <- SS_res / qchisq(alpha / 2, df = reg_fit$df, lower.tail = TRUE)\nlow_bd\n\n[1] 12.4\n\nupp_bd\n\n[1] 17.8\n\n\n\n# MS_res (sigma_hat^2)\nsumm_reg_fit$sigma ^ 2\n\n[1] 14.7\n\n\n\nForget about the proof.\nLet‚Äôs be practical and see how to get those numbers in R.\nThere is no built-in function for Confidence interval for sigma^2. You can write your own function to compute it though."
  },
  {
    "objectID": "slides/04-slr.html#hypothesis-testing-beta_1",
    "href": "slides/04-slr.html#hypothesis-testing-beta_1",
    "title": "Simple Linear Regression \n",
    "section": "Hypothesis Testing: \\(\\beta_1\\)\n",
    "text": "Hypothesis Testing: \\(\\beta_1\\)\n\n\n \\(H_0: \\beta_1 = \\beta_1^0 \\quad H_1: \\beta_1 \\ne \\beta_1^0\\) \nstandard error of \\(b_1\\): \\(se(b_1) = \\sqrt{\\frac{MS_{res}}{S_{xx}}}\\)\n\nTest statistic: \\(t_{test} = \\frac{b_1 - \\color{red}{\\beta_1^0}}{se(b_1)} \\sim t_{n-2}\\) under \\(H_0\\).\nReject \\(H_0\\) in favor of \\(H_1\\) if\n\n\n\\(|t_{test}| > t_{\\alpha/2, \\, n-2}\\) (critical value method)\n\n\\(\\text{p-value} = 2P(t_{n-2} > |t_{test}|) < \\alpha\\) (p-value method)\n\n\n\n\nin addition to estimation, we may be interested in testing.\nTo do the testing on \\(\\beta_1\\), the testing procedure is basically the same as the procedure for population mean \\(\\mu\\) we reviewed last week.\nUsually \\(\\beta_1^0 = 0\\), but we may be interested in other values.\ngood growth \\(\\beta_1 > 2\\)"
  },
  {
    "objectID": "slides/04-slr.html#hypothesis-testing-beta_0",
    "href": "slides/04-slr.html#hypothesis-testing-beta_0",
    "title": "Simple Linear Regression \n",
    "section": "Hypothesis Testing: \\(\\beta_0\\)\n",
    "text": "Hypothesis Testing: \\(\\beta_0\\)\n\n\n \\(H_0: \\beta_0 = \\beta_0^0 \\quad H_1: \\beta_0 \\ne \\beta_0^0\\) \nstandard error of \\(b_0\\): \\(se(b_0) = \\sqrt{MS_{res}\\left(1/n + \\overline{x}^2/S_{xx}\\right)}\\)\n\nTest statistic: \\(t_{test} = \\frac{b_0 - \\color{red}{\\beta_0^0}}{se(b_0)} \\sim t_{n-2}\\) under \\(H_0\\)\n\nReject \\(H_0\\) in favor of \\(H_1\\) if\n\n\n\\(|t_{test}| > t_{\\alpha/2, \\, n-2}\\) (critical value method)\n\n\\(\\text{p-value} = 2P(t_{n-2} > |t_{test}|) < \\alpha\\) (p-value method)\n\n\n\n\nHere is the \\(\\beta_0\\) version of the testing.\nBasically we are more interested in \\(\\beta_1\\) because \\(\\beta_1\\) sort of measures the relationship between \\(X\\) and \\(Y\\).\nAgain it depends on your research question."
  },
  {
    "objectID": "slides/04-slr.html#hypothesis-testing-beta_0-1",
    "href": "slides/04-slr.html#hypothesis-testing-beta_0-1",
    "title": "Simple Linear Regression \n",
    "section": "Hypothesis Testing: \\(\\beta_0\\)\n",
    "text": "Hypothesis Testing: \\(\\beta_0\\)\n\n\n \\(H_0: \\beta_0 = \\beta_0^0 \\quad H_1: \\beta_0 \\ne \\beta_0^0\\) \nstandard error of \\(b_0\\): \\(se(b_0) = \\sqrt{MS_{res}\\left(1/n + \\overline{x}^2/S_{xx}\\right)}\\)\n\nTest statistic: \\(t_{test} = \\frac{b_0 - \\color{red}{\\beta_0^0}}{se(b_0)} \\sim t_{n-2}\\) under \\(H_0\\)\n\nReject \\(H_0\\) in favor of \\(H_1\\) if\n\n\n\\(|t_{test}| > t_{\\alpha/2, \\, n-2}\\) (critical value method)\n\n\\(\\text{p-value} = 2P(t_{n-2} > |t_{test}|) < \\alpha\\) (p-value method)\n\n\n\n\nHere is the \\(\\beta_0\\) version of the testing.\nBasically we are more interested in \\(\\beta_1\\) because \\(\\beta_1\\) sort of measures the relationship between \\(X\\) and \\(Y\\).\nAgain it depends on your research question."
  },
  {
    "objectID": "slides/04-slr.html#r-lab-testing-on-beta_0-and-beta_1",
    "href": "slides/04-slr.html#r-lab-testing-on-beta_0-and-beta_1",
    "title": "Simple Linear Regression \n",
    "section": "\nR Lab Testing on \\(\\beta_0\\) and \\(\\beta_1\\)\n",
    "text": "R Lab Testing on \\(\\beta_0\\) and \\(\\beta_1\\)\n\n\nsumm_reg_fit\n\n...\nlm(formula = hwy ~ displ, data = mpg)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-7.104 -2.165 -0.224  2.059 15.010 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)   35.698      0.720    49.5   <2e-16 ***\ndispl         -3.531      0.195   -18.1   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n...\n\n\n\nsumm_reg_fit$coefficients\n\n            Estimate Std. Error t value  Pr(>|t|)\n(Intercept)    35.70      0.720    49.6 2.12e-125\ndispl          -3.53      0.195   -18.2  2.04e-46\n\n\n\nTesting \\(H_0: \\beta_0 = 0\\) and \\(H_0: \\beta_1 = 0\\)"
  },
  {
    "objectID": "slides/04-slr.html#interpretation-of-testing-results",
    "href": "slides/04-slr.html#interpretation-of-testing-results",
    "title": "Simple Linear Regression \n",
    "section": "Interpretation of Testing Results",
    "text": "Interpretation of Testing Results\n\n \\(H_0: \\beta_1 = 0 \\quad H_1: \\beta_1 \\ne 0\\) \n\nFailing to reject \\(H_0: \\beta_1 = 0\\) implies there is no linear relationship between \\(Y\\) and \\(X\\).\n\n\nSo back to the testing on \\(\\beta_1\\).\nIf we do not reject \\(H_0\\), it implies there is no linear relationship between \\(Y\\) and \\(X\\). Right? Because \\(\\beta_1\\), the slope of the regression line is pretty much 0.\n\n\n\n\n\n\n\n\n\n\n\n\nIf we reject \\(H_0: \\beta_1 = 0\\), does it mean \\(X\\) and \\(Y\\) are linearly related?\n\n\nBut it actually has two cases. One \\(x\\) and \\(y\\) may have no relationship at all, or they don‚Äôt have linear relationship but have another kind of relationship, like quadratic.\nSo we have to be careful when we interpret the testing result. OK.\nWe have to be more careful and precise on what we are claiming."
  },
  {
    "objectID": "slides/04-slr.html#test-of-significance-of-regression",
    "href": "slides/04-slr.html#test-of-significance-of-regression",
    "title": "Simple Linear Regression \n",
    "section": "Test of Significance of Regression",
    "text": "Test of Significance of Regression\n\n\nRejecting \\(H_0: \\beta_1 = 0\\) could mean\n\nthe straight-line model is adequate\nbetter results could be obtained with a more complicated model\n\n\n\n\n\n\nRejecting \\(H_0: \\beta_1 = 0\\) could mean\n\nthe straight-line model is adequate\nbetter results could be obtained with a more complicated model even though there is a linear effect of \\(x\\)"
  },
  {
    "objectID": "slides/04-slr.html#x---y-relationship-explains-some-deviation",
    "href": "slides/04-slr.html#x---y-relationship-explains-some-deviation",
    "title": "Simple Linear Regression \n",
    "section": "\n\\(X\\) - \\(Y\\) Relationship Explains Some Deviation",
    "text": "\\(X\\) - \\(Y\\) Relationship Explains Some Deviation\n\nSuppose we only have data \\(Y\\) and have no information about \\(X\\) or no information about the relationship between \\(X\\) and \\(Y\\). How do we predict a value of \\(Y\\)?\n\n\nFor example, suppose we only have MPG information for the sample of cars and have no info about displacement and their relationship. How do we predict a car‚Äôs MPG?\n\n\nOur best guess would be \\(\\overline{y}\\) if the data have no pattern, i.e., \\(\\hat{y}_i = \\overline{y}\\).\nAs we were treating \\(X\\) and \\(Y\\) as uncorrelated.\nThe (total) deviation from the mean is \\((y_i - \\overline{y})\\).\n\n\nWhen we have no information about the relationship between \\(X\\) and \\(Y\\), to predict a value of \\(y\\) using the same value given any value of \\(x\\).\nWhen \\(X\\) and \\(Y\\) are uncorrelated, the regression model is not helping predict \\(Y\\) because \\(X\\) provides no information about \\(Y\\).\nIt means \\(b_1 = 0\\) and \\(\\hat{y}_i = \\bar{y}\\) for all values of \\(X\\).\nThe result is the same as the one when we only have data of \\(Y\\).\nThis prediction deviation \\((y_i - \\overline{y})\\) is generally the biggest deviation we can have when we have no information about how y varies or how y is affected by others.\n\n\n\n\nIf \\(X\\) and \\(Y\\) are linearly related, fitting a linear regression helps us predict the value of \\(Y\\) when the value of \\(X\\) is provided.\n\\(\\hat{y}_i = b_0 + b_1x_i\\) is closer to \\(y_i\\) than \\(\\overline{y}\\).\nThe regression model explains some deviation of \\(y\\)."
  },
  {
    "objectID": "slides/04-slr.html#partition-of-deviation",
    "href": "slides/04-slr.html#partition-of-deviation",
    "title": "Simple Linear Regression \n",
    "section": "Partition of Deviation",
    "text": "Partition of Deviation\n\nTotal deviation = Deviation explained by regression + Unexplained deviation\n\\((y_i - \\overline{y}) = (\\hat{y}_i - \\overline{y}) + (y_i - \\hat{y}_i)\\)\n\\((19 - 9) = (13 - 9) + (19 - 13)\\)\n\n\n\nSo, we can actually Partition the total deviation into two parts.\nPartition of deviation: Total deviation = Explained deviation by regression + Unexplained deviation\n\n\\((y_i - \\overline{y}) = (\\hat{y}_i - \\overline{y}) + (y_i - \\hat{y}_i)\\)\nHere gives you a graphical example.\nTake \\(y_1\\) for example. The total deviation is \\(y_1 - \\bar{y}\\), the red vertical line.\nNow suppose X and Y are linear related, and we fit a linear regression model, then the fitted value is \\(\\hat{y}_1\\).\nNote that \\(\\hat{y}_1\\) is closer to \\(y_1\\), and the regression does provide some prediction power, although the two are not exactly the same.\nThe deviation \\((\\hat{y}_1 - \\overline{y})\\) is the amount of deviation reduced or explained by the regressor \\(X\\).\nThe difference between the fitted value and the observation is the deviation that can not explained or captured by the predictor \\(X\\).\nEven we already make use of the information \\(X\\) provide, some deviation is still there and remained unexplained."
  },
  {
    "objectID": "slides/04-slr.html#sum-of-squares-ss",
    "href": "slides/04-slr.html#sum-of-squares-ss",
    "title": "Simple Linear Regression \n",
    "section": "Sum of Squares (SS)",
    "text": "Sum of Squares (SS)\n\n\\(\\sum_{i=1}^n(y_i - \\overline{y})^2 = \\sum_{i=1}^n(\\hat{y}_i - \\overline{y})^2 + \\sum_{i=1}^n(y_i - \\hat{y}_i)^2\\)\nTotal SS \\((SS_T)\\) = Regression SS \\((SS_R)\\) + Residual SS \\((SS_{res})\\)\n\\(df_T = df_R + df_{res}\\)\n\\(\\color{blue}{(n-1) = 1 +(n-2)}\\)\n\n\n\n\n \\(df_T = n - 1\\): lose 1 df with constraint \\(\\sum_{i=1}^n(y_i - \\overline{y}) = 0\\)\n\n\n \\(df_R = 1\\): all \\(\\hat{y}_i\\) are on the regression line with 2 dfs (intercept and slope), but with constraint \\(\\sum_{i=1}^n(\\hat{y}_i - \\overline{y}) = 0\\)\n\n\n \\(df_{res} = n - 2\\): lose 2 dfs because \\(\\beta_0\\) and \\(\\beta_1\\) are estimated by \\(b_0\\) and \\(b_1\\), which are linear combo of \\(y_i\\)\n\n\n\nTotal variability = variability explained by regression + unexplained variability\n\\(\\sum_{i=1}^n(y_i - \\overline{y})^2 = \\sum_{i=1}^n(\\hat{y}_i - \\overline{y})^2 + \\sum_{i=1}^n(y_i - \\hat{y}_i)^2 + 2\\sum_{i=1}^n(\\hat{y}_i - \\overline{y})(y_i - \\hat{y}_i)\\)\n\n\n \\(df_T = n - 1\\): lose 1 df with constraint \\(\\sum_{i=1}^n(y_i - \\overline{y}) = 0\\)\n\n\n \\(df_R = 1\\): all \\(\\hat{y}_i\\) are on the regression line with 2 dfs (intercept and slope), but with constraint \\(\\sum_{i=1}^n(\\hat{y}_i - \\overline{y}) = 0\\)\n\n\n \\(df_{res} = n - 2\\): lose 2 dfs because \\(\\beta_0\\) and \\(\\beta_1\\) are estimated by \\(b_0\\) and \\(b_1\\), which are linear combo of \\(y_i\\)\n\ndegrees of freedom is the equivalent number of values in the calculation of a statistic that are free to vary.\n\n\\(SS_R = b_1S_{xy}\\) or \\(SS_{res} = SS_T - b_1S_{xy}\\)."
  },
  {
    "objectID": "slides/04-slr.html#anova-for-testing-significance-of-regression",
    "href": "slides/04-slr.html#anova-for-testing-significance-of-regression",
    "title": "Simple Linear Regression \n",
    "section": "ANOVA for Testing Significance of Regression",
    "text": "ANOVA for Testing Significance of Regression\n\n\nA larger value of \\(F_{test}\\) indicates that regression is significant.\nReject \\(H_0\\) if\n\n\\(F_{test} > F_{\\alpha, 1, n-2}\\)\n\n\\(\\text{p-value} = P(F_{1, n-2} > F_{test}) < \\alpha\\).\n\n\nThe ANOVA is designed to test \\(H_0\\) that all predictors have no value in predicting \\(y\\).\nIn SLR, the \\(F\\)-test gives the same result as a two-sided \\(t\\)-test of \\(H_0: \\beta_1=0\\).\n\n\nANOVA is used for testing significance of regression.\nIt is testing if any predictors or regressors have explanatory power for predicting y.\nIn other words, it is testing if the whole regression model is useful or not.\nHere is the ANOVA table.\n\\(H_0: \\beta_1 = 0\\)\nA larger value of \\(F_{test}\\) indicates that regression is significant.\nReject \\(H_0\\) in favor of \\(H_1\\) if \\(F_{test} > F_{\\alpha, 1, n-2}\\) or \\(\\text{p-value} = P(F_{1, n-2} > F_{test}) < \\alpha\\).\nThe ANOVA is designed to test \\(H_0\\) that all predictors have no value in predicting \\(y\\).\nIn SLR, there is only one predictor, and hence the \\(F\\)-test of ANOVA gives the same result as a two-sided \\(t\\)-test of \\(H_0: \\beta_1=0\\).\n\n\nWhat is the \\(H_0\\) of ANOVA \\(F\\)-test if there are \\(k \\ge 2\\) predictors?\n\n\nIf we have \\(k \\ge 2\\) predictors, the \\(F\\)-test is testing \\(H_0: \\beta_1=\\beta_2=\\cdots=\\beta_k=0\\)."
  },
  {
    "objectID": "slides/04-slr.html#r-lab-anova-table",
    "href": "slides/04-slr.html#r-lab-anova-table",
    "title": "Simple Linear Regression \n",
    "section": "\nR Lab ANOVA Table",
    "text": "R Lab ANOVA Table\n\nanova(reg_fit)\n\nAnalysis of Variance Table\n\nResponse: hwy\n           Df Sum Sq Mean Sq F value Pr(>F)    \ndispl       1   4848    4848     329 <2e-16 ***\nResiduals 232   3414      15                   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n\nFor \\(H_0: \\beta_1 = 0\\) in SLR, \\(t_{test}^2 = F_{test}\\).\n\n\nsumm_reg_fit$coefficients \n\n            Estimate Std. Error t value  Pr(>|t|)\n(Intercept)    35.70      0.720    49.6 2.12e-125\ndispl          -3.53      0.195   -18.2  2.04e-46\n\nsumm_reg_fit$coefficients[2, 3] ^ 2\n\n[1] 329\n\n\n\nFor \\(H_0: \\beta_1 = 0\\), \\(t_{test}^2 = \\left(\\frac{b_1}{\\sqrt{MS_{res}/S_{xx}}}\\right)^2 = \\frac{b_1^2S_{xx}}{MS_{res}} = \\frac{b_1S_{xy}}{MS_{res}} = \\frac{MS_R}{MS_{res}} = F_{test}\\)"
  },
  {
    "objectID": "slides/04-slr.html#coefficient-of-determination",
    "href": "slides/04-slr.html#coefficient-of-determination",
    "title": "Simple Linear Regression \n",
    "section": "Coefficient of Determination",
    "text": "Coefficient of Determination\n\nThe coefficient of determination \\((R^2)\\) is the proportion of the variation in \\(y\\) that is explained by the regression model: \\[R^2 = \\frac{SS_R}{SS_T} =\\frac{SS_T - SS_{res}}{SS_T} = 1 - \\frac{SS_{res}}{SS_T}\\]\n\\(R^2\\) as the proportionate reduction of total variation associated with the use of \\(X\\).\n(a) \\(\\hat{y}_i = y_i\\) and \\(\\small SS_{res} = \\sum_{i=1}^n(y_i - \\hat{y}_i)^2 = 0\\). (b) \\(\\hat{y}_i = \\overline{y}\\) and \\(\\small SS_R = \\sum_{i=1}^n(\\hat{y}_i - \\overline{y})^2 = 0\\).\n\n\n\n\ncoefficient of determination can be used to measure the quality of our regression or the explanatory power of regressors.\nHere (a) and (b) are two extreme cases.\nIn (a), the fitted value = the true observation. So the regression model explains all the variation in \\(Y\\), and hence \\(R^2 = 1\\).\nIn (b), the fitted value = mean of y as if we don‚Äôt have information about \\(x\\) or \\(x\\) is totally useless in predicting \\(Y\\). In this case, the regression model explains no the variation in \\(Y\\), and all variation remain unexplained. So \\(R^2 = 0\\)."
  },
  {
    "objectID": "slides/04-slr.html#r-lab-r2",
    "href": "slides/04-slr.html#r-lab-r2",
    "title": "Simple Linear Regression \n",
    "section": "\nR Lab \\(R^2\\)\n",
    "text": "R Lab \\(R^2\\)\n\n\nsumm_reg_fit\n\n...\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)   35.698      0.720    49.5   <2e-16 ***\ndispl         -3.531      0.195   -18.1   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.84 on 232 degrees of freedom\nMultiple R-squared:  0.587, Adjusted R-squared:  0.585 \nF-statistic:  329 on 1 and 232 DF,  p-value: <2e-16\n...\n\n\n\nsumm_reg_fit$r.squared\n\n[1] 0.587"
  },
  {
    "objectID": "slides/04-slr.html#predicting-the-mean-response-sampling-distribution",
    "href": "slides/04-slr.html#predicting-the-mean-response-sampling-distribution",
    "title": "Simple Linear Regression \n",
    "section": "Predicting the Mean Response: Sampling Distribution",
    "text": "Predicting the Mean Response: Sampling Distribution\n\n\nWith predictor value \\(x = x_0\\), we want to estimate the mean response \\[E(y\\mid x_0) = \\mu_{y|x_0} = \\beta_0 + \\beta_1 x_0\\].\n\n The mean highway MPG \\(E(y \\mid x_0)\\) when displacement is \\(x = x_0 = 5.5\\). \n\n\nIf \\(x_0\\) is within the range of \\(x\\), an unbiased point estimate of \\(E(y\\mid x_0)\\) is \\[\\widehat{E(y\\mid x_0)} = \\hat{\\mu}_{y | x_0} = b_0 + b_1 x_0\\]\nThe sampling distribution of \\(\\hat{\\mu}_{y | x_0}\\) is \\[N\\left( \\mu_{y | x_0} = \\beta_0 + \\beta_1 x_0, \\sigma^2\\left(\\frac{1}{n} + \\frac{(x_0 - \\overline{x})^2}{S_{xx}} \\right) \\right)\\]\n\n\nRemember there are two types of prediction, predicting the mean response \\(E(y\\mid x_0)\\) and predicting an observation value given a value of \\(x\\).\nHere, we are doing real prediction, because we are not predicting the observation given the \\(x\\) value in the data set, but we use our data set to do prediction when \\(x\\) is a new value that is not in the data set.\nSo \\(x_0\\) is any new value that is not shown in the data set.\nWe are interested in predicting a new mean response or new observation given the new value of \\(X\\). OK.\nFor a given predictor value \\(x = x_0\\), we want to estimate the mean response \\(E(y\\mid x_0) = \\mu_{y|x_0}.\\)\n\n The mean highway MPG \\(E(y \\mid x_0)\\) when displacement is \\(x = x_0 = 5.5\\). \n\n\nIf \\(x_0\\) is within the range of the sample data on \\(x\\), an unbiased point estimator for \\(E(y\\mid x_0)\\) is \\[\\widehat{E(y\\mid x_0)} = \\hat{\\mu}_{y | x_0} = b_0 + b_1 x_0\\]\n\nThe sampling distribution of \\(\\hat{\\mu}_{y | x_0}\\) is \\[N\\left( \\mu_{y | x_0} = \\beta_0 + \\beta_1 x_0, \\sigma^2\\left(\\frac{1}{n} + \\frac{(x_0 - \\overline{x})^2}{S_{xx}} \\right) \\right)\\]\n\n\n\\(\\widehat{E(y\\mid x_0)} = \\hat{\\mu}_{y | x_0} = b_0 + b_1 x_0\\); \\(\\quad E(y\\mid x_0) = \\mu_{y | x_0} = \\beta_0 + \\beta_1 x_0\\)"
  },
  {
    "objectID": "slides/04-slr.html#predicting-the-mean-response-confidence-interval",
    "href": "slides/04-slr.html#predicting-the-mean-response-confidence-interval",
    "title": "Simple Linear Regression \n",
    "section": "Predicting the Mean Response: Confidence Interval",
    "text": "Predicting the Mean Response: Confidence Interval\n\\((\\hat{\\mu}_{y | x_0} = b_0 + b_1 x_0) \\sim N\\left( \\mu_{y | x_0} = \\beta_0 + \\beta_1 x_0, \\sigma^2\\left(\\frac{1}{n} + \\frac{(x_0 - \\overline{x})^2}{S_{xx}} \\right) \\right)\\)\n\\[\\frac{(b_0 + b_1 x_0) - (\\beta_0 + \\beta_1 x_0)}{\\sigma\\sqrt{\\frac{1}{n} + \\frac{(x_0 - \\overline{x})^2}{S_{xx}}}} \\sim N(0, 1)\\]\n\n\\[\\frac{(b_0 + b_1 x_0) - (\\beta_0 + \\beta_1 x_0)}{s\\sqrt{\\frac{1}{n} + \\frac{(x_0 - \\overline{x})^2}{S_{xx}}}} \\sim t_{n-2}\\]\n\n\nThe \\((1-\\alpha)100\\%\\) CI for \\(E(y\\mid x_0)\\) is \\(\\boxed{\\hat{\\mu}_{y | x_0} \\pm t_{\\alpha/2, n-2} s\\sqrt{\\frac{1}{n} + \\frac{(x_0 - \\overline{x})^2}{S_{xx}}}}\\).\n\n\n\nDoes the length of the CI for \\(E(y\\mid x_0)\\) stay the same at any location of \\(x_0\\)?\n\n\nThe length of the CI depends on the location of the point of interest."
  },
  {
    "objectID": "slides/04-slr.html#predicting-new-observations-sampling-distribution",
    "href": "slides/04-slr.html#predicting-new-observations-sampling-distribution",
    "title": "Simple Linear Regression \n",
    "section": "Predicting New Observations: Sampling Distribution",
    "text": "Predicting New Observations: Sampling Distribution\n\nPredict the value of a new observation \\(y_0\\) with \\(x = x_0\\).\n\n The highway MPG of a car \\(y_0(x_0)\\) when its displacement is \\(x = x_0 = 5.5\\). \n\n\nAn unbiased point estimate of \\(y_0(x_0)\\) is \\[\\hat{y}_0(x_0) = b_0 + b_1 x_0\\]\n\n\n\n\nWhat is the sampling distribution of \\(\\hat{y}_0\\)?\n\n\n\n\n\\(\\hat{y}_0 = b_0 + b_1 x_0 \\sim N\\left(\\beta_0 + \\beta_1 x_0, \\sigma^2\\left(\\frac{1}{n} + \\frac{(x_0 - \\overline{x})^2}{S_{xx}} \\right) \\right)\\)\n\n\n\n\nWhat is the distribution of \\(y_0 = \\beta_0 + \\beta_1x_0 + \\epsilon\\)?\n\n\n\n\\(y_0(x_0)\\) itself is a r.v.\n\n\n\n\n\\(y_0 \\sim N\\left(\\beta_0 + \\beta_1 x_0, \\sigma^2 \\right)\\)\n\n\n\n\nWhat is the distribution of \\(y_0 - \\hat{y}_0\\)?\n\n\nPredict the value of a new observation \\(y_0\\) corresponding to a specified value of predictor \\(x = x_0\\).\n\n The highway MPG of a car \\(y_0(x_0)\\) when its displacement is \\(x = x_0 = 5.5\\). \n\n\nAn unbiased point estimator for \\(y_0(x_0)\\) is \\[\\hat{y}_0(x_0) = b_0 + b_1 x_0\\]"
  },
  {
    "objectID": "slides/04-slr.html#predicting-new-observations-prediction-interval",
    "href": "slides/04-slr.html#predicting-new-observations-prediction-interval",
    "title": "Simple Linear Regression \n",
    "section": "Predicting New Observations: Prediction Interval",
    "text": "Predicting New Observations: Prediction Interval\n\n\n\\(y_0 - \\hat{y}_0 \\sim N\\left(0, \\sigma^2\\left(1 + \\frac{1}{n} + \\frac{(x_0 - \\overline{x})^2}{S_{xx}} \\right) \\right)\\)  \n\n\n\n\\[\\frac{(y_0 - \\hat{y}_0) - \\color{red}{0}}{\\sigma\\sqrt{1 + \\frac{1}{n} + \\frac{(x_0 - \\overline{x})^2}{S_{xx}}}} \\sim N(0, 1)\\]\n\n\n\\[\\frac{y_0 - \\hat{y}_0}{s\\sqrt{1 + \\frac{1}{n} + \\frac{(x_0 - \\overline{x})^2}{S_{xx}}}} \\sim t_{n-2}\\]\n\n\nThe \\((1-\\alpha)100\\%\\) prediction interval (PI) for \\(y_0(x_0)\\) is \\(\\small \\boxed{\\hat{y_0} \\pm t_{\\alpha/2, n-2} s\\sqrt{1+ \\frac{1}{n} + \\frac{(x_0 - \\overline{x})^2}{S_{xx}}}}\\)\n\n\n\nWhat is the difference between CI for \\(E(y\\mid x_0)\\) and PI for \\(y_0(x_0)\\)?\n\n\n\n\n\nThe PI is wider as it includes the uncertainty about \\(b_0\\), \\(b_1\\) as well as \\(y_0\\) due to error \\(\\epsilon\\).\n\n\nThe PI at \\(x_0\\) is wider than the CI at \\(x_0\\) because the PI depends on both the uncertainty about the fitted model \\((b_0\\) and \\(b_1)\\) and the error \\(\\epsilon\\) associated with the future observation \\(y_0\\)."
  },
  {
    "objectID": "slides/04-slr.html#r-lab-prediction",
    "href": "slides/04-slr.html#r-lab-prediction",
    "title": "Simple Linear Regression \n",
    "section": "\nR Lab Prediction",
    "text": "R Lab Prediction\n\n## CI for the mean response\npredict(reg_fit, newdata = data.frame(displ = 5.5), interval = \"confidence\", level = 0.95)\n\n   fit  lwr  upr\n1 16.3 15.4 17.2\n\n## PI for the new observation\npredict(reg_fit, newdata = data.frame(displ = 5.5), interval = \"predict\", level = 0.95)\n\n   fit  lwr  upr\n1 16.3 8.67 23.9\n\n\n\n\nIf I can only use one point or value to predict \\(y_0\\), the best guess is the predicted value on the regression line.\nAfter all, values around the lines would more likely to be drawn based on our model.\nHere you can understand why uncertainty quantification is important.\nYes, we can predict an new observation value, but the prediction quality is gonna be bad because we are predict a random variable, not a constant, and \\(y_0\\) can vary a lot around the regression line.\nAnd prediction interval gives us an idea of how good or how bad our prediction is."
  },
  {
    "objectID": "slides/04-slr.html#section-3",
    "href": "slides/04-slr.html#section-3",
    "title": "Simple Linear Regression \n",
    "section": "",
    "text": "CI is the shortest when \\(x = \\bar{x}\\). (Also PI)\nPI length looks the same along with \\(x\\) because the \\(\\sigma^2\\) dominates the uncertainty, comparing to the uncertainty about \\(b_0\\) and \\(b_1\\)."
  },
  {
    "objectID": "slides/04-slr.html#considerations-in-regression-extrapolation",
    "href": "slides/04-slr.html#considerations-in-regression-extrapolation",
    "title": "Simple Linear Regression \n",
    "section": "Considerations in Regression: Extrapolation",
    "text": "Considerations in Regression: Extrapolation\n\nRegression models are intended as interpolation equations over the range of the regressors used to fit the model.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFirst, Regression models are intended as interpolation equations over the range of the regressors used to fit the model.\nRegression does a really bad job for extrapolation.\nLook at this figure for example.\nSuppose the data we collected have the range of X like this.\nBased on the data, we train our model, and estimate the parameters for prediction and inference.\nOur prediction and inference will be more plausible or convicing when the new \\(x\\) is within the range of \\(X\\).\nAgain, the only information we have is our data.\nWhen we do extrapolation, it‚Äôs like doing a prediction or inference without any reliable information at hand.\nOur conclusion may be totally wrong. Based on the collected data, we thought x and y are linear related.\nBut the true x-y relationship may not be linear at all, and because our data only cover a small range of \\(X\\), we don‚Äôt know what really happens outside the range of data.\nWe cannot say much about the predictive mean response or observation outside the range of \\(X\\), and we cannot conclude the relationship between x and y outside the range as well."
  },
  {
    "objectID": "slides/04-slr.html#considerations-in-the-use-of-regression-outliers",
    "href": "slides/04-slr.html#considerations-in-the-use-of-regression-outliers",
    "title": "Simple Linear Regression \n",
    "section": "Considerations in the Use of Regression: Outliers",
    "text": "Considerations in the Use of Regression: Outliers\n\n\nOutlier: An observation that is considerably different from the rest of the data (unusual in \\(y\\) direction)\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe estimate of the intercept may be incorrect.\nThe residual mean square may be inflated.\n\n\nsummary(lm_outlier)$sigma ^ 2\n\n[1] 1421\n\nsummary(lm_no_outlier)$sigma ^ 2\n\n[1] 101\n\n\n\n\n\nThe second consideration is outlier.\nit might be just measurement error.\nIf it is not an typo, we have to look into this point with great care, and see why this happened and see if we need to do some correction to it.\nWe will talk about outliers in detail later in this course."
  },
  {
    "objectID": "slides/04-slr.html#considerations-in-regression-influential-points",
    "href": "slides/04-slr.html#considerations-in-regression-influential-points",
    "title": "Simple Linear Regression \n",
    "section": "Considerations in Regression: Influential Points",
    "text": "Considerations in Regression: Influential Points\n\n\nInfluential point: A point that strongly affects the slope of the line. (unusual in \\(x\\) direction)\n\n\n\nOur inference and prediction may be totally distorted just because of one single influential point. So again, we have to very careful about it."
  },
  {
    "objectID": "slides/04-slr.html#considerations-in-regression-causal-relationship",
    "href": "slides/04-slr.html#considerations-in-regression-causal-relationship",
    "title": "Simple Linear Regression \n",
    "section": "Considerations in Regression: Causal Relationship?",
    "text": "Considerations in Regression: Causal Relationship?\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRegression is just a model. It does not tell you X-Y‚Äôs relationship is correlation relationship or causal relationship.\nIt is the person who uses the model tell the relationship.\nSo we have to be careful when interpreting regression result.\nThey are correlated, not one causes the other.\nActually, there is another factor that causes the sales and # of drownings to go up and down together."
  },
  {
    "objectID": "slides/04-slr.html#considerations-in-regression-unknown-predictor",
    "href": "slides/04-slr.html#considerations-in-regression-unknown-predictor",
    "title": "Simple Linear Regression \n",
    "section": "Considerations in Regression: Unknown Predictor",
    "text": "Considerations in Regression: Unknown Predictor\n\nMaximum daily load \\((Y)\\) on an electric power generation system and the maximum daily temperature \\((X)\\).\nTo predict tomorrow maximum daily load, we must first know tomorrow maximum temperature.\nThe prediction of maximum load is conditional on the temperature forecast."
  },
  {
    "objectID": "slides/04-slr.html#likelihood-function",
    "href": "slides/04-slr.html#likelihood-function",
    "title": "Simple Linear Regression \n",
    "section": "Likelihood Function",
    "text": "Likelihood Function\n\nMaximum likelihood estimation is a method of finding point estimators.\nSuppose \\(y_i \\stackrel{iid}{\\sim} f(y|\\theta)\\)\n\nThe joint probability function of the data \\((y_1, y_2, \\dots, y_n)\\) is \\[f(y_1, y_2, \\dots, y_n|\\theta) = \\prod_{i = 1}^nf(y_i|\\theta)\\]\n\nWhen the function is viewed as a function of \\(\\theta\\), with the data given, it is called the likelihood function \\(L(\\theta|{\\bf y} = (y_1, y_2, \\dots, y_n))\\): \\[L(\\theta|{\\bf y}) =  \\prod_{i = 1}^nf(y_i|\\theta)\\]\n\n\n\\(L(\\theta|{\\bf y})\\) is not a probability or density function.\n\n\nMaximum likelihood is a method of finding point estimators.\nSample independently from a population whose pmf/pdf is \\(f(y|\\theta)\\) with unknown \\(\\theta\\).\nThe joint probability function of the data \\((y_1, y_2, \\dots, y_n)\\) is \\[f(y_1, y_2, \\dots, y_n|\\theta) = \\prod_{i = 1}^nf(y_i|\\theta)\\]\n\nWhen the function is viewed as a function of \\(\\theta\\), with the data given, it is called the likelihood function \\(L(\\theta|{\\bf y} = (y_1, y_2, \\dots, y_n))\\): \\[L(\\theta) =  \\prod_{i = 1}^nf(y_i|\\theta)\\]"
  },
  {
    "objectID": "slides/04-slr.html#likelihood-function-1",
    "href": "slides/04-slr.html#likelihood-function-1",
    "title": "Simple Linear Regression \n",
    "section": "Likelihood Function",
    "text": "Likelihood Function\n\nFor easier calculation and computation, we work with the log-likelihood function \\[\\ell(\\theta|{\\bf y}) := \\log L(\\theta|{\\bf y})\\]\n\n\n\n\n\n\n\nFor easier calculation and computation, we usually work with the log-likelihood function defined by \\[\\ell(\\theta|{\\bf y}) := \\log L(\\theta|{\\bf y})\\]"
  },
  {
    "objectID": "slides/04-slr.html#example-maximum-likelihood-estimation",
    "href": "slides/04-slr.html#example-maximum-likelihood-estimation",
    "title": "Simple Linear Regression \n",
    "section": "Example: Maximum Likelihood Estimation",
    "text": "Example: Maximum Likelihood Estimation\n\nMaximizing \\(L(\\theta)\\) with respect to \\(\\theta\\) yields the maximum likelihood estimator of \\(\\theta\\), i.e., \\[\\hat{\\theta}_{ML} = \\underset{\\theta}{\\arg \\max} L(\\theta) = \\underset{\\theta}{\\arg \\max} \\log L(\\theta)\\]\n\n\nIntuition: Given the data \\((y_1, y_2, \\dots, y_n)\\), we are finding a value of parameter \\(\\theta\\) so that the given data set is most likely to be sampled.\n\n\n\nExample: Suppose \\(Y_1, Y_2, \\dots, Y_n \\stackrel{iid}{\\sim} Bernoulli(\\theta)\\), where \\(\\theta\\), the probability of success, is the unknown parameter to be estimated.\n\n\n\n\nWhat is the Bernoulli distribution?\n\n\n\n\nThe probability function of \\(Y_i\\) is \\(P(Y_i = y_i) = f(y_i) = \\theta^{y_i}(1-\\theta)^{1-y_i}\\) for \\(y_i=0, 1\\). \\[L(\\theta) = \\prod_{i = 1}^nf(y_i|\\theta) = \\prod_{i = 1}^n\\theta^{y_i}(1-\\theta)^{1-y_i} = \\theta^{\\sum_{i=1}^ny_i}(1-\\theta)^{n-\\sum_{i=1}^ny_i}\\]"
  },
  {
    "objectID": "slides/04-slr.html#example-maximum-likelihood-estimation-1",
    "href": "slides/04-slr.html#example-maximum-likelihood-estimation-1",
    "title": "Simple Linear Regression \n",
    "section": "Example: Maximum Likelihood Estimation",
    "text": "Example: Maximum Likelihood Estimation\n\\[L(\\theta) = \\prod_{i = 1}^nf(y_i|\\theta) = \\prod_{i = 1}^n\\theta^{y_i}(1-\\theta)^{1-y_i} = \\theta^{\\sum_{i=1}^ny_i}(1-\\theta)^{n-\\sum_{i=1}^ny_i}\\]\n\n\nSuppose \\(n = 20\\) and \\(\\sum_{i=1}^{20}y_i = 12\\) from the sample data\n\n\\(L(\\theta) = \\theta^{12}(1-\\theta)^{8}\\)\n\\(\\log L(\\theta) = 12 \\log(\\theta)+8\\log(1-\\theta)\\)\n\n\nFist order condition: \\(\\frac{d \\, \\log L(\\theta)}{d \\, \\theta} = \\frac{12}{\\theta} - \\frac{8}{1-\\theta} = 0\\).\n\\(\\hat{\\theta}_{ML} = 12/20 = 0.6\\).\n\n\n\nGiven \\(n = 20\\) and \\(\\sum_{i=1}^{20}y_i = 12\\), \\(\\hat{\\theta}_{ML} = 12/20 = 0.6\\) makes the data most possible."
  },
  {
    "objectID": "slides/04-slr.html#example-maximum-likelihood-estimation-2",
    "href": "slides/04-slr.html#example-maximum-likelihood-estimation-2",
    "title": "Simple Linear Regression \n",
    "section": "Example: Maximum Likelihood Estimation",
    "text": "Example: Maximum Likelihood Estimation\n\nThe estimated probability of successes \\(\\hat{\\theta}_{ML}\\) is the sample proportion of successes \\(\\frac{\\sum_{i=1}^n y_i}{n} = \\frac{12}{20}\\)."
  },
  {
    "objectID": "slides/04-slr.html#linear-regression-estimation-by-maximum-likelihood",
    "href": "slides/04-slr.html#linear-regression-estimation-by-maximum-likelihood",
    "title": "Simple Linear Regression \n",
    "section": "Linear Regression: Estimation by Maximum Likelihood",
    "text": "Linear Regression: Estimation by Maximum Likelihood\n\n\n\n\\(Y_i = \\beta_0 + \\beta_1X_i + \\epsilon_i\\) with \\(\\epsilon_i \\stackrel{iid}{\\sim} N(0, \\sigma^2)\\). This means \\(Y_i \\stackrel{iid}{\\sim} N(\\beta_0 + \\beta_1X_i, \\sigma^2)\\)\n\n\n\n\n\nGiven data \\(\\{(x_i, y_i)\\}_{i=1}^n\\), \\[\\small L(\\beta_0, \\beta_1, \\sigma^2 \\mid \\{(x_i, y_i)\\}_{i=1}^n) = \\prod_{i=1}^nN(y_i \\mid \\beta_0 + \\beta_1x_i, \\sigma^2)\\] \\[\\begin{align}\n\\small\n\\ell(\\beta_0, \\beta_1, \\sigma^2 \\mid \\{(x_i, y_i)\\}_{i=1}^n)\n\\small\n&= \\sum_{i=1}^n\\log N(y_i \\mid \\beta_0 + \\beta_1x_i, \\sigma^2) \\\\\n\\end{align}\\]\n\n\n\n\\[\\begin{align}\n\\small \\quad \\quad\n    \\left.\\frac{\\partial \\ell}{\\partial\\beta_0}\\right\\vert_{\\tilde{\\beta}_0, \\tilde{\\beta}_1, \\tilde{\\sigma}^2} & = 0\\\\\n  \\small \\quad \\quad\n    \\left. \\frac{\\partial \\ell}{\\partial\\beta_1}\\right\\vert_{\\tilde{\\beta}_0, \\tilde{\\beta}_1, \\tilde{\\sigma}^2} &= 0\\\\\n  \\small \\quad \\quad\n    \\left. \\frac{\\partial \\ell}{\\partial\\sigma^2}\\right\\vert_{\\tilde{\\beta}_0, \\tilde{\\beta}_1, \\tilde{\\sigma}^2} &= 0\n\\end{align}\\]"
  },
  {
    "objectID": "slides/04-slr.html#estimation-by-maximum-likelihood",
    "href": "slides/04-slr.html#estimation-by-maximum-likelihood",
    "title": "Simple Linear Regression \n",
    "section": "Estimation by Maximum Likelihood",
    "text": "Estimation by Maximum Likelihood\n\n\\(\\color{red}{\\tilde{\\beta}_0 = \\overline{y} - \\tilde{\\beta}_1\\overline{x}} = b_0\\)\n\\(\\color{red}{\\tilde{\\beta}_1 = \\frac{\\sum_{i=1}^n(x_i - \\overline{x})y_i}{\\sum_{i=1}^n(x_i - \\overline{x})^2}} = b_1\\)\n\\(\\color{red}{\\tilde{\\sigma}^2 = \\frac{\\sum_{i=1}^n(y_i - \\tilde{\\beta}_0 - \\tilde{\\beta}_1x_i)^2}{n}}\\)\nThe MLE of \\(\\sigma^2\\) is biased.\nIn general, MLE have better statistical properties than LSE.\nMLE requires a full distributional assumption whereas LSE does not.\n\n(Not a serious problem here though)\n\n\n\nmath4780-f23.github.io/website"
  },
  {
    "objectID": "slides/01-syllabus.html#my-journey",
    "href": "slides/01-syllabus.html#my-journey",
    "title": "Welcome Aboard! üôå",
    "section": "My Journey",
    "text": "My Journey\n\nAssistant Professor (2020/08 - )\n\n\n\n\n\n\n\n\n\n\nPostdoctoral Fellow\n\n\n\n\n\n\n\n\n\n\nPhD in Statistics\n\n\n\n\n\n\n\n\n\n\nMA in Economics/PhD program in Statistics\n\n\n\n\n\n\n\n\n\nAfter college, working and doing military service for several years, I came to the US for my PhD degree. Originally I would like to study economics, but then I switched my major to statistics.\n\nI got my master degree in economics from Indiana University Bloomington, then I transferred to UC Santa Cruz to finish my PhD studies.\nThen I spent two years doing my postdoctoral research at Rice University in Houston, Texas.\nFinally, in fall 2020, I came to Marquette as an assistant professor.\nMidwest/Indiana-West/California-South/Texas-Midwest/Wisconsin\nBeen to any one of these universities/cities?\nThe most beautiful campus.\nWho are international students? I can totally understand how hard studying and living in another country. Feel free to share your stories or difficulties, and I am more than happy to help you if you have any questions.\nPoor listening and speaking skills. I was shy.\nOK so, this is my background. How about you introducing yourself as well. You can share anything, your major, hobbies, your favorite food, what do you want to do after graduation, anything,\nI have the class list. I‚Äôd like to learn your face and remember your name. You know, you all wear a mask. It‚Äôs hard to recognize you and connect your name and your face.\nWhen I call your name, you can say something about yourself. No need to be long, couple of seconds are good."
  },
  {
    "objectID": "slides/01-syllabus.html#my-research",
    "href": "slides/01-syllabus.html#my-research",
    "title": "Welcome Aboard! üôå",
    "section": "My Research",
    "text": "My Research\n\nBayesian spatio-temporal modeling and machine learning algorithms in neuroimaging\nBayesian Deep Learning for image classification\nEfficient MCMC for big \\(n\\) big \\(p\\) streaming data\nGame-based learning for STEM and data science education\n\n\n\nfMRI\n\n\n\n\n\n\n\n\n\nEEG"
  },
  {
    "objectID": "slides/01-syllabus.html#how-to-reach-me",
    "href": "slides/01-syllabus.html#how-to-reach-me",
    "title": "Welcome Aboard! üôå",
    "section": "How to Reach Me",
    "text": "How to Reach Me\n\nOffice hours TuTh 3:20 - 4:50 PM in Cudahy Hall 353.\nüìß cheng-han.yu@marquette.edu\n\nAnswer questions within 24 hours.\nExpect a reply on Monday if shoot me a message on weekends.\nStart your subject line with [math4780] or [mssc5780] followed by a clear description of your question.\n\n\n\n\n\nI will NOT reply your e-mail if ‚Ä¶ Check the email policy in the syllabus!"
  },
  {
    "objectID": "slides/01-syllabus.html#textbook-lra",
    "href": "slides/01-syllabus.html#textbook-lra",
    "title": "Welcome Aboard! üôå",
    "section": "Textbook (LRA)",
    "text": "Textbook (LRA)\n\n\n\n\nIntroduction to Linear Regression Analysis, 6th edition, by D. C. Montgomery, E. A. Peck and G. G. Vining. Publisher: Wiley. \n\nCh 1 ~ 10, 13.\nCareful about typos.\n\nIn the Preface,\n\nThe book ‚Ä¶ for a course taken by seniors and 1st-year graduate students‚Ä¶.\n\n\nSome knowledge of matrix algebra is also necessary.\n\n\n\n\n\n\n\n\n\n\n\n\n\nCourse materials are grabbed from several books and resources.\nOur course website would be helpful."
  },
  {
    "objectID": "slides/01-syllabus.html#reference-cmr",
    "href": "slides/01-syllabus.html#reference-cmr",
    "title": "Welcome Aboard! üôå",
    "section": "Reference (CMR)",
    "text": "Reference (CMR)\n\n\n\n\nClassical and Modern Regression with Applications, by Raymond Myers. Publisher: Duxbury Press.\nCh 1 ~ 8.\nThe textbook when I was a master student at Indiana University.\nOutdated (published in 1990) and no code involved.\nExplains concepts well.\n\nIn the Preface,\n\nThe book ‚Ä¶ for seniors and graduate students majoring in statistics or user subject-matter fields.\n\n\n\n\n\n\n\n\n\n\n\n\n\nI also provide some other reference books. They are all optional, but some course materials may borrow from these books.\nIt‚Äôs a pretty old book which was published in 1990. So it‚Äôs a little bit outdated. But it explains basic ideas and concepts pretty well. I enjoyed reading it."
  },
  {
    "objectID": "slides/01-syllabus.html#reference-car",
    "href": "slides/01-syllabus.html#reference-car",
    "title": "Welcome Aboard! üôå",
    "section": "Reference (CAR)",
    "text": "Reference (CAR)\n\n\n\n\nAn R Companion to Applied Regression, by John Fox and Sanford Weisberg. Publisher: SAGE.\nCh 3, 5, 8.\nR scripts and data on the book Website\n\nDoing regression easier by the car and effects R packages.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIf you wanna learn how to use R to do regression, this book could be helpful.\nThe authors create 2 R packages car and effects that make doing regression so much easier.\nLater you‚Äôll see we can just use some simple function calls to generate some important and useful graphics for regression analysis.\n\nJohn Fox received a BA from the City College of New York and a PhD from the University of Michigan, both in Sociology. Sanford Weisberg is Professor Emeritus of statistics at the University of Minnesota. He has also served as the director of the University‚Ä≤s Statistical Consulting Service, and has worked with hundreds of social scientists and others on the statistical aspects of their research. He earned a BA in statistics from the University of California, Berkeley, and a Ph.D., also in statistics, from Harvard University"
  },
  {
    "objectID": "slides/01-syllabus.html#reference-isl",
    "href": "slides/01-syllabus.html#reference-isl",
    "title": "Welcome Aboard! üôå",
    "section": "Reference (ISL)",
    "text": "Reference (ISL)\n\n\n\n\nAn Introduction to Statistical Learning, by James et. al.¬†Publisher: Springer.\nCh 3.\nR and Python code\n\nIn the Preface,\n\nISL ‚Ä¶ for advanced undergraduates or master‚Äôs students in Statistics or related quantitative fields‚Ä¶‚Ä¶"
  },
  {
    "objectID": "slides/01-syllabus.html#more-references",
    "href": "slides/01-syllabus.html#more-references",
    "title": "Welcome Aboard! üôå",
    "section": "More References",
    "text": "More References\n\nApplied Regression Analysis and Generalized Linear Models\nApplied Linear Regression\nLinear Models with R/Python\nApplied Linear Statistical Models\nRegression Daignostics\n\nand more!"
  },
  {
    "objectID": "slides/01-syllabus.html#prerequisites",
    "href": "slides/01-syllabus.html#prerequisites",
    "title": "Welcome Aboard! üôå",
    "section": "Prerequisites",
    "text": "Prerequisites\n\nOn bulletin: MATH 2780 (Baby Regression), MATH 4720 (Intro Stats) or equivalent.\n\n\nHelpful if you\n\ncode (any language) üíª\n\n\n\n\nlearned basic linear algebra (MATH 3100) üî¢\n\n\n\n\ntook probability (MATH 4700) and statistical inference (MATH 4710) üé≤\n\n\nOn the Marquette bulletin, the prerequisite is either MATH 2780 or MATH 4720 or equivalent.\nBut this course requires more than basic statistics experience.\nNowadays, nobody is doing regression by hand, and we will write some code to do regression. So it would be helpful if you code any language before."
  },
  {
    "objectID": "slides/01-syllabus.html#my-statistics-book",
    "href": "slides/01-syllabus.html#my-statistics-book",
    "title": "Welcome Aboard! üôå",
    "section": "My Statistics Book",
    "text": "My Statistics Book\n\nPlease review probability distributions, confidence interval, and hypothesis testing!"
  },
  {
    "objectID": "slides/01-syllabus.html#course-website---httpsmath4780-f23.github.iowebsite",
    "href": "slides/01-syllabus.html#course-website---httpsmath4780-f23.github.iowebsite",
    "title": "Welcome Aboard! üôå",
    "section": "Course Website - https://math4780-f23.github.io/website/\n",
    "text": "Course Website - https://math4780-f23.github.io/website/\n\n\nAll course materials"
  },
  {
    "objectID": "slides/01-syllabus.html#course-website---d2l",
    "href": "slides/01-syllabus.html#course-website---d2l",
    "title": "Welcome Aboard! üôå",
    "section": "Course Website - D2L\n",
    "text": "Course Website - D2L\n\n\nHomework submission\nGradebook"
  },
  {
    "objectID": "slides/01-syllabus.html#grading-policy",
    "href": "slides/01-syllabus.html#grading-policy",
    "title": "Welcome Aboard! üôå",
    "section": "Grading Policy ‚ú®",
    "text": "Grading Policy ‚ú®\n\nFor MATH 4780 (MSSC 5780) students, the grade is earned out of 1000 (1200) total points distributed as follows:\n\nHomework 1 to 6: 480 pts (80 pts each)\nIn-class Exam: 160 pts\nFinal project: 300 pts\nClass Participation: 60 pts\nMSSC 5780 work: 200 pts\n\n\n‚ùå No extra credit projects/homework/exam to compensate for a poor grade.\nIndividual grade will NOT be curved.\nWant to obtain a good grade? Study hard. No pain, no gain! ‚úç ‚úç\n\n\nEveryone has the same opportunity to do well in this class.\n4780 students are encouraged to do MSSC problems: Up to 100 pts extra credit.\nI am not a harsh grader, but this is not an easy A course as well."
  },
  {
    "objectID": "slides/01-syllabus.html#grade-percentage-conversion",
    "href": "slides/01-syllabus.html#grade-percentage-conversion",
    "title": "Welcome Aboard! üôå",
    "section": "Grade-Percentage Conversion",
    "text": "Grade-Percentage Conversion\n\n\\([x, y)\\) means greater than or equal to \\(x\\) and less than \\(y\\).\nFor example, 94.0 is in [94, 100] and the grade is A and 93.8 is in [90, 94) and the grade is A-.\n\n\n\n\n\n\n Grade \n    Percentage \n  \n\n\n A \n    [94, 100] \n  \n\n A- \n    [90, 94) \n  \n\n B+ \n    [87, 90) \n  \n\n B \n    [84, 87) \n  \n\n B- \n    [80, 84) \n  \n\n C+ \n    [77, 80) \n  \n\n C \n    [74, 77) \n  \n\n C- \n    [70, 74) \n  \n\n D+ \n    [65, 70) \n  \n\n D \n    [60, 65) \n  \n\n F \n    [0, 60)"
  },
  {
    "objectID": "slides/01-syllabus.html#homework-480-pts",
    "href": "slides/01-syllabus.html#homework-480-pts",
    "title": "Welcome Aboard! üôå",
    "section": "Homework (480 pts)",
    "text": "Homework (480 pts)\n\nHomework will be assigned through the course website in weekly schedule.\nD2L > Assessments > Dropbox and upload your homework in PDF format.\nYou must submit YOUR OWN work. üôè\n‚ùå No make-up homework.\nDue Friday 11:59 PM (09/08, 09/22, 10/06, 10/27, 11/17, 12/08. Hard deadline and no late submission).\n‚ùå Handwriting is NOT allowed for data analysis part."
  },
  {
    "objectID": "slides/01-syllabus.html#in-class-exam-160-pts",
    "href": "slides/01-syllabus.html#in-class-exam-160-pts",
    "title": "Welcome Aboard! üôå",
    "section": "In-class Exam (160 pts)",
    "text": "In-class Exam (160 pts)\n\nMidterm exam is held in class on 10/17.\nüìÑ One piece of letter size cheat sheet is allowed. It has to be turned-in with your in-class exam. (Or entirely open book?)\nExam covers materials in Week 1 to 7."
  },
  {
    "objectID": "slides/01-syllabus.html#project-300-pts",
    "href": "slides/01-syllabus.html#project-300-pts",
    "title": "Welcome Aboard! üôå",
    "section": "Project (300 pts)",
    "text": "Project (300 pts)\n\nYou will be doing a team project.\nWhat is your project about?\nWritten report or oral presentation or both?\nHow is your projected evaluated?\nMore information will be released later.\n\nanalyzing real-world data using regression methods"
  },
  {
    "objectID": "slides/01-syllabus.html#class-participation-60-pts",
    "href": "slides/01-syllabus.html#class-participation-60-pts",
    "title": "Welcome Aboard! üôå",
    "section": "Class Participation (60 pts)",
    "text": "Class Participation (60 pts)\n\nFive minute presentation on exercise problems?\nMore details will be released later."
  },
  {
    "objectID": "slides/01-syllabus.html#mssc-5780-work-200-pts",
    "href": "slides/01-syllabus.html#mssc-5780-work-200-pts",
    "title": "Welcome Aboard! üôå",
    "section": "MSSC 5780 Work (200 pts)",
    "text": "MSSC 5780 Work (200 pts)\n\nSome extra work are for MSSC 5780 students.\nüëç MATH 4780 students are encouraged to do the MSSC problems to earn extra points!"
  },
  {
    "objectID": "slides/01-syllabus.html#what-computing-language-we-use-i-teach",
    "href": "slides/01-syllabus.html#what-computing-language-we-use-i-teach",
    "title": "Welcome Aboard! üôå",
    "section": "What Computing Language We Use (I Teach)?",
    "text": "What Computing Language We Use (I Teach)?\n\nüññ The best language for statistical computing!\n\n\n#| fig-cap: ‚ÄòR Website‚Äô - R is the best language for statistical computing. - Its code for doing regression analysis is pretty clean and concise. - You won‚Äôt regret learning R."
  },
  {
    "objectID": "slides/01-syllabus.html#which-programming-language",
    "href": "slides/01-syllabus.html#which-programming-language",
    "title": "Welcome Aboard! üôå",
    "section": "Which Programming Language?",
    "text": "Which Programming Language?\n\n‚úÖ May use any other language (Python, MATLAB, etc) to do your work.\n‚ùå I would NOT debug your code or comment on your technical issues of those languages.\nIf time permitted, I‚Äôll translate R to Python."
  },
  {
    "objectID": "slides/01-syllabus.html#academic-integrity",
    "href": "slides/01-syllabus.html#academic-integrity",
    "title": "Welcome Aboard! üôå",
    "section": "Academic Integrity",
    "text": "Academic Integrity\nThis course expects all students to follow University and College statements on academic integrity.\n\n\nHonor Pledge and Honor Code: I recognize the importance of personal integrity in all aspects of life and work. I commit myself to truthfulness, honor, and responsibility, by which I earn the respect of others. I support the development of good character, and commit myself to uphold the highest standards of academic integrity as an important aspect of personal integrity. My commitment obliges me to conduct myself according to the Marquette University Honor Code.\n\n\n‚ùå You know what I am talking about. Yes, DO NOT CHEAT.\n\n\n\nmath4780-f23.github.io/website"
  },
  {
    "objectID": "slides/08-diag-normality.html#assumptions-of-linear-regression",
    "href": "slides/08-diag-normality.html#assumptions-of-linear-regression",
    "title": "Regression Diagnostics: Normality üìñ",
    "section": "Assumptions of Linear Regression",
    "text": "Assumptions of Linear Regression\n\\(Y_i= \\beta_0 + \\beta_1X_{i1} + \\beta_2X_{i2} + \\dots + \\beta_kX_{ik} + \\epsilon_i\\)\n\n\n\n\n\\(E(Y \\mid X)\\) and \\(X\\) are linearly related.\n\\(\\small E(\\epsilon_i) = 0\\)\n\\(\\small \\mathrm{Var}(\\epsilon_i) = \\sigma^2\\)\n\n\\(\\small \\mathrm{Cov}(\\epsilon_i, \\epsilon_j) = 0\\) for all \\(i \\ne j\\).\n\n\\(\\small \\epsilon_i \\stackrel{iid}{\\sim} N(0, \\sigma^2)\\) (for statistical inference)\n\n\n\n\n\n\n\n\n\n\n\n\n\nSo it‚Äôs not that uncommon to see violation of assumptions.\nToday, we are going to learn how to check whether those assumptions are valid or satisfied.\nAnd probably the following two weeks, if the assumptions are not satisfied, how do we deal with it. OK.\n\n\n\nIf the assumptions are violated, a different sample could lead to a different conclusion!\n\n\\(R^2\\) tells us how good the model is fitted to the data, but says nothing about the correctness of the model.\nAll the inferences are based on the assumption that the model is correct."
  },
  {
    "objectID": "slides/08-diag-normality.html#non-normality",
    "href": "slides/08-diag-normality.html#non-normality",
    "title": "Regression Diagnostics: Normality üìñ",
    "section": "Non-normality",
    "text": "Non-normality\n\nThe central limit theorem assures the validity of inferences based on the least-squares (LS) coefficients in all but small samples.\n\n\nWithout normality,\n\n\nHeavier tailed errors:\n\nLS estimators do not have the smallest variance among unbiased estimators.\ngive rise to outliers.\n\n\n\nGauss-Markov does not require normality.\n\n\n\n\n\nSkewed errors:\n\ntend to generate outliers in the direction of the skew.\nthe conditional mean of \\(y\\) given \\(x\\) is not a good measure of center of a highly skewed distribution.\n\n\n\n\n\n\n\nMultimodal errors:\n\nsuggest the omission of one or more categorical regressors.\n\n\n\n\n\n\\(\\epsilon_i \\stackrel{iid}{\\sim} N(0, \\sigma^2)\\): mean 0, constant variance, normally distributed, and uncorrelated.\nThe form of the model (linearity) and the specification of the predictors are correct.\nCheck model adequacy by residual plots and lack-of-fit tests.\nMethods for building models when some of the assumptions are violated.\n\ndata transformation\ngeneralized least squares (GLS) and weighted least squares (WLS)"
  },
  {
    "objectID": "slides/08-diag-normality.html#detecting-nonnormality",
    "href": "slides/08-diag-normality.html#detecting-nonnormality",
    "title": "Regression Diagnostics: Normality üìñ",
    "section": "Detecting Nonnormality",
    "text": "Detecting Nonnormality\n\n\nThe R-student residuals \\(t_i \\sim t_{n-p-1}\\) if the model assumptions are correct.\nCompare the distribution of the \\(t_i\\)s to \\(t_{n-p-1}\\) in QQ plot.\n\n\nOne way to address the assumption of normality is to compare the distribution of the R-student residuals to \\(t_{n-p-1}\\) in QQ plot."
  },
  {
    "objectID": "slides/08-diag-normality.html#qq-plot-of-r-student-residuals-comparing-t_n-p-1",
    "href": "slides/08-diag-normality.html#qq-plot-of-r-student-residuals-comparing-t_n-p-1",
    "title": "Regression Diagnostics: Normality üìñ",
    "section": "QQ plot of R-Student Residuals Comparing \\(t_{n-p-1}\\)\n",
    "text": "QQ plot of R-Student Residuals Comparing \\(t_{n-p-1}\\)\n\n\ncar::qqPlot(delivery_lm, id = TRUE, col.lines = \"blue\", \n            reps = 1000, ylab = \"Ordered R-Student Residuals\", pch = 16)\n\n\n\nThe car package calls the R-student residuals studentized residuals.\nif put lm object, car::qqPlot automatically create qqplot for R-student residual comparing with \\(t_{n-p-1}\\) distribution.\nNo severe problem in delivery data\nid = controls point identification; if FALSE, no points are identified; can be a list of named arguments to the showLabels function; TRUE is equivalent to list(method=‚Äúy‚Äù, n=2, cex=1, col=carPalette()[1], location=‚Äúlr‚Äù), which identifies the 2 points with the 2 points with the most extreme verical values ‚Äî studentized residuals for the ‚Äúlm‚Äù method."
  },
  {
    "objectID": "slides/08-diag-normality.html#density-plot-of-r-student-residuals",
    "href": "slides/08-diag-normality.html#density-plot-of-r-student-residuals",
    "title": "Regression Diagnostics: Normality üìñ",
    "section": "Density plot of R-Student Residuals",
    "text": "Density plot of R-Student Residuals\n\nrstud <- rstudent(delivery_lm)\nhist(rstud, prob = TRUE, breaks = 10, xlab = \"R-Student Residuals\", main = \"\")\nlines(density(rstud, adjust = 2), col = 4, lwd = 2)\n\n\n\nThe QQ plot draws attention to the tail behavior of the R-student residuals but is less effective in visualizing their distribution as a whole.\nCheck histogram or smooth density plot of the R-student residuals to get the general shape of the distribution.\nMay be better to add another categorical regressor\ncar::densityPlot(rstud, adjust = 2, xlab = ‚ÄúR-Student Residuals‚Äù)"
  },
  {
    "objectID": "slides/08-diag-normality.html#correcting-nonnormality",
    "href": "slides/08-diag-normality.html#correcting-nonnormality",
    "title": "Regression Diagnostics: Normality üìñ",
    "section": "Correcting Nonnormality",
    "text": "Correcting Nonnormality\n\nA response that is close to normal usually makes the assumption of normal errors more tenable.\n\nHeavier tailed errors:\n\nUse robust regression (Chap 15 in LRA)\n\n\n\nSkewed errors:\n\nTransform response data for symmetry\n\n\n\nMultimodal errors:\n\nAdd one or more relevant categorical variables.\n\n\n\n\nWant the error or the response conditional on \\(x\\)s to be like normal after correction."
  },
  {
    "objectID": "slides/08-diag-normality.html#transformation-for-symmetry",
    "href": "slides/08-diag-normality.html#transformation-for-symmetry",
    "title": "Regression Diagnostics: Normality üìñ",
    "section": "Transformation for Symmetry",
    "text": "Transformation for Symmetry\nPower transformation: \\(y \\rightarrow y^{\\lambda}\\)\n\nmake the distribution of \\(y\\) more normal, at least more symmetric.\n\n\\(y\\) can take on positive values only.\n\n\\(y \\rightarrow \\ln(y)\\) for \\(\\lambda = 0.\\)\n\n\n\nLadder of powers and roots (Tukey, 1977):\n\nNo transformation: \\(\\lambda = 1\\)\n\n\nDescending: spreads out the small values of \\(y\\) relative to the large values.\n\n\n\\(\\lambda = 1/2\\) (square root); \\(\\lambda = 0\\) (natural log); \\(\\lambda = -1\\) (inverse)\n\n\n\nAscending: spreads out the large values relative to the small values.\n\n\n\\(\\lambda = 2\\) (square); \\(\\lambda = 3\\) (cube)\n\n\n\nif \\(\\lambda\\) is descending from \\(1\\) to \\(-1\\), the fransformation increasingly spreads out the small values of \\(y\\) relative to the large values.\n\n\n\n\nThe order of \\(y\\) is reversed if \\(\\lambda < 0\\) is used for power transformation."
  },
  {
    "objectID": "slides/08-diag-normality.html#correcting-skewness-by-power-transformation",
    "href": "slides/08-diag-normality.html#correcting-skewness-by-power-transformation",
    "title": "Regression Diagnostics: Normality üìñ",
    "section": "Correcting Skewness by Power Transformation",
    "text": "Correcting Skewness by Power Transformation"
  },
  {
    "objectID": "slides/08-diag-normality.html#box-cox-transformation-on-y",
    "href": "slides/08-diag-normality.html#box-cox-transformation-on-y",
    "title": "Regression Diagnostics: Normality üìñ",
    "section": "Box-Cox Transformation on \\(y\\)\n",
    "text": "Box-Cox Transformation on \\(y\\)\n\n\n\nA modified power transformation by Box and Cox (1964):\n\\[y^{(\\lambda)} = \\begin{cases}\n    \\frac{y^{\\lambda}-1}{\\lambda},       & \\quad \\lambda \\ne 0\\\\\n    \\ln y,  & \\quad \\lambda = 0\n  \\end{cases}\\]\n\n\nFor all \\(\\lambda\\), \\(y^{(\\lambda)} = 1\\) at \\(y = 1\\).\nThe order of the transformed data \\(y^{(\\lambda)}\\) is the same as that of \\(y\\), even for \\(\\lambda < 0.\\)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe derivative w.r.t. \\(y\\) of \\(y^{(\\lambda)}\\) at \\(y = 1\\) is 1 for any \\(\\lambda\\)."
  },
  {
    "objectID": "slides/08-diag-normality.html#box-cox-transformation-on-y-choose-lambda-analytically",
    "href": "slides/08-diag-normality.html#box-cox-transformation-on-y-choose-lambda-analytically",
    "title": "Regression Diagnostics: Normality üìñ",
    "section": "Box-Cox Transformation on \\(y\\): Choose \\(\\lambda\\) Analytically",
    "text": "Box-Cox Transformation on \\(y\\): Choose \\(\\lambda\\) Analytically\n\nThe model to be fit is \\[y_i^{(\\lambda)} = \\beta_0 + \\beta_1x_{i1}+ \\cdots + \\beta_kx_{ik} + \\epsilon_i^*\\]\n\nChoose \\(\\lambda\\) so that the transformed errors \\(\\epsilon_i^*\\) look as nearly normally distributed as possible."
  },
  {
    "objectID": "slides/08-diag-normality.html#r-lab-cia-world-factbook-data",
    "href": "slides/08-diag-normality.html#r-lab-cia-world-factbook-data",
    "title": "Regression Diagnostics: Normality üìñ",
    "section": "\nR Lab CIA World Factbook Data",
    "text": "R Lab CIA World Factbook Data\n\nCIA World Facebook for 134 nations downloaded at https://github.com/thewiremonkey/factbook.csv\n\n\n\n\n\n\n\n\n\n            gdp infant gini health  region\nAlbania    11.1   12.8   34    6.0  Europe\nAlgeria    14.3   21.0   35    5.2  Africa\nArgentina  22.1    9.7   46    8.5 America\nArmenia     7.4   13.5   31    4.5  Europe\nAustralia  46.6    4.4   30    9.1 Oceania\nAustria    45.4    3.5   26   11.5  Europe\nAzerbaijan 17.9   25.7   34    5.4    Asia\nBangladesh  3.4   44.1   32    3.6    Asia\nBelarus    18.2    3.6   27    5.0  Europe\nBelgium    41.7    3.4   26   10.8  Europe\nBenin       1.9   55.7   36    4.5  Africa\nBhutan      7.7   35.9   39    3.8    Asia\n\n\n\n\n\ngdp: GDP per capita in thousands of U.S. dollars\n\ninfant: Infant mortality rate per 1000 live births\n\ngini: Gini coefficient for the distribution of family income\n\nhealth: Health expenditures as a percentage of GDP"
  },
  {
    "objectID": "slides/08-diag-normality.html#r-lab-r-student-residuals",
    "href": "slides/08-diag-normality.html#r-lab-r-student-residuals",
    "title": "Regression Diagnostics: Normality üìñ",
    "section": "\nR Lab R-Student Residuals",
    "text": "R Lab R-Student Residuals\n\nSee how gdp, health and gini affect infant.\nR-Student residuals are right-skewed, suggesting transforming the response, infant mortality, down the ladder of powers.\n\n\nciafit <- lm(infant ~ gdp + health + gini, data = CIA)\nr_stud <- rstudent(ciafit)\ncar::qqPlot(ciafit, id = FALSE)\ncar::densityPlot(r_stud)"
  },
  {
    "objectID": "slides/08-diag-normality.html#r-lab-box-cox-transformation",
    "href": "slides/08-diag-normality.html#r-lab-box-cox-transformation",
    "title": "Regression Diagnostics: Normality üìñ",
    "section": "\nR Lab Box-Cox Transformation",
    "text": "R Lab Box-Cox Transformation\n\nCodemat <- matrix(r_stud)\nfor (lam in c(0.5, 0, -0.5, -1)) {\n    refit <- update(\n        ciafit, car::bcPower(infant, lam) ~ .\n        )\n    mat <- cbind(rstudent(refit), mat)\n}\ncolnames(mat) <- c(-1, -0.5, \"log\", 0.5, 1)\nboxplot(\n    mat, id = FALSE, \n    xlab = expression(\"Powers,\" ~ lambda),\n    ylab = expression(\n        \"R-Student Residuals for \" \n        ~ Infant ^ (lambda))\n    )"
  },
  {
    "objectID": "slides/08-diag-normality.html#r-lab-choose-lambda-analytically",
    "href": "slides/08-diag-normality.html#r-lab-choose-lambda-analytically",
    "title": "Regression Diagnostics: Normality üìñ",
    "section": "\nR Lab Choose \\(\\lambda\\) Analytically",
    "text": "R Lab Choose \\(\\lambda\\) Analytically\n\nsummary(car::powerTransform(ciafit, family = \"bcPower\"))\n\nbcPower Transformation to Normality \n   Est Power Rounded Pwr Wald Lwr Bnd Wald Upr Bnd\nY1     -0.22       -0.33        -0.37       -0.072\n\nLikelihood ratio test that transformation parameter is equal to 0\n (log transformation)\n                      LRT df  pval\nLR test, lambda = (0)   8  1 0.005\n\nLikelihood ratio test that no transformation is needed\n                      LRT df   pval\nLR test, lambda = (1) 181  1 <2e-16\n\n\n\n\n\\(\\hat{\\lambda} = -0.22\\), and the \\(95\\%\\) CI for \\(\\lambda\\) is \\([-0.37, -0.07]\\).\n\n\\(\\lambda = 1\\) not in the interval, providing support for transforming response.\n\n\\(\\lambda = 0\\) (log transformation) is slightly outside the interval.\n\ncar::powerTransform() uses the maximum likelihood-like approach to select a \\(\\lambda\\) estimate, \\(\\hat{\\lambda}\\)."
  },
  {
    "objectID": "slides/08-diag-normality.html#other-issues",
    "href": "slides/08-diag-normality.html#other-issues",
    "title": "Regression Diagnostics: Normality üìñ",
    "section": "Other Issues",
    "text": "Other Issues\n\nApply power transformations to data with zero or negative values by adding a positive constant to the data to make all values positive.\n\n\n\\(\\log(y + 10)\\) if all \\(y > -10\\).\n\n\n\n\n\nPower transformations are effective when the ratio of the largest to smallest values is sufficiently large.\n\nIf \\(y_{max}/y_{min} \\approx 1\\), power transformations are nearly linear.\nIncrease the ratio by adding a negative constant, \\((y_{max} - c)/(y_{min}-c)\\)\n\n\n\n\n\n\n\nIf acceptable, choose log transformation for simple interpretation.\n\n\nlogciafit <- lm(log(infant) ~ gdp + health + gini, data = CIA)\ncoef(logciafit)\n\n(Intercept)         gdp      health        gini \n      3.016      -0.044      -0.055       0.022 \n\n\n\nAll else held constant, for one unit increase of gdp, the infant mortality rate is expected to be decreased, on average, by 4.3% because \\(\\exp(-0.044) = 0.957\\).\n\nHawkins and Weisberg (2017) - Skewed conditional distribution of \\(y\\) is not a good measure of center. + Create a model for conditional median of the response. (Quantile regression) - OLS regression of the original variable is used to to estimate the expected arithmetic mean and OLS regression of the log transformed outcome variable is to estimated the expected geometric mean of the original variable.\n\n\n\nmath4780-f23.github.io/website"
  },
  {
    "objectID": "slides/16-logistic-reg.html#regression-vs.-classification",
    "href": "slides/16-logistic-reg.html#regression-vs.-classification",
    "title": "Logistic Regression üíª",
    "section": "Regression vs.¬†Classification",
    "text": "Regression vs.¬†Classification\n\nLinear regression assumes that the response \\(Y\\) is numerical.\nIn many situations, \\(Y\\) is categorical.\n\n\n\nNormal vs.¬†COVID vs.¬†Smoking\n\n\n\n\n\n\n\n\n\nfake news vs.¬†true news\n\n\n\n\n\n\n\n\n\n\n\nA process of predicting categorical response is known as classification."
  },
  {
    "objectID": "slides/16-logistic-reg.html#regression-function-fx-vs.-classifier-cx",
    "href": "slides/16-logistic-reg.html#regression-function-fx-vs.-classifier-cx",
    "title": "Logistic Regression üíª",
    "section": "Regression Function \\(f(x)\\) vs.¬†Classifier \\(C(x)\\)\n",
    "text": "Regression Function \\(f(x)\\) vs.¬†Classifier \\(C(x)\\)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSource: https://daviddalpiaz.github.io/r4sl/classification-overview.html"
  },
  {
    "objectID": "slides/16-logistic-reg.html#classifiers",
    "href": "slides/16-logistic-reg.html#classifiers",
    "title": "Logistic Regression üíª",
    "section": "Classifiers",
    "text": "Classifiers\n\nOften, we first predict the probability of each of the categories of \\(Y\\), as a basis for making the classification (soft classifier).\nWe discuss the classifiers\n\nlogistic\nprobit\ncomplementary log-log\n\n\nOther classifiers include (MSSC 6250)\n\nK-nearest neighbors\ntrees/random forests/boosting\nsupport vector machines\nconvolutional neural networks, etc."
  },
  {
    "objectID": "slides/16-logistic-reg.html#classification-example",
    "href": "slides/16-logistic-reg.html#classification-example",
    "title": "Logistic Regression üíª",
    "section": "Classification Example",
    "text": "Classification Example\n\nPredict whether people will default on their credit card payment \\((Y)\\) yes or no, based on monthly credit card balance \\((X)\\).\nWe use the (training) sample data \\(\\{(x_1, y_1), \\dots, (x_n, y_n)\\}\\) to build a classifier."
  },
  {
    "objectID": "slides/16-logistic-reg.html#why-not-linear-regression",
    "href": "slides/16-logistic-reg.html#why-not-linear-regression",
    "title": "Logistic Regression üíª",
    "section": "Why Not Linear Regression?",
    "text": "Why Not Linear Regression?\n\\[Y =\\begin{cases}\n    0  & \\quad \\text{if not default}\\\\\n    1  & \\quad \\text{if default}\n     \\end{cases}\\]\n\n\n\\(Y = \\beta_0 + \\beta_1X + \\epsilon\\), \\(\\, X =\\) credit card balance\n\n\nWhat is the problem of this dummy variable approach?\n\n\n\n\n\n\n\n\\(Y\\) is categorical but coded as dummy variable or indicator variable\nFit linear regression and treat it as a numerical variable."
  },
  {
    "objectID": "slides/16-logistic-reg.html#why-not-linear-regression-1",
    "href": "slides/16-logistic-reg.html#why-not-linear-regression-1",
    "title": "Logistic Regression üíª",
    "section": "Why Not Linear Regression?",
    "text": "Why Not Linear Regression?\n\nSome estimates are outside \\([0, 1]\\).\n\n\n\nThe dummy variable approach \\((Y = 0, 1)\\) cannot be easily extended to \\(Y\\) with more than two categories."
  },
  {
    "objectID": "slides/16-logistic-reg.html#why-not-linear-regression-2",
    "href": "slides/16-logistic-reg.html#why-not-linear-regression-2",
    "title": "Logistic Regression üíª",
    "section": "Why Not Linear Regression?",
    "text": "Why Not Linear Regression?\n\nFirst predict the probability of each category of \\(Y\\).\nPredict probability of default using a S-shaped curve.\n\n\nOften, we first predict the probability of each of the categories of \\(Y\\), as a basis for making the classification. - The predicted probability should be like a S-shaped curve that first is in [0, 1] interval. - Second, the predicted probability of being defualted is increasing in the credit card balance."
  },
  {
    "objectID": "slides/16-logistic-reg.html#framing-the-problem-binary-responses",
    "href": "slides/16-logistic-reg.html#framing-the-problem-binary-responses",
    "title": "Logistic Regression üíª",
    "section": "Framing the Problem: Binary Responses",
    "text": "Framing the Problem: Binary Responses\n\nTreat each outcome (default \\((y = 1)\\) and not default \\((y = 0)\\)) as success and failure arising from separate Bernoulli trials.\n\n\nWhat is a Bernoulli trial?\n\n\n\nA Bernoulli trial is a special case of a binomial trial when the number of trials is \\(m = 1\\):\n\n\\(Bernoulli(\\pi) = binomial(m = 1,\\pi)\\)\n\nexactly two possible outcomes, ‚Äúsuccess‚Äù and ‚Äúfailure‚Äù\nthe probability of success \\(\\pi\\) is constant\n\n\n\n\n\n\n\nIn the credit card example,\n\ndo we have exactly two outcomes?\ndo we have constant probability? \\(P(y_1 = 1) = P(y_2 = 1) = \\cdots = P(y_n = 1) = \\pi?\\)\n\n\n\n\nThe idea is that we can treat \\(Y\\) as a categorical variable and each of its outcome is success or failure arising from separate Bernoulli trials\nAnd what is a Bernoulli trial? We talked about this when we talked about Binomial distribution, right?\nA Bernoulli trial is a random experiment with exactly two possible outcomes, ‚Äúsuccess‚Äù and ‚Äúfailure‚Äù, in which the probability of success is the same every time the experiment is conducted"
  },
  {
    "objectID": "slides/16-logistic-reg.html#binary-responses-with-nonconstant-probability",
    "href": "slides/16-logistic-reg.html#binary-responses-with-nonconstant-probability",
    "title": "Logistic Regression üíª",
    "section": "Binary Responses with Nonconstant Probability",
    "text": "Binary Responses with Nonconstant Probability\n\n\n\nTwo outcomes: default \\((y = 1)\\) and not default \\((y = 0)\\)\n\nThe probability of success \\(\\pi\\) changes with the value of predictor \\(X\\)!\nWith a different value of \\(x_i\\), each Bernoulli trial outcome \\(y_i\\) has a different probability of success \\(\\pi_i\\):\n\n\n\n\n\n\n\n\n\n\n\n\n\\(y_i \\mid x_i \\stackrel{indep}{\\sim} Bernoulli(\\pi(x_i)) = binomial(m=1,\\pi = \\pi(x_i))\\)\n\nHere, Each Bernoulli trial, with different value of \\(x_i\\), the trial outcome \\(y_i\\) can have a separate probability of success $ y_i x_i; p_i ‚àº Bern(p_i) $.\nActually this probability \\(\\pi_i\\) is affected by the predictor \\(x_i\\). A different value of \\(x\\) will give you a different value of \\(\\pi\\).\nLike linear regression, different value of \\(x\\) give us different mean of \\(Y\\)\n\n\n\n\n\\(X =\\) balance. \\(x_1 = 2000\\) has a larger \\(\\pi_1 = \\pi(2000)\\) than \\(\\pi_2 = \\pi(500)\\) with \\(x_2 = 500\\) because credit cards with a higher balance tend to be default."
  },
  {
    "objectID": "slides/16-logistic-reg.html#logistic-regression-1",
    "href": "slides/16-logistic-reg.html#logistic-regression-1",
    "title": "Logistic Regression üíª",
    "section": "Logistic Regression",
    "text": "Logistic Regression\n\n\nLogistic regression models a binary response \\((Y)\\) using predictors \\(X_1, \\dots, X_k\\).\n\n\n\\(k = 1\\): simple logistic regression\n\n\\(k > 1\\): multiple logistic regression\n\n\n\n\nLogistic regression models a binary categorical outcome \\((Y)\\) using numerical and categorical predictors \\(X_1, \\dots, X_k\\).\n\n\nInstead of predicting \\(y_i\\) directly, we use the predictors to model its probability of success, \\(\\pi_i\\).\n\n\nBut how?\n\n\nBut remember, we are not predicting \\(Y\\) directly. Instead, our goal is to use predictors \\(X_1, \\dots, X_k\\) to estimate the probability of success \\(\\pi\\) of the Bernoulli variable \\(Y\\). And if \\(\\pi > threshold\\), say 0.5, \\(\\hat{Y} = 1\\), if \\(\\pi < threshold\\), \\(\\hat{Y} = 0\\).\n\n\n\n\n\n\n\n\n\nTransform \\(\\pi \\in (0, 1)\\) into another variable \\(\\eta \\in (-\\infty, \\infty)\\). Then construct a linear predictor on \\(\\eta\\): \\(\\eta_i = \\beta_0 + \\beta_1x_i\\) \n\n\n\nAnd the idea is that we transform \\(p \\in (0, 1)\\) into another variable \\(\\eta \\in (-\\infty, \\infty)\\). So that we can reasonably fit a linear regression on \\(\\eta\\). \n\n\n\n\n\n\nLogit function: For \\(0 < \\pi < 1\\)\n\n\n\\[\\eta = logit(\\pi) = \\ln\\left(\\frac{\\pi}{1-\\pi}\\right)\\]\n\nAnd the function that transforms \\(p\\) into \\(\\eta\\) is the so called logit function defined as\n\n\\(\\eta = logit(p) = \\ln\\left(\\frac{p}{1-p}\\right)\\) \n\nYou see when \\(p\\) is approaching 0, p/1-p is also approaching 0, and so log of it is approaching -\\(\\infty\\).\nWhen p is close to 1, p/1-p is close to \\(\\infty\\), so is log because log is an increasing function."
  },
  {
    "objectID": "slides/16-logistic-reg.html#logit-function-eta-logitpi-lnleftfracpi1-piright",
    "href": "slides/16-logistic-reg.html#logit-function-eta-logitpi-lnleftfracpi1-piright",
    "title": "Logistic Regression üíª",
    "section": "Logit function \\(\\eta = logit(\\pi) = \\ln\\left(\\frac{\\pi}{1-\\pi}\\right)\\)\n",
    "text": "Logit function \\(\\eta = logit(\\pi) = \\ln\\left(\\frac{\\pi}{1-\\pi}\\right)\\)\n\n\n\nHere visualizes the logit function.\nIt‚Äôs an one-to-one increasing function of \\(\\pi\\) and so the inference on \\(\\eta\\) can be transformed back to the inference of \\(\\pi\\) with no problems."
  },
  {
    "objectID": "slides/16-logistic-reg.html#logistic-function",
    "href": "slides/16-logistic-reg.html#logistic-function",
    "title": "Logistic Regression üíª",
    "section": "Logistic Function",
    "text": "Logistic Function\n\nThe logit function \\(\\eta = logit(\\pi) = \\ln\\left(\\frac{\\pi}{1-\\pi}\\right)\\) takes a value \\(\\pi \\in (0, 1)\\) and maps it to a value \\(\\eta \\in (-\\infty, \\infty)\\).\n\nLogistic function: \\[\\pi = logistic(\\eta) = \\frac{\\exp(\\eta)}{1+\\exp(\\eta)} = \\frac{1}{1+\\exp(-\\eta)} \\in (0, 1)\\]\n\nThe logistic function takes a value \\(\\eta \\in (-\\infty, \\infty)\\) and maps it to a value \\(\\pi \\in (0, 1)\\).\n\n\n\nSo once \\(\\eta\\) is estimated by the linear regression, we use the logistic function to transform \\(\\eta\\) back to the probability.\n\n\nThe logit function \\(\\eta = logit(\\pi) = \\ln\\left(\\frac{\\pi}{1-\\pi}\\right)\\) takes a value \\(\\pi \\in (0, 1)\\) and maps it to a value \\(\\eta \\in (-\\infty, \\infty)\\).\n\nInverse logit (logistic) function: \\[\\pi = logistic(\\eta) = \\frac{\\exp(\\eta)}{1+\\exp(\\eta)} = \\frac{1}{1+\\exp(-\\eta)} \\in (0, 1)\\]\n\nThe inverse logit function takes a value \\(\\eta\\) between \\(-\\infty\\) and \\(\\infty\\) and maps it to a value \\(\\pi\\) between 0 and 1."
  },
  {
    "objectID": "slides/16-logistic-reg.html#logistic-function-pi-logisticeta-fracexpeta1expeta",
    "href": "slides/16-logistic-reg.html#logistic-function-pi-logisticeta-fracexpeta1expeta",
    "title": "Logistic Regression üíª",
    "section": "Logistic Function \\(\\pi = logistic(\\eta) = \\frac{\\exp(\\eta)}{1+\\exp(\\eta)}\\)\n",
    "text": "Logistic Function \\(\\pi = logistic(\\eta) = \\frac{\\exp(\\eta)}{1+\\exp(\\eta)}\\)\n\n\n\nWe are almost there. The value of logistic function is what we need for predicting the probability that \\(Y = 1\\).\nGiven any value of \\(\\eta\\), there is a corresponding estimated probability.\nSo if we can use our predictors to get eta first, then we can use the eta to predict the probability of \\(Y\\) being equal to when the predictors are at the level for getting \\(\\eta\\)."
  },
  {
    "objectID": "slides/16-logistic-reg.html#simple-logistic-regression-model",
    "href": "slides/16-logistic-reg.html#simple-logistic-regression-model",
    "title": "Logistic Regression üíª",
    "section": "Simple Logistic Regression Model",
    "text": "Simple Logistic Regression Model\nFor \\(i = 1, \\dots, n\\) and with one predictor \\(X\\): \\[(Y_i \\mid X = x_i) \\stackrel{indep}{\\sim} Bernoulli(\\pi(x_i))\\] \\[\\text{logit}(\\pi_i) = \\ln \\left( \\frac{\\pi(x_i)}{1 - \\pi(x_i)} \\right) = \\eta_i = \\beta_0+\\beta_1 x_{i}\\] \n\nwith sample size \\(n\\) and with \\(k\\) predictors, we have the logistic regression model like this\nFirst, we have a probability distribution Bernoulli describing how the outcome or response data are generated.\n\n\n\\(Y_i \\mid {\\bf x}_i; \\pi_i \\sim \\text{Bern}(p_i)\\), \\({\\bf x}_i = (x_{1,i}, \\cdots, x_{k,i})\\), \\(i = 1, \\dots, n\\)\n\n\n\nThen we have a link function, logit function, that relates the linear regression to the parameter of the outcome distribution, which is the parameter \\(p\\), the probability of success in the Bernoulli distribution.\n\n\\(\\text{logit}(\\pi_i) = \\eta_i = \\beta_0+\\beta_1 x_{1,i} + \\cdots + \\beta_k x_{k,i}\\)\n\n\n\n\nOnce we get the estimates \\(\\hat{\\beta}_0\\) and \\(\\hat{\\beta}_1\\), \\[\\small \\hat{\\pi}_i = \\frac{\\exp(\\hat{\\beta}_0+\\hat{\\beta}_1 x_{i} )}{1+\\exp(\\hat{\\beta}_0+\\hat{\\beta}_1 x_{i})} = \\frac{1}{1+\\exp(-\\hat{\\beta}_0-\\hat{\\beta}_1 x_{i}))}\\]\n.alert[ In general, if \\(E(Y_i) = \\mu_i\\), \\(g(\\mu_i) = \\eta_i = {\\bf x}_i'\\boldsymbol \\beta\\), \\(\\mu_i= g^{-1}(\\eta_i) = g^{-1}({\\bf x}_i'\\boldsymbol \\beta)\\). Here \\(\\mu_i = \\pi_i\\), \\(g(\\cdot) = \\text{logit}(\\cdot)\\).] - From which we get the probability of success derived by the logistic function \\[E(y_i) = \\pi_i = \\frac{\\exp(\\beta_0+\\beta_1 x_{1,i} + \\cdots + \\beta_k x_{k,i})}{1+\\exp(\\beta_0+\\beta_1 x_{1,i} + \\cdots + \\beta_k x_{k,i})}\\] or the predicted probability is the one that replaces \\(\\beta\\) with estimates \\(b\\). \\[\\hat{\\pi}_i = \\frac{\\exp(\\hat{\\beta}_0+\\hat{\\beta}_1 x_{1,i} + \\cdots + \\hat{\\beta}_k x_{k,i})}{1+\\exp(\\hat{\\beta}_0+\\hat{\\beta}_1 x_{1,i} + \\cdots + \\hat{\\beta}_k x_{k,i})}\\]"
  },
  {
    "objectID": "slides/16-logistic-reg.html#r-lab-credit-card-default",
    "href": "slides/16-logistic-reg.html#r-lab-credit-card-default",
    "title": "Logistic Regression üíª",
    "section": "\nR Lab Credit Card Default",
    "text": "R Lab Credit Card Default\n\n\n\n\nlibrary(ISLR)\ndata(\"Default\")  ## load the dataset\nhead(Default, 10)\n\n   default student balance income\n1       No      No     730  44362\n2       No     Yes     817  12106\n3       No      No    1074  31767\n4       No      No     529  35704\n5       No      No     786  38463\n6       No     Yes     920   7492\n7       No      No     826  24905\n8       No     Yes     809  17600\n9       No      No    1161  37469\n10      No      No       0  29275\n\nstr(Default)\n\n'data.frame':   10000 obs. of  4 variables:\n $ default: Factor w/ 2 levels \"No\",\"Yes\": 1 1 1 1 1 1 1 1 1 1 ...\n $ student: Factor w/ 2 levels \"No\",\"Yes\": 1 2 1 1 1 2 1 2 1 1 ...\n $ balance: num  730 817 1074 529 786 ...\n $ income : num  44362 12106 31767 35704 38463 ...\n\n\n\n\ntable(Default$default)\n\n\n  No  Yes \n9667  333 \n\ntable(Default$student)\n\n\n  No  Yes \n7056 2944 \n\nsummary(Default$balance)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n      0     482     824     835    1166    2654 \n\nsummary(Default$income)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n    772   21340   34553   33517   43808   73554"
  },
  {
    "objectID": "slides/16-logistic-reg.html#r-lab-simple-logistic-regression",
    "href": "slides/16-logistic-reg.html#r-lab-simple-logistic-regression",
    "title": "Logistic Regression üíª",
    "section": "\nR Lab Simple Logistic Regression",
    "text": "R Lab Simple Logistic Regression\n\nlibrary(tidyverse)\nDefault |>  \n    group_by(default) |>\n    summarise(avg_balance = mean(balance))\n\n# A tibble: 2 √ó 2\n  default avg_balance\n  <fct>         <dbl>\n1 No             804.\n2 Yes           1748.\n\n\n\n\nlogit_fit <- glm(default ~ balance, data = Default, family = binomial)\nsumm_logit_fit <- summary(logit_fit)\nsumm_logit_fit$coefficients\n\n            Estimate Std. Error z value  Pr(>|z|)\n(Intercept) -10.6513    0.36116   -29.5 3.62e-191\nbalance       0.0055    0.00022    25.0 1.98e-137\n\n\n\n\\(\\hat{\\eta} = \\text{logit}(\\hat{\\pi}) = \\ln \\left( \\frac{\\hat{\\pi}}{1 - \\hat{\\pi}}\\right) = -10.651 + 0.0055 \\times \\text{balance}\\)"
  },
  {
    "objectID": "slides/16-logistic-reg.html#eta-vs.-x",
    "href": "slides/16-logistic-reg.html#eta-vs.-x",
    "title": "Logistic Regression üíª",
    "section": "\n\\(\\eta\\) vs.¬†\\(x\\)\n",
    "text": "\\(\\eta\\) vs.¬†\\(x\\)\n\n\n\\(\\hat{\\eta} = \\text{logit}(\\hat{\\pi}) = \\ln \\left( \\frac{\\hat{\\pi}}{1 - \\hat{\\pi}}\\right) = -10.651 + 0.0055 \\times \\text{balance}\\)"
  },
  {
    "objectID": "slides/16-logistic-reg.html#interpretation-of-coefficients",
    "href": "slides/16-logistic-reg.html#interpretation-of-coefficients",
    "title": "Logistic Regression üíª",
    "section": "Interpretation of Coefficients",
    "text": "Interpretation of Coefficients\nThe ratio \\(\\frac{\\pi}{1-\\pi} \\in (0, \\infty)\\) is called the odds of some event.\n\nExample: If 1 in 5 people will default, the odds is 1/4 since \\(\\pi = 0.2\\) implies an odds of \\(0.2/(1‚àí0.2) = 1/4\\).\n\n\\[\\ln \\left( \\frac{\\pi(x)}{1 - \\pi(x)} \\right)= \\beta_0 + \\beta_1x\\] - Increasing \\(x\\) by one unit changes the log-odds by \\(\\beta_1\\), or it multiplies the odds by \\(e^{\\beta_1}\\).\n\n\n\n\\(\\beta_1\\) does not correspond to the change in \\(\\pi(x)\\) associated with a one-unit increase in \\(x\\).\n\n\\(\\beta_1\\) is the change in log odds associated with one-unit increase in \\(x\\)."
  },
  {
    "objectID": "slides/16-logistic-reg.html#r-lab-interpretation-of-coefficients",
    "href": "slides/16-logistic-reg.html#r-lab-interpretation-of-coefficients",
    "title": "Logistic Regression üíª",
    "section": "\nR Lab Interpretation of Coefficients",
    "text": "R Lab Interpretation of Coefficients\n\n\n            Estimate Std. Error z value  Pr(>|z|)\n(Intercept) -10.6513    0.36116   -29.5 3.62e-191\nbalance       0.0055    0.00022    25.0 1.98e-137\n\n\n\n\\(\\hat{\\eta} = \\text{logit}(\\hat{\\pi}) = \\ln \\left( \\frac{\\hat{\\pi}}{1 - \\hat{\\pi}}\\right) = -10.651 + 0.0055 \\times \\text{balance}\\)\n\n\n\n\\(\\hat{\\eta}(x) = \\hat{\\beta}_0 + \\hat{\\beta}_1x\\)\n\\(\\hat{\\eta}(x+1) = \\hat{\\beta}_0 + \\hat{\\beta}_1(x+1)\\)\n\\(\\hat{\\eta}(x+1) - \\hat{\\eta}(x) = \\hat{\\beta}_1 = \\ln(\\text{odds}_{x+1}) - \\ln(\\text{odds}_{x})\\)\nOne-unit increase in balance increases the log odds of default by 0.0055 units.\n\n\n\n\nThe odds ratio, \\(\\widehat{OR} = \\frac{\\text{odds}_{x+1}}{\\text{odds}_{x}} = e^{\\hat{\\beta}_1} = e^{0.0055} = 1.005515\\).\nThe odds of default increases by 0.55% with additional one unit of credit card balance."
  },
  {
    "objectID": "slides/16-logistic-reg.html#probability-curve",
    "href": "slides/16-logistic-reg.html#probability-curve",
    "title": "Logistic Regression üíª",
    "section": "Probability Curve",
    "text": "Probability Curve\n\n\n\nThe relationship between \\(\\pi(x)\\) and \\(x\\) is not linear! \\[\\pi(x) = \\frac{\\exp(\\beta_0+\\beta_1 x)}{1+\\exp(\\beta_0+\\beta_1 x)}\\]\n\nThe amount that \\(\\pi(x)\\) changes due to a one-unit change in \\(x\\) depends on the current value of \\(x\\).\nRegardless of the value of \\(x\\), if \\(\\beta_1 > 0\\), increasing \\(x\\) will be increasing \\(\\pi(x)\\)."
  },
  {
    "objectID": "slides/16-logistic-reg.html#prdefault-when-balance-is-2000",
    "href": "slides/16-logistic-reg.html#prdefault-when-balance-is-2000",
    "title": "Logistic Regression üíª",
    "section": "Pr(default) When Balance is 2000",
    "text": "Pr(default) When Balance is 2000\n\\[\\log\\left(\\frac{\\hat{\\pi}}{1-\\hat{\\pi}}\\right) = -10.651+0.0055\\times 2000\\]   \n\\[ \\hat{\\pi} = \\frac{1}{1+\\exp(-(-10.651+0.0055 \\times 2000)} = 0.586\\]\n\n\npi_hat <- predict(logit_fit, type = \"response\")\neta_hat <- predict(logit_fit, type = \"link\")  ## default gives us b0 + b1*x\npredict(logit_fit, newdata = data.frame(balance = 2000), type = \"response\")\n\n    1 \n0.586"
  },
  {
    "objectID": "slides/16-logistic-reg.html#probability-curve-1",
    "href": "slides/16-logistic-reg.html#probability-curve-1",
    "title": "Logistic Regression üíª",
    "section": "Probability Curve",
    "text": "Probability Curve\n\nWhat is the probability of default when the balance is 500? What about balance 2500?\n\n\n\n\n\n\n\n\n\n\n\n\n\n500 balance: Pr(default) = 0\n2000 balance, Pr(default) = 0.59\n2500 balance, Pr(default) = 0.96"
  },
  {
    "objectID": "slides/16-logistic-reg.html#multiple-logistic-regression-model",
    "href": "slides/16-logistic-reg.html#multiple-logistic-regression-model",
    "title": "Logistic Regression üíª",
    "section": "Multiple Logistic Regression Model",
    "text": "Multiple Logistic Regression Model\nFor \\(i = 1, \\dots, n\\) and with \\(k\\) predictors: \\[Y_i \\mid \\pi_i({\\bf x}_i) \\stackrel{indep}{\\sim} \\text{Bernoulli}(\\pi_i), \\quad {\\bf x}_i' = (x_{i1}, \\dots, x_{ik})\\] \\[\\text{logit}(\\pi_i) = \\ln \\left( \\frac{\\pi_i}{1 - \\pi_i} \\right) = \\eta_i = \\beta_0+\\beta_1 x_{i1} + \\cdots + \\beta_k x_{ik} = {\\bf x}_i'\\boldsymbol \\beta\\]\n\nThe \\(\\text{logit}(\\pi_i)\\) is a link function that links the linear predictor and the mean of \\(Y_i\\).\n\n\nwith sample size \\(n\\) and with \\(k\\) predictors, we have the logistic regression model like this\nFirst, we have a probability distribution Bernoulli describing how the outcome or response data are generated.\n\n\n\\(Y_i \\mid {\\bf x}_i; \\pi_i \\sim \\text{Bern}(p_i)\\), \\({\\bf x}_i = (x_{1,i}, \\cdots, x_{k,i})\\), \\(i = 1, \\dots, n\\)\n\n\n\nThen we have a link function, logit function, that relates the linear regression to the parameter of the outcome distribution, which is the parameter \\(p\\), the probability of success in the Bernoulli distribution.\n\\(\\text{logit}(\\pi_i) = \\eta_i = \\beta_0+\\beta_1 x_{1,i} + \\cdots + \\beta_k x_{k,i}\\)\n\n\\[\\small E(Y_i) = \\pi_i = \\frac{\\exp(\\eta_i)}{1 + \\exp(\\eta_i)} = \\frac{\\exp(\\beta_0+\\beta_1 x_{i1} + \\cdots + \\beta_k x_{ik})}{1+\\exp(\\beta_0+\\beta_1 x_{i1} + \\cdots + \\beta_k x_{ik})} = \\frac{\\exp( {\\bf x}_i'\\boldsymbol \\beta)}{1 + \\exp({\\bf x}_i'\\boldsymbol \\beta)}\\] \\[\\small \\hat{\\pi}_i = \\frac{\\exp(\\hat{\\beta}_0+\\hat{\\beta}_1 x_{i1} + \\cdots + \\hat{\\beta}_k x_{ik})}{1+\\exp(\\hat{\\beta}_0+\\hat{\\beta}_1 x_{i1} + \\cdots + \\hat{\\beta}_k x_{ik})}\\]\n\nFrom which we get the probability of success derived by the logistic function \\[E(y_i) = \\pi_i = \\frac{\\exp(\\beta_0+\\beta_1 x_{1,i} + \\cdots + \\beta_k x_{k,i})}{1+\\exp(\\beta_0+\\beta_1 x_{1,i} + \\cdots + \\beta_k x_{k,i})}\\] or the predicted probability is the one that replaces \\(\\beta\\) with estimates \\(b\\). \\[\\hat{\\pi}_i = \\frac{\\exp(\\hat{\\beta}_0+\\hat{\\beta}_1 x_{1,i} + \\cdots + \\hat{\\beta}_k x_{k,i})}{1+\\exp(\\hat{\\beta}_0+\\hat{\\beta}_1 x_{1,i} + \\cdots + \\hat{\\beta}_k x_{k,i})}\\]\n\n\nIn general, if \\(E(Y_i) = \\mu_i\\), \\(g(\\mu_i) = \\eta_i = {\\bf x}_i'\\boldsymbol \\beta\\), \\(\\mu_i= g^{-1}(\\eta_i) = g^{-1}({\\bf x}_i'\\boldsymbol \\beta)\\). Here \\(\\mu_i = \\pi_i\\), \\(g(\\cdot) = \\text{logit}(\\cdot)\\)."
  },
  {
    "objectID": "slides/16-logistic-reg.html#r-lab-multiple-logistic-regression",
    "href": "slides/16-logistic-reg.html#r-lab-multiple-logistic-regression",
    "title": "Logistic Regression üíª",
    "section": "\nR Lab Multiple Logistic Regression",
    "text": "R Lab Multiple Logistic Regression\n\nmulti_logit_fit <- glm(default ~ balance + I(income/1000), data = Default, \n                       family = binomial)\nsumm_multi_logit_fit <- summary(multi_logit_fit)\nsumm_multi_logit_fit$coefficients\n\n                Estimate Std. Error z value  Pr(>|z|)\n(Intercept)    -11.54047   0.434756  -26.54 2.96e-155\nbalance          0.00565   0.000227   24.84 3.64e-136\nI(income/1000)   0.02081   0.004985    4.17  2.99e-05\n\n\n\n\\(\\hat{\\eta} = \\text{logit}(\\hat{\\pi}) = \\ln \\left( \\frac{\\hat{\\pi}}{1 - \\hat{\\pi}}\\right) = -11.54 + 0.0056 \\times \\text{balance} + 0.021 \\times \\text{income}\\)\nOne with a credit card balance of $1,500 and an income of $40,000 has an estimated probability of default of \\[\\hat{\\pi} = \\frac{1}{1+ \\exp(-(-11.54 + 0.0056(1500) + 0.021(40)))} = 0.091\\]\n\n\nWhy we multiply the income coefficient by 40, rather than 40,000?"
  },
  {
    "objectID": "slides/16-logistic-reg.html#multiple-binary-outcomes",
    "href": "slides/16-logistic-reg.html#multiple-binary-outcomes",
    "title": "Logistic Regression üíª",
    "section": "Multiple Binary Outcomes",
    "text": "Multiple Binary Outcomes\n\nSo far we consider only one binary outcome for each combination of predictors.\nOften we have repeated observations or trials at each level of the regressors.\nOriginally, at \\(X = x_i\\), we have one single observation \\(y_i = 0\\) or \\(1\\).\nNow, at \\(X = x_i\\), we have \\(m_i\\) trials and \\(y_i\\) of them are ones (successes).\nLet \\(y_{i,j}\\) be an indicator (Bernoulli) variable taking value \\(0\\) or \\(1\\) for the \\(j\\)-th trial at \\(x_i\\).\n\\(y_i = y_{i,1} + y_{i,2} + \\cdots + y_{i, m_i} = \\sum_{j=1}^{m_i} y_{i,j}\\)."
  },
  {
    "objectID": "slides/16-logistic-reg.html#bernoulli-to-binomial",
    "href": "slides/16-logistic-reg.html#bernoulli-to-binomial",
    "title": "Logistic Regression üíª",
    "section": "Bernoulli to binomial",
    "text": "Bernoulli to binomial\n\nExample:\n\n4 dosages of a combination of drugs, \\((10, 15, 20, 25)\\)\n\n10 patients for each dosage\nsee how dosage level affects the number of cure of some disease among patients\n\n\n\n\\(i = 1, 2, 3, 4\\), \\(x_1 = 10, \\dots, x_4 = 25\\), \\(n = 4\\)\n\n\n\\(m_i = 10\\) for each \\(i\\)\n\n\n\\(y_i = y_{i, 1} + \\cdots + y_{i, 10}\\) is the number of patients whose disease is cured at \\(i\\)-th dosage, where \\(y_{i, j} = 1\\) if \\(j\\)-th patient is cured, \\(0\\) otherwise.\n\n\nWhat is our response and any distribution can be used to model that?\n\n\nSo far we consider only one binary outcome for each combination of predictors, and the response is Bernoulli distributed.\nOften we have more than one or repeated observations or trials at each level of the \\(x\\) variables.\nOriginally, at \\(X = x_i\\), we have one single observation \\(y_i = 0\\) or \\(1\\). Now, at \\(X = x_i\\), we have \\(m_i\\) trials and \\(s_i\\) of them are ones (successes).\n\nRegressors are dosages of a combination of drugs and a success represents a cure of a disease."
  },
  {
    "objectID": "slides/16-logistic-reg.html#binomial-responses",
    "href": "slides/16-logistic-reg.html#binomial-responses",
    "title": "Logistic Regression üíª",
    "section": "Binomial Responses",
    "text": "Binomial Responses\n\nThe responses \\(Y_i \\sim binomial(m_i, \\pi_i)\\).\nAssume \\(Y_1, Y_2, \\dots, Y_n\\) are independent. ( \\(m_i\\) trials, \\(y_{i, 1}, \\dots, y_{i, m_i}\\), are independent too by the definition of a binomial experiment )\n\n\n\n\nNumber of trials\nNumber of successes\nRegressors\n\n\n\n\\(m_1\\)\n\\(y_1\\)\n\\(x_{11}, x_{12}, \\dots, x_{1k}\\)\n\n\n\\(m_2\\)\n\\(y_2\\)\n\\(x_{21}, x_{22}, \\dots, x_{2k}\\)\n\n\n\\(\\vdots\\)\n\\(\\vdots\\)\n\\(\\vdots\\)\n\n\n\\(m_n\\)\n\\(y_n\\)\n\\(x_{n1}, x_{n2}, \\dots, x_{nk}\\)\n\n\n\n\n\\(Pr(Y_i = y_i) = {m_i \\choose y_i} \\pi_i^{y_i} (1-\\pi_i)^{m_i - y_i}\\)\n\\(E[Y_i] = m_i\\pi_i\\)\n\\(\\mathrm{Var}[Y_i] = m_i\\pi_i(1-\\pi_i)\\)"
  },
  {
    "objectID": "slides/16-logistic-reg.html#r-lab-strength-of-fastener-lra-13.3",
    "href": "slides/16-logistic-reg.html#r-lab-strength-of-fastener-lra-13.3",
    "title": "Logistic Regression üíª",
    "section": "\nR Lab Strength of Fastener (LRA 13.3)",
    "text": "R Lab Strength of Fastener (LRA 13.3)\n\nThe compressive strength of an alloy fastener used in aircraft construction is being studied.\nTen loads were selected over the range 2500 ‚Äì 4300 psi and a number of fasteners were tested at those loads.\nThe numbers of fasteners failing at each load were recorded.\n\n\nfastener <- read.csv(\"./data/data-prob-13-3.csv\")\ncolnames(fastener) <- c(\"load\", \"m\", \"y\")\nfastener\n\n   load   m  y\n1  2500  50 10\n2  2700  70 17\n3  2900 100 30\n4  3100  60 21\n5  3300  40 18\n6  3500  85 43\n7  3700  90 54\n8  3900  50 33\n9  4100  80 60\n10 4300  65 51"
  },
  {
    "objectID": "slides/16-logistic-reg.html#r-lab-binomial-response-logistic-regression",
    "href": "slides/16-logistic-reg.html#r-lab-binomial-response-logistic-regression",
    "title": "Logistic Regression üíª",
    "section": "\nR Lab Binomial Response Logistic Regression",
    "text": "R Lab Binomial Response Logistic Regression\n\nbinom_fit <- glm(cbind(y, m - y) ~ load,  #<<\n                 data = fastener, family = binomial)\nbinom_summ <- summary(binom_fit)\nbinom_summ$coef\n\n            Estimate Std. Error z value Pr(>|z|)\n(Intercept) -5.33971   0.545693   -9.79 1.30e-22\nload         0.00155   0.000158    9.83 8.45e-23\n\n\n\\[\\hat{\\pi} = \\frac{e^{-5.34 + 0.0015x}}{1 + e^{-5.34 + 0.0015x}}\\]\nThe complete test data are shown below."
  },
  {
    "objectID": "slides/16-logistic-reg.html#sensitivity-and-specificity",
    "href": "slides/16-logistic-reg.html#sensitivity-and-specificity",
    "title": "Logistic Regression üíª",
    "section": "Sensitivity and Specificity",
    "text": "Sensitivity and Specificity\n\n\n\n\n\n\n\n\n0\n1\n\n\n\nLabeled 0\nTrue Negative (TN)\nFalse Negative (FN)\n\n\nLabeled 1\nFalse Positive (FP)\nTrue Positive (TP)\n\n\n\n\n\nSensitivity (True Positive Rate) \\(= P( \\text{Labeled 1} \\mid \\text{1}) = \\frac{TP}{TP+FN}\\)\nSpecificity (True Negative Rate) \\(= P( \\text{Labeled 0} \\mid \\text{0}) = \\frac{TN}{FP+TN}\\)\nAccuracy \\(= \\frac{TP + TN}{TP+FN+FP+TN}\\) \nMore on Wiki page"
  },
  {
    "objectID": "slides/16-logistic-reg.html#r-lab-confusion-matrix",
    "href": "slides/16-logistic-reg.html#r-lab-confusion-matrix",
    "title": "Logistic Regression üíª",
    "section": "\nR Lab Confusion Matrix",
    "text": "R Lab Confusion Matrix\n\npred_prob <- predict(logit_fit, type = \"response\")\ntable(pred_prob > 0.5, Default$default)\n\n       \n          No  Yes\n  FALSE 9625  233\n  TRUE    42  100\n\n\n\nPackages:\n\n\ncaret package (Classification And REgression Training)\n\nyardstick of tidymodels\n\n\n\n\n\ncaret::confusionMatrix()\nyardstick::conf_mat()\n\n\n\ncaret package (Classification And REgression Training) provides tools for predictive modeling.\n\n\n## Classification And REgression Training\nlibrary(caret) \ncaret::confusionMatrix(data = a factor of predicted classes, \n                       reference = a factor of classes to be used as the true results)\n\n\n\nyardstick is a package of tidymodels for estimating how well models are working.\n\n\nyardstick::conf_mat(data = a data frame, \n                    truth = true class column that is a factor,\n                    estimate = predicted class column that is a factor)"
  },
  {
    "objectID": "slides/16-logistic-reg.html#r-lab-receiver-operating-characteristic-roc-curve",
    "href": "slides/16-logistic-reg.html#r-lab-receiver-operating-characteristic-roc-curve",
    "title": "Logistic Regression üíª",
    "section": "\nR Lab Receiver Operating Characteristic (ROC) Curve",
    "text": "R Lab Receiver Operating Characteristic (ROC) Curve\n\n\nReceiver operating characteristic (ROC) curve plots True Positive Rate (Sensitivity) vs.¬†False Positive Rate (1 - Specificity)\nPackages: ROCR, pROC, yardstick::roc_curve()\n\n\n\n\n\nlibrary(ROCR)\n\n# create an object of class prediction \npred <- ROCR::prediction(\n    predictions = pred_prob, \n    labels = Default$default)\n\n# calculates the ROC curve\nroc <- ROCR::performance(\n    prediction.obj = pred, \n    measure = \"tpr\",\n    x.measure = \"fpr\")\n\n\n\nplot(roc, colorize = TRUE)\n\n\n\n\n\n\n\n\n\nhttps://rviews.rstudio.com/2019/03/01/some-r-packages-for-roc-curves/ .footnote[ .small[ +Originally developed for operators of military radar receivers, hence the name.]]"
  },
  {
    "objectID": "slides/16-logistic-reg.html#r-lab-area-under-curve-auc",
    "href": "slides/16-logistic-reg.html#r-lab-area-under-curve-auc",
    "title": "Logistic Regression üíª",
    "section": "\nR Lab Area Under Curve (AUC)",
    "text": "R Lab Area Under Curve (AUC)\nFind the area under the curve:\n\n\n\n## object of class 'performance'\nauc <- ROCR::performance(\n    prediction.obj = pred, \n    measure = \"auc\")\nauc@y.values\n\n[[1]]\n[1] 0.948"
  },
  {
    "objectID": "slides/16-logistic-reg.html#r-lab-roc-curve-comparison",
    "href": "slides/16-logistic-reg.html#r-lab-roc-curve-comparison",
    "title": "Logistic Regression üíª",
    "section": "\nR Lab ROC Curve Comparison",
    "text": "R Lab ROC Curve Comparison\n\nWhich model performs better?\n\n\nRemember! Compare the candidates using the test data.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nmath4780-f23.github.io/website"
  },
  {
    "objectID": "slides/05-mlr.html#why-multiple-regression",
    "href": "slides/05-mlr.html#why-multiple-regression",
    "title": "Multiple Linear Regression üë©‚Äçüíª",
    "section": "Why Multiple Regression?",
    "text": "Why Multiple Regression?\n\nOur target response may be affected by several factors.\nTotal sales \\((Y)\\) and amount of money spent on advertising on YouTube (YT) \\((X_1)\\), Facebook (FB) \\((X_2)\\), Instagram (IG) \\((X_3)\\).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPredict sales based on the three advertising expenditures and see which medium is more effective.\n\n\nThe first question you may ask is Why we want Multiple Regression?\nIn practice, we often have more than one predictor.\nHow amount of money spent on advertising on different media affect the total sales of some product, we may need more than one predictors.\nBecause usually company will spend money on several different media, not just one.\nData: total sales \\((Y)\\) and amount of money spent on advertising on TV \\((X_1)\\), YouTube \\((X_2)\\), and Instagram \\((X_3)\\).\nWe want to predict sales based on the three advertising expenditures and see which medium is more effective."
  },
  {
    "objectID": "slides/05-mlr.html#fit-separate-simple-linear-regression-models",
    "href": "slides/05-mlr.html#fit-separate-simple-linear-regression-models",
    "title": "Multiple Linear Regression üë©‚Äçüíª",
    "section": "Fit Separate Simple Linear Regression Models",
    "text": "Fit Separate Simple Linear Regression Models\nFit three separate independent SLR models:\n\n\n‚ùå Fitting a separate SLR model for each predictor is not satisfactory.\n\nYou may wonder, how about we just fit three Separate Simple Linear Regression Models, one for each predictor.\nYes, we could do that. And if we do this, we‚Äôll see that advertising on the 3 media is valuable because the more the money we put in, the higher sales of products we‚Äôll get.\nFitting a separate SLR model for each predictor is not satisfactory. That‚Äôs see why."
  },
  {
    "objectID": "slides/05-mlr.html#dont-fit-a-separate-simple-linear-regression",
    "href": "slides/05-mlr.html#dont-fit-a-separate-simple-linear-regression",
    "title": "Multiple Linear Regression üë©‚Äçüíª",
    "section": "Don‚Äôt Fit a Separate Simple Linear Regression",
    "text": "Don‚Äôt Fit a Separate Simple Linear Regression\n\nüëâ How to make a single prediction of sales given levels of the 3 advertising media budgets?\n\n How to predict the sales when the amount spent on YT is 50, 100 on FB and 30 on IG? \n\n\n\n\n\nüëâ Each regression equation ignores the other 2 media in forming coefficient estimates.\n\n The effect of FB advertising on sales may be increased or decreased when YT and IG advertising are in the model. \n IG advertising may have no impact on sales when YT and FB advertising are in the model. \n\n\n\n\n\n\nüëçüëç Better approach: extend the SLR model so that it can directly accommodate multiple predictors.\n\n\n\nFitting a separate SLR model for each predictor is not satisfactory.\n\nIt is unclear how to make a single prediction of sales given levels of the three advertising media budgets, since each of the budgets is associated with a separate regression equation.\nEach of the three regression equations ignores the other two media in forming estimates for the regression coefficients.\nIf the three media budgets are correlated with each other, this can lead to very misleading estimates of the individual media effects on sales.\n\n\nA better approach is to extend the simple linear regression model so that it can directly accommodate multiple predictors."
  },
  {
    "objectID": "slides/05-mlr.html#section-1",
    "href": "slides/05-mlr.html#section-1",
    "title": "Multiple Linear Regression üë©‚Äçüíª",
    "section": "",
    "text": "I hope you don‚Äôt feel‚Ä¶\n\nSource: https://memegenerator.net/instance/62248186/first-world-problems-multiple-regression-more-like-multiple-depression"
  },
  {
    "objectID": "slides/05-mlr.html#section-2",
    "href": "slides/05-mlr.html#section-2",
    "title": "Multiple Linear Regression üë©‚Äçüíª",
    "section": "",
    "text": "What I hope is‚Ä¶"
  },
  {
    "objectID": "slides/05-mlr.html#multiple-linear-regression-mlr-model",
    "href": "slides/05-mlr.html#multiple-linear-regression-mlr-model",
    "title": "Multiple Linear Regression üë©‚Äçüíª",
    "section": "Multiple Linear Regression (MLR) Model",
    "text": "Multiple Linear Regression (MLR) Model\n\nWe have \\(k\\) distinct predictors. The (population) multiple linear regression model: \\[Y_i= \\beta_0 + \\beta_1X_{i1} + \\beta_2X_{i2} + \\dots + \\beta_kX_{ik} + \\epsilon_i\\]\n\n\n\\(X_{ij}\\): \\(j\\)-th regressor value on \\(i\\)-th measurement, \\(j = 1, \\dots, k\\).\n\n\\(\\beta_j\\): \\(j\\)-th coefficient quantifying the association between \\(X_j\\) and \\(Y\\).\n\n\n\nIn the advertising example, \\(k = 3\\) and \\[\\texttt{sales} = \\beta_0 + \\beta_1 \\times \\texttt{YouTube} + \\beta_2 \\times  \\texttt{Facebook} + \\beta_3 \\times \\texttt{Instagram} + \\epsilon\\]\n\n\n\n\n\nAssumptions as SLR\n\n\\(\\epsilon_i \\stackrel{iid}{\\sim} N(0, \\sigma^2)\\)\n\n\n\n\n\n\nWhen \\(k = 1\\), MLR is reduced to SLR.\n\n\nTwo subscripts of X.\nLater, we will learn how to interpret the coefficients correctly.\nWe interpret \\(\\beta_j\\), \\(j = 1, \\dots, p\\), as the average effect on \\(Y\\) of a one unit increase in \\(X_j\\), holding all other predictors fixed.\n\n\n\n\nHow many parameters are there in the model?"
  },
  {
    "objectID": "slides/05-mlr.html#sample-mlr-model",
    "href": "slides/05-mlr.html#sample-mlr-model",
    "title": "Multiple Linear Regression üë©‚Äçüíª",
    "section": "Sample MLR Model",
    "text": "Sample MLR Model\n\nGiven the training sample data \\((x_{11}, \\dots, x_{1k}, y_1), (x_{21}, \\dots, x_{2k}, y_2), \\dots, (x_{n1}, \\dots, x_{nk}, y_n),\\)\n\nThe sample MLR model: \\[\\begin{align}\ny_i &= \\beta_0 + \\beta_1x_{i1} + \\beta_2x_{i2} + \\dots + \\beta_k x_{ik} + \\epsilon_i \\\\\n&= \\beta_0 + \\sum_{j=1}^k\\beta_j x_{ij} + \\epsilon_i, \\quad i = 1, 2, \\dots, n\n\\end{align}\\]\n\n\n\n\nWriting our data and model without using matrix starts getting tedious.\nLater we‚Äôll see our to write it in a more clean way using matrix notation.\nour mpg data set in the homework has this type of structure.\nIn R we usually store this kind of data set in R structure call data frame."
  },
  {
    "objectID": "slides/05-mlr.html#regression-hyperplane",
    "href": "slides/05-mlr.html#regression-hyperplane",
    "title": "Multiple Linear Regression üë©‚Äçüíª",
    "section": "Regression Hyperplane",
    "text": "Regression Hyperplane\n\n\nSLR: regression line\nMLR: regression hyperplane or response surface\n\\(y = \\beta_0 + \\beta_1x_1 + \\beta_{2}x_2 + \\epsilon\\)\n\\(E(y \\mid x_1, x_2) = 50 + 10x_1 + 7x_2\\)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNow I‚Äôm gonna show you what a MLR looks like when we fit it to the data.\nIf we have two predictors, we will have a sample regression plane.\nIf we have more than two predictors in the model, we are not able to visualize it, but the idea is the same.\nWe will have a something called hyperplane or response surface that basically play the same role as the regression plane in 2D or regression line in 1D.\nThe plot on the right is the contour plot when we project the plot onto the x1-x2 plane. You can see that basically the higher x1 and/or the higher x2, the higher value of y.\nMoreover, you can see that the level curves are straight and parallel, meaning that the effect of x1 on y does not change with the values of x2 or the effect does not depend on the level of x2.\nhttps://cran.r-project.org/web/packages/rsm/vignettes/rsm-plots.pdf\nhttp://www.sthda.com/english/wiki/scatterplot3d-3d-graphics-r-software-and-data-visualization\nhttps://stackoverflow.com/questions/18147595/plot-3d-plane-true-regression-surface"
  },
  {
    "objectID": "slides/05-mlr.html#response-surface-interaction-model",
    "href": "slides/05-mlr.html#response-surface-interaction-model",
    "title": "Multiple Linear Regression üë©‚Äçüíª",
    "section": "Response Surface : Interaction Model",
    "text": "Response Surface : Interaction Model\n\n\\(y = \\beta_0 + \\beta_1x_1 + \\beta_{2}x_2 + \\beta_{12}x_1x_2 + \\epsilon\\)\nThis is in fact a linear regression model: let \\(\\beta_3 = \\beta_{12}, x_3 = x_1x_2\\).\n\\(E(y \\mid x_1, x_2) = 50 + 10x_1 + 7x_2 + 5x_1x_2\\)\nüòé ü§ì A linear model generates a nonlinear response surface!\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRemember in SLR, we can have a liner model that describes a nonlinear relationship.\nSame in MLR. We can have a linear model that generates a nonlinear response surface!\n\\(y = \\beta_0 + \\beta_1x_1 + \\beta_{2}x_2 + \\beta_{12}x_1x_2 + \\epsilon\\)\nThis is in fact a linear regression model: let \\(\\beta_3 = \\beta_{12}, x_3 = x_1x_2\\)."
  },
  {
    "objectID": "slides/05-mlr.html#response-surface-2nd-order-with-interaction-model",
    "href": "slides/05-mlr.html#response-surface-2nd-order-with-interaction-model",
    "title": "Multiple Linear Regression üë©‚Äçüíª",
    "section": "Response Surface : 2nd Order with Interaction Model",
    "text": "Response Surface : 2nd Order with Interaction Model\n\n\\(y = \\beta_0 + \\beta_1x_1 + \\beta_{2}x_2 + \\beta_{11}x_1^2 + \\beta_{22}x_2^2 + \\beta_{12}x_1x_2 + \\epsilon\\)\n\\(E(y) = 800+10x_1+7x_2 -8.5x_1^2-5x_2^2- 4x_1x_2\\)\nüòé ü§ì A linear regression model can describe a complex nonlinear relationship between the response and predictors!"
  },
  {
    "objectID": "slides/05-mlr.html#least-square-estimation-of-the-coefficients",
    "href": "slides/05-mlr.html#least-square-estimation-of-the-coefficients",
    "title": "Multiple Linear Regression üë©‚Äçüíª",
    "section": "Least Square Estimation of the Coefficients",
    "text": "Least Square Estimation of the Coefficients\n\\[\\begin{align}\ny_i &= \\beta_0 + \\beta_1x_{i1} + \\beta_2x_{i2} + \\dots + \\beta_k x_{ik} + \\epsilon_i \\\\\n&= \\beta_0 + \\sum_{j=1}^k\\beta_j x_{ij} + \\epsilon_i, \\quad i = 1, 2, \\dots, n\n\\end{align}\\]\n\nThe least-squares function is \\[S(\\alpha_0, \\alpha_1, \\dots, \\alpha_k) = \\sum_{i=1}^n\\left(y_i - \\alpha_0 - \\sum_{j=1}^k\\alpha_j x_{ij}\\right)^2\\] The function \\(S(\\cdot)\\) must be minimized with respect to the coefficients, i.e., \\[(b_0, b_1, \\dots, b_k) = \\underset{{\\alpha_0, \\alpha_1, \\dots, \\alpha_k}}{\\mathrm{arg \\, min}}  S(\\alpha_0, \\alpha_1, \\dots, \\alpha_k)\\]\n\n\n\nAs SLR, we can define the least-squares function as the sum of squares of epsilon.\nThe idea is to minimize the function \\(S\\) w.r.t. the model coefficients \\(\\beta_0, \\beta_1, \\dots, \\beta_k\\).\nIn other words, we are going to choose the sample statistics \\(b_0\\), \\(b_1\\), ‚Ä¶, \\(b_k\\) as the estimates of \\(\\beta_0, \\beta_1, \\dots, \\beta_k\\) so that \\(S(.)\\) is minimized when \\(b_0\\), \\(b_1\\), ‚Ä¶, \\(b_k\\) are plugged in the function."
  },
  {
    "objectID": "slides/05-mlr.html#geometry-of-least-square-estimation",
    "href": "slides/05-mlr.html#geometry-of-least-square-estimation",
    "title": "Multiple Linear Regression üë©‚Äçüíª",
    "section": "Geometry of Least Square Estimation",
    "text": "Geometry of Least Square Estimation\n\n\nIf we look at the geometry of Least Square Estimation of the MLR, we have a visualization like this.\nAgain, in SLR, different \\(b_0\\) and \\(b_1\\)s give us different sample regression lines.\nIn MLR, suppose we have two predictors, and different \\(b_0\\) and \\(b_1\\) and \\(b_2\\) give us different sample regression planes.\nso geometrically speaking, we are trying to find a sample regression plane such that the sum of the squared distance between the observations (denoted by those blue points) and the plane is minimized.\nFor more than 2 predictor case, we are not able to visualize it because we live a 3D world, but the idea is exactly the same. And we called the regression plane a hyperplane. OK."
  },
  {
    "objectID": "slides/05-mlr.html#least-squares-normal-equations",
    "href": "slides/05-mlr.html#least-squares-normal-equations",
    "title": "Multiple Linear Regression üë©‚Äçüíª",
    "section": "Least-squares Normal Equations",
    "text": "Least-squares Normal Equations\n\\[\\begin{align}\n\\left.\\frac{\\partial S}{\\partial\\alpha_0}\\right\\vert_{b_0, b_1, \\dots, b_k} &= -2 \\sum_{i=1}^n\\left(y_i - b_0 - \\sum_{j=1}^k b_j x_{ij}\\right) = 0\\\\\n\\left.\\frac{\\partial S}{\\partial\\alpha_j}\\right\\vert_{b_0, b_1, \\dots, b_k} &= -2 \\sum_{i=1}^n\\left(y_i - b_0 - \\sum_{j=1}^k b_j x_{ij}\\right)x_{ij} = 0, \\quad j = 1, 2, \\dots, k\n\\end{align}\\]\n\n\n\\(p = k + 1\\) equations with \\(p\\) unknown parameters.\nThe ordinary least squares estimators are the solutions to the normal equations.\n\n\nSo again similar to SLR, we can take derivative w.r.t \\(\\beta_0\\), \\(\\beta_1\\), to the \\(\\beta_k\\).\nAnd we are gonna have \\(p = k + 1\\) equations with \\(p\\) unknown parameters.\nSo we can find one and only one solution to \\(\\beta_0\\), \\(\\beta_1\\), to the \\(\\beta_k\\), which are \\(b_0\\), \\(b_1\\), ‚Ä¶, \\(b_k\\).\nAnd the \\(p\\) equations are the least squares normal questions.\nYou guys solve the equations when k = 1.\nDo you think it is easy to solve the questions when the number of predictors is 2 or more than 2?\n\n\nüçπ üç∫ üç∏ ü•Ç I buy you a drink if you solve the equations for \\(k \\ge 2\\) by hand without using matrix notations or operations!\n\n\nHere I am happy to treat you and buy you a drink if you can solve the equations for \\(k \\ge 2\\) by hand without using matrix notations or operations!\nCome to my office hours and show your work. And we go to a bar together. Sounds good?\nIn fact, if you try, you are gonna deal with lots of algebra, but if we write the system of equations in a matrix form, the solution becomes very clean.\nThat‚Äôs see why."
  },
  {
    "objectID": "slides/05-mlr.html#r-lab-delivery-time-data",
    "href": "slides/05-mlr.html#r-lab-delivery-time-data",
    "title": "Multiple Linear Regression üë©‚Äçüíª",
    "section": "\nR Lab Delivery Time Data",
    "text": "R Lab Delivery Time Data\n\n\n\n# Load the data set\ndelivery <- read.csv(file = \"./data/data-ex-3-1.csv\",\n                     header = TRUE)\ndelivery_data <- delivery[, -1]\ncolnames(delivery_data) <- c(\"time\", \"cases\", \"distance\")\nstr(delivery_data)\n\n'data.frame':   25 obs. of  3 variables:\n $ time    : num  16.7 11.5 12 14.9 13.8 ...\n $ cases   : int  7 3 3 4 6 7 2 7 30 5 ...\n $ distance: int  560 220 340 80 150 330 110 210 1460 605 ...\n\n\n\n\n\\(y\\): the amount of time required by the route driver to stock the vending machines with beverages\n\n\\(x_1\\): the number of cases stocked\n\n\\(x_2\\): the distance walked by the driver\n\n\n\n\ndelivery_data\n\n   time cases distance\n1  16.7     7      560\n2  11.5     3      220\n3  12.0     3      340\n4  14.9     4       80\n5  13.8     6      150\n6  18.1     7      330\n7   8.0     2      110\n8  17.8     7      210\n9  79.2    30     1460\n10 21.5     5      605\n11 40.3    16      688\n12 21.0    10      215\n13 13.5     4      255\n14 19.8     6      462\n15 24.0     9      448\n16 29.0    10      776\n17 15.3     6      200\n18 19.0     7      132\n19  9.5     3       36\n20 35.1    17      770\n21 17.9    10      140\n22 52.3    26      810\n23 18.8     9      450\n24 19.8     8      635\n25 10.8     4      150\n\n\n\n\n\n\n\n\\(y\\): the amount of time required by the route driver to stock the vending machines with beverages in an outlet.\n\n\\(x_1\\): the number of cases stocked\n\n\\(x_2\\): the distance walked by the driver\nGoal: fit a MLR model \\(y = \\beta_0 + \\beta_1x_1 + \\beta_2x_2 + \\epsilon\\) to the amount of time required by the route driver to service the vending machines (use readr::read_csv if the file is large)"
  },
  {
    "objectID": "slides/05-mlr.html#r-lab-scatterplot-matrix",
    "href": "slides/05-mlr.html#r-lab-scatterplot-matrix",
    "title": "Multiple Linear Regression üë©‚Äçüíª",
    "section": "\nR Lab Scatterplot Matrix",
    "text": "R Lab Scatterplot Matrix\n\npairs(delivery_data)\n\n\n\nEach plot shows the relationship between a pair of variables."
  },
  {
    "objectID": "slides/05-mlr.html#r-lab-3d-scatterplot",
    "href": "slides/05-mlr.html#r-lab-3d-scatterplot",
    "title": "Multiple Linear Regression üë©‚Äçüíª",
    "section": "\nR Lab 3D Scatterplot",
    "text": "R Lab 3D Scatterplot\n\nCodepar(mgp = c(2, 0.8, 0), las = 1, mar = c(4, 4, 0, 0))\nlibrary(scatterplot3d)\nscatterplot3d(x = delivery_data$cases, y = delivery_data$distance, z = delivery_data$time,\n              xlab =\"cases\", ylab = \"distance\", zlab = \"time\",\n              xlim = c(2, 30), ylim = c(36, 1640), zlim = c(8, 80),\n              box = TRUE, color = \"blue\", mar = c(3, 3, 0, 2), angle = 30, pch = 16)\n\n\n\nSometimes a 3D scatterplot is useful in visualizing the relationship between the response and the regressors when there are only two regressors."
  },
  {
    "objectID": "slides/05-mlr.html#r-lab-multiple-linear-regression",
    "href": "slides/05-mlr.html#r-lab-multiple-linear-regression",
    "title": "Multiple Linear Regression üë©‚Äçüíª",
    "section": "\nR Lab Multiple Linear Regression",
    "text": "R Lab Multiple Linear Regression\n\ndelivery_lm <- lm(time ~ cases + distance, data = delivery_data)\ndelivery_lm$coef\n\n(Intercept)       cases    distance \n      2.341       1.616       0.014 \n\n\n\\[\\hat{y} = 2.34 + 1.62x_1 + 0.014x_2\\]\n\n\\(b_1\\): All else held constant, for one case of product stocked increase, we expect the delivery time to be longer, on average, by 1.62 minutes.\n\\(b_2\\): All else held constant, one additional foot walked by the driver causes the delivery time, on average, to be 0.014 minutes longer.\n\\(b_0\\): The delivery time with no number of cases of product stocked and no distance walked by the driver is expected to be 2.34 minutes. (Make sense?!)"
  },
  {
    "objectID": "slides/05-mlr.html#r-lab-regression-plane",
    "href": "slides/05-mlr.html#r-lab-regression-plane",
    "title": "Multiple Linear Regression üë©‚Äçüíª",
    "section": "\nR Lab Regression Plane",
    "text": "R Lab Regression Plane"
  },
  {
    "objectID": "slides/05-mlr.html#estimation-of-sigma2",
    "href": "slides/05-mlr.html#estimation-of-sigma2",
    "title": "Multiple Linear Regression üë©‚Äçüíª",
    "section": "Estimation of \\(\\sigma^2\\)\n",
    "text": "Estimation of \\(\\sigma^2\\)\n\n\n\\(SS_{res} = \\sum_{i=1}^ne_i^2 = \\sum_{i=1}^n(y_i - \\hat{y}_i)^2\\).\n\\(MS_{res} = \\frac{SS_{res}}{n - p}\\) with \\(p = k + 1\\).\n\\(s^2 = MS_{res}\\) is unbiased for \\(\\sigma^2\\), i.e., \\(E[MS_{res}] = \\sigma^2\\).\n\n\n\\(s^2\\) of SLR may be quite larger than the \\(s^2\\) of MLR.\n\\(s^2\\) measures the variation of the unexplained noise about the fitted regression line/hyperplane, so we prefer a small residual mean square.\n\n\n\n\\(SS_{res} = \\sum_{i=1}^ne_i^2 = {\\bf e'e} = {\\bf (y-Xb)'(y-Xb)} = {\\bf y'y - b'X'y} = \\sum_{i=1}^n(y_i - \\hat{y}_i)^2\\).\nIf our model is specified correctly, \\(\\hat{\\sigma}^2\\) depends on our data quality.\nIf our data have lots of noise itself, there is nothing we can do."
  },
  {
    "objectID": "slides/05-mlr.html#r-lab-estimation-of-sigma2",
    "href": "slides/05-mlr.html#r-lab-estimation-of-sigma2",
    "title": "Multiple Linear Regression üë©‚Äçüíª",
    "section": "\nR Lab Estimation of \\(\\sigma^2\\)\n",
    "text": "R Lab Estimation of \\(\\sigma^2\\)\n\n\n## method 1\nsumm_delivery <- summary(delivery_lm)  ## check names(summ_delivery)\nsumm_delivery$sigma ^ 2\n\n[1] 11\n\n\n\n\n## method 2\nn <- length(delivery_lm$residuals)\n(SS_res <- sum(delivery_lm$residuals * delivery_lm$residuals))\n\n[1] 234\n\nSS_res / (n - 3)\n\n[1] 11\n\n\n\n\n\n## method 3\n(SS_res1 <- sum((delivery_data$time - delivery_lm$fitted.values)^2))\n\n[1] 234\n\nSS_res1 / (n - 3)\n\n[1] 11"
  },
  {
    "objectID": "slides/05-mlr.html#properties-of-lses",
    "href": "slides/05-mlr.html#properties-of-lses",
    "title": "Multiple Linear Regression üë©‚Äçüíª",
    "section": "Properties of LSEs",
    "text": "Properties of LSEs\n\nüëç \\({\\bf b} = (b_0, b_1, \\dots, b_k)'\\) is BLUE.\n\n Linear : Each \\(b_j\\) is a linear combination of \\(y_1, \\dots, y_n\\).\n\n Unbiased : Each \\(b_j\\) is normally distributed with mean \\(\\beta_j\\).\n\n Best : Each \\(b_j\\) has the minimum variance, comparing to all other unbiased estimator for \\(\\beta_j\\) that is a linear combo of \\(y_1, \\dots, y_n\\)."
  },
  {
    "objectID": "slides/05-mlr.html#wald-ci-for-coefficients",
    "href": "slides/05-mlr.html#wald-ci-for-coefficients",
    "title": "Multiple Linear Regression üë©‚Äçüíª",
    "section": "Wald CI for Coefficients",
    "text": "Wald CI for Coefficients\nThe \\((1-\\alpha)100\\%\\) Wald CI for \\(\\beta_j\\), \\(j = 0, 1, \\dots, k\\) is \\[\\left(b_j- t_{\\alpha/2, n-p}~se(b_j), \\quad b_j + t_{\\alpha/2, n-p}~ se(b_j)\\right)\\]\n\n(ci <- confint(delivery_lm))\n\n             2.5 % 97.5 %\n(Intercept) 0.0668  4.616\ncases       1.2618  1.970\ndistance    0.0069  0.022\n\n\n\nThese are marginal CIs seperately for each \\(b_j\\).\n\n\nEach of the statistics \\(\\frac{b_j - \\beta_j}{\\sqrt{\\hat{\\sigma}^2C_{jj}}}, j = 0, 1, 2, \\dots, k\\) follows \\(t_{n-p}\\) distribution, where \\(\\hat{\\sigma}^2 = MS_{res}\\), and \\(C_{jj}\\) is the \\(j\\)-th diagonal element of \\({\\bf (X'X)}^{-1}\\).\nThe \\((1-\\alpha)100\\%\\) CI for \\(\\beta_j\\), \\(j = 0, 1, \\dots, k\\) is \\(\\left(b_j- t_{\\alpha/2, n-p} \\sqrt{\\hat{\\sigma}^2C_{jj}}, \\quad b_j + t_{\\alpha/2, n-p} \\sqrt{\\hat{\\sigma}^2C_{jj}}\\right)\\)\n\nThese interval estimates do not take correlation of coefficients into account. One coefficient may be higher when another is lower.\nWe can only use the intervals one at a time when doing interval estimation. When we want to do interval estimation for several coefficients together at the same time, these intervals are not that accurate."
  },
  {
    "objectID": "slides/05-mlr.html#correlated-coefficients",
    "href": "slides/05-mlr.html#correlated-coefficients",
    "title": "Multiple Linear Regression üë©‚Äçüíª",
    "section": "Correlated Coefficients",
    "text": "Correlated Coefficients\n\nCovariance of random variables \\(X\\) and \\(Y\\), \\(\\mathrm{Cov}(X, Y)\\) is defined as \\[\\small \\mathrm{Cov}(X, Y) = E[(X - E(X))(Y - E(Y))]\\]\n\n\nWhat is \\(\\mathrm{Cov}(X, X)\\)?\n\n\n\n\n\nThe covariance matrix of the coefficient vector \\({\\bf b} = (b_0, b_1, b_2)'\\) is \\[\\scriptsize \\begin{align} \\mathrm{Cov}({\\bf b}) &= \\begin{bmatrix} \\mathrm{Cov}(b_0, b_0) & \\mathrm{Cov}(b_0, b_1) & \\mathrm{Cov}(b_0, b_2) \\\\ \\mathrm{Cov}(b_1, b_0) & \\mathrm{Cov}(b_1, b_1) & \\mathrm{Cov}(b_1, b_2) \\\\ \\mathrm{Cov}(b_2, b_0) & \\mathrm{Cov}(b_2, b_1) & \\mathrm{Cov}(b_2, b_2) \\end{bmatrix} \\end{align}\\]\n\n\n\nThe correlation matrix of the coefficient vector \\({\\bf b} = (b_0, b_1, b_2)'\\) is \\[\\scriptsize \\begin{align} \\mathrm{Corr}({\\bf b}) &= \\begin{bmatrix} 1 & r_{01} & r_{02} \\\\ r_{10} & 1 & r_{12} \\\\ r_{20} & r_{21} & 1 \\end{bmatrix} \\end{align}\\]"
  },
  {
    "objectID": "slides/05-mlr.html#correlated-coefficients-1",
    "href": "slides/05-mlr.html#correlated-coefficients-1",
    "title": "Multiple Linear Regression üë©‚Äçüíª",
    "section": "Correlated Coefficients",
    "text": "Correlated Coefficients\nAll \\(b_j\\) are correlated!\n\n\n\n## variance-covariance matrix\n(V <- vcov(delivery_lm))\n\n            (Intercept)    cases distance\n(Intercept)     1.20282 -0.04726 -8.9e-04\ncases          -0.04726  0.02915 -5.1e-04\ndistance       -0.00089 -0.00051  1.3e-05\n\n## standard error\nsqrt(diag(V))\n\n(Intercept)       cases    distance \n     1.0967      0.1707      0.0036 \n\n\n\n\n## correlation matrix\ncov2cor(V)\n\n            (Intercept) cases distance\n(Intercept)        1.00 -0.25    -0.22\ncases             -0.25  1.00    -0.82\ndistance          -0.22 -0.82     1.00\n\n\n\n\n\n\\(b_1\\) and \\(b_2\\) are negatively correlated.\nIndividual CI ignores the correlation between \\(b_j\\)s.\n\n\n\nHow do we specify a confidence level that applies simultaneously to a set of interval estimates? For example, a \\(95\\%\\) confidence ‚Äúinterval‚Äù for both \\(b_1\\) and \\(b_2\\)."
  },
  {
    "objectID": "slides/05-mlr.html#confidence-region",
    "href": "slides/05-mlr.html#confidence-region",
    "title": "Multiple Linear Regression üë©‚Äçüíª",
    "section": "Confidence Region",
    "text": "Confidence Region\n\nThe \\((1-\\alpha)100\\%\\) CI for a set of \\(b_j\\)s will be an elliptically-shaped region!\n\n\n\n\n## confidence region\ncar::confidenceEllipse(\n    delivery_lm, \n    levels = 0.95, \n    which.coef = c(\"cases\", \"distance\"), \n    main = expression(\n        paste(\"95% Confidence Region for \", \n              beta[1], \" and \",  beta[2])\n        )\n    )\n## marginal CI for cases\nabline(v = ci[2, ], lty = 2, lwd = 2)  \n## marginal CI for distance\nabline(h = ci[3, ], lty = 2, lwd = 2)  \n\n\n\n\n\n\n\n\n\n\n\n\n\nSome points (red) within the marginal intervals are implausible according to the joint region.\nThe joint region includes values of the coefficient for cases (black), for example, that are excluded from the marginal interval.\n\n\nBonferroni\nScheffe S-method\nmaximum modulus t procedure With repeated sampling, 95% of such ellipses will simultaneously include Œ≤1 and Œ≤2, if the fitted model is correct and normality holds. The orientation of the ellipse reflects the negative correlation between the estimates. Contrast the 95% confidence ellipse with the marginal 95% confidence intervals, also shown on the plot. Some points within the marginal intervals‚Äîwith larger values for both of the coefficients, for example‚Äîare implausible according to the joint region. Similarly, the joint region includes values of the coefficient for income, for example, that are excluded from the marginal interval."
  },
  {
    "objectID": "slides/05-mlr.html#ci-for-the-mean-response-ey-mid-bf-x_0",
    "href": "slides/05-mlr.html#ci-for-the-mean-response-ey-mid-bf-x_0",
    "title": "Multiple Linear Regression üë©‚Äçüíª",
    "section": "CI for the Mean Response \\(E(y \\mid {\\bf x}_0)\\)\n",
    "text": "CI for the Mean Response \\(E(y \\mid {\\bf x}_0)\\)\n\n\nThe fitted value at a point \\({\\bf x}_0 = (1, x_{01}, x_{02}, \\dots, x_{0k})'\\) is \\[\\hat{y}_0 = b_0 + b_1x_{01} + \\cdots + b_kx_{0k}\\]\n\nThis is an unbiased estimator for \\(E(y \\mid {\\bf x}_0)\\) \n\nThe \\((1-\\alpha)100\\%\\) CI for \\(E(y \\mid {\\bf x}_0)\\) is \\[\\left(\\hat{y}_0 - t_{\\alpha/2, n-p} ~ se(\\hat{y}_0), \\quad \\hat{y}_0 + t_{\\alpha/2, n-p} ~ se(\\hat{y}_0)\\right)\\]\n\n\n\npredict(delivery_lm,\n        newdata = data.frame(cases = 8, distance = 275),\n        interval = \"confidence\", level = 0.95)\n\n  fit lwr upr\n1  19  18  21\n\n\n\nThe fitted value at a point \\({\\bf x}_0 = (1, x_{01}, x_{02}, \\dots, x_{0k})'\\) is \\(\\hat{y}_0 = {\\bf x}_0'\\bf b\\).\nThis is an unbiased estimator for \\(E(y \\mid {\\bf x}_0)\\).\n\n\\(\\frac{\\hat{y}_0 - E(y|{\\bf x}_0)}{\\sqrt{\\hat{\\sigma}^2{\\bf x}_0'{\\bf (X'X)}^{-1}{\\bf x}_0}}\\sim t_{n-p}\\).\nThe \\((1-\\alpha)100\\%\\) CI for \\(E(y \\mid {\\bf x}_0)\\) is \\(\\left(\\hat{y}_0 - t_{\\alpha/2, n-p} \\sqrt{\\hat{\\sigma}^2{\\bf x}_0'{\\bf (X'X)}^{-1}{\\bf x}_0}, \\quad \\hat{y}_0 + t_{\\alpha/2, n-p} \\sqrt{\\hat{\\sigma}^2{\\bf x}_0'{\\bf (X'X)}^{-1}{\\bf x}_0}\\right)\\)\n\n\n\n\n  fit lwr upr\n1  19  18  21"
  },
  {
    "objectID": "slides/05-mlr.html#pi-for-new-observations",
    "href": "slides/05-mlr.html#pi-for-new-observations",
    "title": "Multiple Linear Regression üë©‚Äçüíª",
    "section": "PI for New Observations",
    "text": "PI for New Observations\n\nPredict the future observation \\(y_0\\) when \\({\\bf x} = {\\bf x}_0\\).\nA point estimate is \\(\\hat{y}_0 = b_0 + b_1x_{01} + \\cdots + b_kx_{0k}\\).\nThe \\((1-\\alpha)100\\%\\) PI for \\(y_0\\) is \\[\\left(\\hat{y}_0 - t_{\\alpha/2, n-p} ~se(y_0 - \\hat{y}_0), \\quad \\hat{y}_0 + t_{\\alpha/2, n-p} ~se(y_0 - \\hat{y}_0)\\right)\\]\n\n\n\npredict(delivery_lm,\n        newdata = data.frame(cases = 8, distance = 275),\n        interval = \"predict\", level = 0.95)\n\n  fit lwr upr\n1  19  12  26\n\n\n\nPredict the future observation \\(y_0\\) when \\({\\bf x} = {\\bf x}_0\\).\nA point estimate is \\(\\hat{y}_0 = {\\bf x}_0'\\bf b\\).\nThe \\((1-\\alpha)100\\%\\) PI for \\(y_0\\) is \\(\\left(\\hat{y}_0 - t_{\\alpha/2, n-p} \\sqrt{\\hat{\\sigma}^2\\left(1+{\\bf x}_0'{\\bf (X'X)}^{-1}{\\bf x}_0\\right)}, \\quad \\hat{y}_0 + t_{\\alpha/2, n-p} \\sqrt{\\hat{\\sigma}^2\\left(1+{\\bf x}_0'{\\bf (X'X)}^{-1}{\\bf x}_0\\right)}\\right)\\)\n\n\n\n\n  fit lwr upr\n1  19  12  26"
  },
  {
    "objectID": "slides/05-mlr.html#predictor-effect-plots",
    "href": "slides/05-mlr.html#predictor-effect-plots",
    "title": "Multiple Linear Regression üë©‚Äçüíª",
    "section": "Predictor Effect Plots",
    "text": "Predictor Effect Plots\n\nA complete picture of the regression surface generated by the fitted model would require drawing a \\(p\\)-dimensional graph.\nPredictor effect plots look at one or two-dimensional plots for each predictor.\nTo plot the predictor effect plot for \\(x_1\\), fix the values of all other predictors ( \\(x_2\\) in the example), substituting these fixed values into the fitted regression equation.\nFix \\(x_2\\) at its average, \\(\\hat{y} = 2.34 + 1.62 ~x_1 + 0.014 (409.28)\\)\nFix \\(x_1\\) at its average, \\(\\hat{y} = 2.34 + 1.62 (8.76) + 0.014 ~x_2\\)\nThe slope would be the same for any choice of fixed values of other predictors.\nThe intercepts depend on the values of other predictors."
  },
  {
    "objectID": "slides/05-mlr.html#r-lab-predictor-effect-plots",
    "href": "slides/05-mlr.html#r-lab-predictor-effect-plots",
    "title": "Multiple Linear Regression üë©‚Äçüíª",
    "section": "\nR Lab Predictor Effect Plots",
    "text": "R Lab Predictor Effect Plots\n\nlibrary(effects)\nplot(effects::predictorEffects(mod = delivery_lm))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nrug plot\nthe mean of time increases linearly as cases or distance increases.\nGiven x2 variable in the model, what is the effect on x1 on the response.\nWithout correction for simultaneous statistical inference\nplot(effects::predictorEffects(mod = delivery_lm, confint = list(type = ‚ÄúScheffe‚Äù))) ‚Äòarg‚Äô should be one of ‚Äúpointwise‚Äù, ‚ÄúScheffe‚Äù, ‚Äúscheffe‚Äù"
  },
  {
    "objectID": "slides/05-mlr.html#extrapolation",
    "href": "slides/05-mlr.html#extrapolation",
    "title": "Multiple Linear Regression üë©‚Äçüíª",
    "section": "Extrapolation",
    "text": "Extrapolation\n\nIn MLR, it‚Äôs easy to inadvertently extrapolate since the regressors jointly define the region containing the data.\n\n\n\n\n\n\n\n\n\n\ndefine the smallest convex set containing all of the original \\(n\\) regressor points, as the regressor vairiable hull (RVH).\na point within the ranges of x1 and x2 may not be necessarily a interpolation point."
  },
  {
    "objectID": "slides/05-mlr.html#test-for-significance-of-regression",
    "href": "slides/05-mlr.html#test-for-significance-of-regression",
    "title": "Multiple Linear Regression üë©‚Äçüíª",
    "section": "Test for Significance of Regression",
    "text": "Test for Significance of Regression\n\n\nTest for significance: Determine if there is a linear relationship between the response and any of the regressor variables.\n\n \\(H_0: \\beta_1 = \\beta_2 = \\cdots = \\beta_k = 0 \\quad H_1: \\beta_j \\ne 0 \\text{ for at least one } j\\)  \n\n\n\n\n\nReject \\(H_0\\) if \\(F_{test} > F_{\\alpha, k, n - k - 1}\\).\n\n\nIn SLR, test for significance is the same as testing individual coefficient \\(\\beta_1 = 0\\).\nWe may reject the null when the truth is all beta‚Äôs are nonzero, or only one single beta is nonzero.\nIf there is only one nonzero beta, and we reject the null, basically, the nearly all the variation of Y is explained by the corresponding predictor that has the nonzero beta.\nWe can still say the model is significant because that particular predictor has a significant effect on explaining y.\nAnother way to represent SS_T"
  },
  {
    "objectID": "slides/05-mlr.html#r-lab-test-for-significance",
    "href": "slides/05-mlr.html#r-lab-test-for-significance",
    "title": "Multiple Linear Regression üë©‚Äçüíª",
    "section": "\nR Lab Test for Significance",
    "text": "R Lab Test for Significance\n \\(H_0: \\beta_{1} = \\beta_{2} = 0 \\quad H_1: \\beta_j \\ne 0 \\text{ for at least one } j\\) \n\nsumm_delivery\n\n...\nlm(formula = time ~ cases + distance, data = delivery_data)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-5.788 -0.663  0.436  1.157  7.420 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  2.34123    1.09673    2.13  0.04417 *  \ncases        1.61591    0.17073    9.46  3.3e-09 ***\ndistance     0.01438    0.00361    3.98  0.00063 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.3 on 22 degrees of freedom\nMultiple R-squared:  0.96,  Adjusted R-squared:  0.956 \nF-statistic:  261 on 2 and 22 DF,  p-value: 4.69e-16\n...\n\n\n\nHow do we obtain the ANOVA Table?"
  },
  {
    "objectID": "slides/05-mlr.html#r-lab-wrong-anova-table",
    "href": "slides/05-mlr.html#r-lab-wrong-anova-table",
    "title": "Multiple Linear Regression üë©‚Äçüíª",
    "section": "\nR Lab Wrong ANOVA Table",
    "text": "R Lab Wrong ANOVA Table\n\n## This is for sequential F-test\nanova(delivery_lm)\n\nAnalysis of Variance Table\n\nResponse: time\n          Df Sum Sq Mean Sq F value  Pr(>F)    \ncases      1   5382    5382   506.6 < 2e-16 ***\ndistance   1    168     168    15.8 0.00063 ***\nResiduals 22    234      11                    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\nThis is so called Type-I ANOVA table for a sequential F-test, which is NOT what we want."
  },
  {
    "objectID": "slides/05-mlr.html#r-lab-anova-table-null-vs.-full",
    "href": "slides/05-mlr.html#r-lab-anova-table-null-vs.-full",
    "title": "Multiple Linear Regression üë©‚Äçüíª",
    "section": "\nR Lab ANOVA Table: Null vs.¬†Full",
    "text": "R Lab ANOVA Table: Null vs.¬†Full\n\nTesting coefficients is like model comparison: Compare the full model with the model under \\(H_0\\)!\n\n\n\n\n\nFull model: including both cases and distance predictors\n\nNull model: no predictors \\((\\beta_1 = \\beta_2 = 0)\\)\n\n\n\n\n\n\n\n\n\n\n\n\n## regression with intercept only\nnull_model <- lm(time ~ 1,\n                 data = delivery_data)\nanova(null_model, delivery_lm)\n\n...\nAnalysis of Variance Table\n\nModel 1: time ~ 1\nModel 2: time ~ cases + distance\n  Res.Df  RSS Df Sum of Sq   F  Pr(>F)    \n1     24 5785                             \n2     22  234  2      5551 261 4.7e-16 ***\n...\n\n\n\n\n\nfirst row: info of SS_T\nsecond row: info of SS_R and SS_res\ndoes not show MS"
  },
  {
    "objectID": "slides/05-mlr.html#r2-and-adjusted-r2",
    "href": "slides/05-mlr.html#r2-and-adjusted-r2",
    "title": "Multiple Linear Regression üë©‚Äçüíª",
    "section": "\n\\(R^2\\) and Adjusted \\(R^2\\)\n",
    "text": "\\(R^2\\) and Adjusted \\(R^2\\)\n\n\n\n\\(R^2 = \\frac{SS_R}{SS_T} = 1 - \\frac{SS_{res}}{SS_T}\\)\n\ncalculated exactly as in SLR.\naccesses how well the regression model fits the data.\nmeasures the proportion of variability in \\(Y\\) that is explained by the regression model or the \\(k\\) predictors.\nThe model with one additional predictor always gets a higher \\(R^2\\).\n\n\n\n\nThere are other criterion or metrics for accessing model adequacy or model fit that may be better than R^2\nThe model with one additional predictor always gets a higher \\(R^2\\) even the new predictor has no explanatory power or useless in predicting y. Because the predictor may capture the random noise variation, leading to decrease in \\(SS_{res}\\).\nIf we use R^2 as the measure of model fit, then we will tend to include lots of regressors in the model, and we will always choose the model that has the most predictors, even though lots of predictors have no or little contribution to the fitting or prediction.\nThe idea is a little bit like overfitting in machine learning.\nA complex or a larger model has several disadvantages.\n\nFirst, it‚Äôs more difficult to interpret your model and results. It reduces the interpretability.\nSecond, the computing or running time is usually longer, and sometimes much longer depending on the order of complexity.\nAlso, the data or the model itself may consume lots of memory spaces.\n\n\n\n\nOccam‚Äôs Razor: Don‚Äôt use a complex model if a simpler model can perform equally well!\n\n\n\n\n\nAdjusted \\(R^2\\)\n\n\\(R^2_{adj} = 1 - \\frac{SS_{res}/(n-p)}{SS_T/(n-1)}\\)\napplies a penalty (through \\(p\\)) for number of variables included in the model.\n\n\n\n\nNot the more the better anymore\nAdjusted R2 doesn‚Äôt increase if the new variable does not provide very little information for prediction\nAdjusted R2 will only increase on adding a variable to the model if the addition of the regressor reduces \\(MS_{res}\\)\n\nThis makes adjusted R2 a preferable metric for model selection in multiple regression models."
  },
  {
    "objectID": "slides/05-mlr.html#adjusted-r2-example",
    "href": "slides/05-mlr.html#adjusted-r2-example",
    "title": "Multiple Linear Regression üë©‚Äçüíª",
    "section": "Adjusted \\(R^2\\) Example",
    "text": "Adjusted \\(R^2\\) Example\n\nSuppose that for a model with 3 predictors, \\(SS_{res} = 90\\), \\(SS_T = 245\\), and \\(n = 15\\). \\[R^2_{adj} = 1 - \\frac{90/(15-4)}{245/(15-1)} = 0.53\\]\n\nNow suppose that the 4-th regressor is added into the model, and \\(SS_{res} = 88\\) (always decreases). Then \\[R^2_{adj} = 1 - \\frac{88/(15-5)}{245/(15-1)} = 0.49\\]\n\n\n\nIntuition: The new added regressor should have explanatory power for \\(y\\) large enough, so that \\(MS_{res}\\) is decreased."
  },
  {
    "objectID": "slides/05-mlr.html#r-lab-r2-and-adjusted-r2",
    "href": "slides/05-mlr.html#r-lab-r2-and-adjusted-r2",
    "title": "Multiple Linear Regression üë©‚Äçüíª",
    "section": "\nR Lab \\(R^2\\) and Adjusted \\(R^2\\)\n",
    "text": "R Lab \\(R^2\\) and Adjusted \\(R^2\\)\n\n\nsumm_delivery\n\n...\n\nResidual standard error: 3.3 on 22 degrees of freedom\nMultiple R-squared:  0.96,  Adjusted R-squared:  0.956 \n...\n\n\n\nsumm_delivery$r.squared\n\n[1] 0.96\n\nsumm_delivery$adj.r.squared\n\n[1] 0.96"
  },
  {
    "objectID": "slides/05-mlr.html#tests-on-individual-regression-coefficients",
    "href": "slides/05-mlr.html#tests-on-individual-regression-coefficients",
    "title": "Multiple Linear Regression üë©‚Äçüíª",
    "section": "Tests on Individual Regression Coefficients",
    "text": "Tests on Individual Regression Coefficients\n\nHypothesis test on any single regression coefficient.\n \\(H_0: \\beta_{j} = 0 \\quad H_1: \\beta_j \\ne 0\\) \n\\(t_{test} = \\frac{b_j}{se(b_j)}\\)\nReject \\(H_0\\) if \\(|t_{test}| > t_{\\alpha/2, n-k-1}\\)\n\nThis is a partial or marginal test: a test of the contribution of \\(X_j\\) given ALL other regressors in the model.\n\n\nWe can also do a one-side test for sure. But I am not sure if there is a R function to do a one-sided test.\nBut we can always compute the test statistic or the p-value ourselves.\nHypothesis test on any single regression coefficient.\n \\(H_0: \\beta_{j} = 0 \\quad H_1: \\beta_j \\ne 0\\) \n\n\\(t_{test} = \\frac{b_j}{\\sqrt{\\hat{\\sigma}^2C_{jj}}}\\), where \\(C_{jj}\\) is the \\(j\\)-th diagonal element of \\(({\\bf X'X})^{-1}\\).\nReject \\(H_0\\) if \\(|t_{test}| > t_{\\alpha/2, n-k-1}\\)\n\nThis is a partial or marginal test: a test of the contribution of \\(X_j\\) given ALL other regressors in the model."
  },
  {
    "objectID": "slides/05-mlr.html#r-lab-tests-on-individual-coefficients",
    "href": "slides/05-mlr.html#r-lab-tests-on-individual-coefficients",
    "title": "Multiple Linear Regression üë©‚Äçüíª",
    "section": "\nR Lab Tests on Individual Coefficients",
    "text": "R Lab Tests on Individual Coefficients\n\nAssess the value of \\(x_2\\) (distance) given that \\(x_1\\) (cases) is in the model.\n \\(H_0: \\beta_{2} = 0 \\quad H_1: \\beta_2 \\ne 0\\) \n\n\nsumm_delivery$coefficients\n\n            Estimate Std. Error t value Pr(>|t|)\n(Intercept)    2.341     1.0967     2.1  4.4e-02\ncases          1.616     0.1707     9.5  3.3e-09\ndistance       0.014     0.0036     4.0  6.3e-04\n\n\nqt(0.05/2, df = delivery_lm$df.residual, lower.tail = FALSE) ## t critical value df = n - 3 = 22\n\n\ndistance \n       4 \n\n\ndistance \n 0.00063"
  },
  {
    "objectID": "slides/05-mlr.html#inference-pitfalls",
    "href": "slides/05-mlr.html#inference-pitfalls",
    "title": "Multiple Linear Regression üë©‚Äçüíª",
    "section": "Inference Pitfalls",
    "text": "Inference Pitfalls\n\n\nThe test \\(H_0:\\beta_j = 0\\) will always be rejected as long as the sample size is large enough, even \\(x_j\\) has a very small effect on \\(y\\).\n\nConsider the practical significance of the result, not just the statistical significance.\nUse the confidence interval to draw conclusions instead of relying only p-values.\n\n\n\n\n\n\n\n\nIf the sample size is small, there may not be enough evidence to reject \\(H_0:\\beta_j = 0\\).\n\nDON‚ÄôT immediately conclude that the variable has no association with the response.\nThere may be a linear association that is just not strong enough to detect given your data, or there may be a non-linear association.\n\n\n\n\n\n\n\nmath4780-f23.github.io/website"
  },
  {
    "objectID": "slides/06-mlr-mat.html#regression-model-in-matrix-form",
    "href": "slides/06-mlr-mat.html#regression-model-in-matrix-form",
    "title": "Multiple Linear Regression - Matrix Approach \n",
    "section": "Regression Model in Matrix Form",
    "text": "Regression Model in Matrix Form\n\\[y_i= \\beta_0 + \\beta_1x_{i1} + \\beta_2x_{i2} + \\dots + \\beta_kx_{ik} + \\epsilon_i, \\quad \\epsilon_i \\stackrel{iid}{\\sim} N(0, \\sigma^2), \\quad i = 1, 2, \\dots, n\\]\n\n\n\\[\\bf y = \\bf X \\boldsymbol \\beta+ \\boldsymbol \\epsilon\\] where\n\\[\\begin{align}\n\\bf y = \\begin{bmatrix}\ny_1 \\\\\ny_2 \\\\\n\\vdots \\\\\ny_n \\end{bmatrix},\\quad\n\\bf X = \\begin{bmatrix}\n  1 & x_{11} & x_{12} & \\cdots & x_{1k} \\\\\n  1 & x_{21} & x_{22} & \\cdots & x_{2k} \\\\\n  \\vdots & \\vdots  & \\vdots  & \\ddots & \\vdots  \\\\\n  1 & x_{n1} & x_{n2} & \\cdots & x_{nk} \\end{bmatrix}, \\quad\n\\boldsymbol \\beta= \\begin{bmatrix}\n\\beta_0 \\\\\n\\beta_1 \\\\\n\\vdots \\\\\n\\beta_k \\end{bmatrix} , \\quad\n\\boldsymbol \\epsilon= \\begin{bmatrix}\n\\epsilon_1 \\\\\n\\epsilon_2 \\\\\n\\vdots \\\\\n\\epsilon_n\\end{bmatrix}\n\\end{align}\\]\n\n\n\\({\\bf X}_{n \\times p}\\): Design matrix\n\n\\(\\boldsymbol \\epsilon\\sim MVN_n({\\bf 0}, \\sigma^2 {\\bf I}_n)\\) 1\n\n\n\nLet‚Äôs first write our model in a matrix form.\n\n\\(\\bf X\\) is usually called the design matrix.\n\n\nFor simplicity and convenience, \\(N({\\bf a}, {\\bf B})\\) represents a multivariate normal distribution with mean vector \\({\\bf a}\\) and covariance matrix \\({\\bf B}\\)."
  },
  {
    "objectID": "slides/06-mlr-mat.html#regression-model-in-matrix-form-1",
    "href": "slides/06-mlr-mat.html#regression-model-in-matrix-form-1",
    "title": "Multiple Linear Regression - Matrix Approach \n",
    "section": "Regression Model in Matrix Form",
    "text": "Regression Model in Matrix Form\n\nFor a random vector \\({\\bf Z} = (Z_1, \\dots, Z_n)'\\), its mean vector is \\[E({\\bf Z}) = E\\begin{pmatrix}\nZ_1 \\\\\nZ_2 \\\\\n\\vdots \\\\\nZ_n\\end{pmatrix} = \\begin{bmatrix}\nE(Z_1) \\\\\nE(Z_2) \\\\\n\\vdots \\\\\nE(Z_n)\\end{bmatrix}\\]\nIts (symmetric) variance-covariance matrix is \\[\\scriptsize \\begin{align} \\mathrm{Cov}({\\bf Z}) = {\\bf \\Sigma} &= \\begin{bmatrix} \\mathrm{Var}(Z_1) & \\mathrm{Cov}(Z_1, Z_2) & \\cdots & \\mathrm{Cov}(Z_1, Z_n) \\\\ \\mathrm{Cov}(Z_2, Z_1) & \\mathrm{Var}(Z_2) & \\cdots & \\mathrm{Cov}(Z_2, Z_n) \\\\ \\vdots  & \\vdots  & \\ddots & \\vdots  \\\\ \\mathrm{Cov}(Z_n, Z_1) & \\mathrm{Cov}(Z_n, Z_2) & \\cdots & \\mathrm{Var}(Z_n) \\end{bmatrix} \\end{align}\\]"
  },
  {
    "objectID": "slides/06-mlr-mat.html#least-squares-estimation-in-matrix-form",
    "href": "slides/06-mlr-mat.html#least-squares-estimation-in-matrix-form",
    "title": "Multiple Linear Regression - Matrix Approach \n",
    "section": "Least Squares Estimation in Matrix Form",
    "text": "Least Squares Estimation in Matrix Form\n\\[S(\\beta_0, \\beta_1, \\dots, \\beta_k) = \\sum_{i=1}^n\\epsilon_i^2 = \\sum_{i=1}^n\\left(y_i - \\beta_0 - \\sum_{j=1}^k\\beta_j x_{ij}\\right)^2\\]\n\\[\\begin{align}\n\\left.\\frac{\\partial S}{\\partial\\beta_0}\\right\\vert_{b_0, b_1, \\dots, b_k} &= -2 \\sum_{i=1}^n\\left(y_i - b_0 - \\sum_{j=1}^k b_j x_{ij}\\right) = 0\\\\\n\\left.\\frac{\\partial S}{\\partial\\beta_j}\\right\\vert_{b_0, b_1, \\dots, b_k} &= -2 \\sum_{i=1}^n\\left(y_i - b_0 - \\sum_{j=1}^k b_j x_{ij}\\right)x_{ij} = 0, \\quad j = 1, 2, \\dots, k\n\\end{align}\\]\n\n\\[\\begin{align}\nS(\\boldsymbol \\beta) = \\sum_{i=1}^n\\epsilon_i^2 = \\boldsymbol \\epsilon'\\boldsymbol \\epsilon&= (\\bf y - \\bf X \\boldsymbol \\beta)'(\\bf y - \\bf X \\boldsymbol \\beta) \\\\\n&= \\bf y'\\bf y - \\boldsymbol \\beta'\\bf X'\\bf y - \\bf y'\\bf X \\boldsymbol \\beta+ \\boldsymbol \\beta' \\bf X' \\bf X \\boldsymbol \\beta\\\\\n&={\\bf y}'{\\bf y} - 2\\boldsymbol \\beta'{\\bf X}'{\\bf y} + \\boldsymbol \\beta' {\\bf X}' {\\bf X} \\boldsymbol \\beta\n\\end{align}\\]"
  },
  {
    "objectID": "slides/06-mlr-mat.html#least-squares-estimation-in-matrix-form-1",
    "href": "slides/06-mlr-mat.html#least-squares-estimation-in-matrix-form-1",
    "title": "Multiple Linear Regression - Matrix Approach \n",
    "section": "Least Squares Estimation in Matrix Form",
    "text": "Least Squares Estimation in Matrix Form\n\\[\\begin{align}\nS(\\boldsymbol \\beta) = \\sum_{i=1}^n\\epsilon_i^2 = \\boldsymbol \\epsilon'\\boldsymbol \\epsilon&= (\\bf y - \\bf X \\boldsymbol \\beta)'(\\bf y - \\bf X \\boldsymbol \\beta) \\\\\n&= \\bf y'\\bf y - \\boldsymbol \\beta'\\bf X'\\bf y - \\bf y'\\bf X \\boldsymbol \\beta+ \\boldsymbol \\beta' \\bf X' \\bf X \\boldsymbol \\beta\\\\\n&={\\bf y}'{\\bf y} - 2\\boldsymbol \\beta'{\\bf X}'{\\bf y} + \\boldsymbol \\beta' {\\bf X}' {\\bf X} \\boldsymbol \\beta\n\\end{align}\\]\n\nLet \\({\\bf t}\\) and \\({\\bf a}\\) be \\(n \\times 1\\) column vectors, and \\({\\bf A}_{n \\times n}\\) is a symmetric matrix.\n\n\\(\\frac{\\partial {\\bf t}'{\\bf a} }{\\partial {\\bf t}} = \\frac{\\partial {\\bf a}'{\\bf t} }{\\partial {\\bf t}} = {\\bf a}\\)\n\\(\\frac{\\partial {\\bf t}'{\\bf A} {\\bf t}}{\\partial {\\bf t}} = 2{\\bf A} {\\bf t}\\)\n\n\n\nNormal equations: \\(\\begin{align} \\left.\\frac{\\partial S}{\\partial \\boldsymbol \\beta}\\right \\vert_{\\bf b} = -2 {\\bf X}' {\\bf y} + 2 {\\bf X}' {\\bf X} {\\bf b} = \\boldsymbol{0} \\end{align}\\)\n\n\n\nTo get the LSE in a matrix form, we first write the sum of squares using matrices.\n\n\nLSE for \\(\\boldsymbol \\beta\\):  \\(\\begin{align} \\boxed{{\\bf b} = ({\\bf X}' {\\bf X}) ^{-1} {\\bf X}' \\bf y} \\end{align}\\)"
  },
  {
    "objectID": "slides/06-mlr-mat.html#normal-equations",
    "href": "slides/06-mlr-mat.html#normal-equations",
    "title": "Multiple Linear Regression - Matrix Approach \n",
    "section": "Normal Equations",
    "text": "Normal Equations\n\\((\\bf X' \\bf X) \\bf b = \\bf X' \\bf y\\)\n\n\nSimple linear regression case."
  },
  {
    "objectID": "slides/06-mlr-mat.html#r-lab-design-matrix",
    "href": "slides/06-mlr-mat.html#r-lab-design-matrix",
    "title": "Multiple Linear Regression - Matrix Approach \n",
    "section": "\nR Lab Design Matrix",
    "text": "R Lab Design Matrix\n\n\n\n\nX <- cbind(1, delivery_data[, c(2, 3)])\n\n   1    7  560\n   1    3  220\n   1    3  340\n   1    4   80\n   1    6  150\n   1    7  330\n   1    2  110\n   1    7  210\n   1   30 1460\n   1    5  605\n   1   16  688\n   1   10  215\n   1    4  255\n   1    6  462\n   1    9  448\n   1   10  776\n   1    6  200\n   1    7  132\n   1    3   36\n   1   17  770\n   1   10  140\n   1   26  810\n   1    9  450\n   1    8  635\n   1    4  150\n\n\n\n\ny <- as.matrix(delivery_data$time)\n\n16.7\n11.5\n12.0\n14.9\n13.8\n18.1\n 8.0\n17.8\n79.2\n21.5\n40.3\n21.0\n13.5\n19.8\n24.0\n29.0\n15.3\n19.0\n 9.5\n35.1\n17.9\n52.3\n18.8\n19.8\n10.8"
  },
  {
    "objectID": "slides/06-mlr-mat.html#r-lab-regression-coefficients",
    "href": "slides/06-mlr-mat.html#r-lab-regression-coefficients",
    "title": "Multiple Linear Regression - Matrix Approach \n",
    "section": "\nR Lab Regression Coefficients",
    "text": "R Lab Regression Coefficients\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\({\\bf b} = ({\\bf X}' {\\bf X}) ^{-1} \\bf X' \\bf y\\)\n\nt(X) %*% X\n\n             1  cases distance\n1           25    219    10232\ncases      219   3055   133899\ndistance 10232 133899  6725688\n\nt(X) %*% y\n\n           [,1]\n1           560\ncases      7375\ndistance 337072\n\n(b <- solve(t(X) %*% X) %*% t(X) %*% y)\n\n          [,1]\n1        2.341\ncases    1.616\ndistance 0.014"
  },
  {
    "objectID": "slides/06-mlr-mat.html#hat-matrix",
    "href": "slides/06-mlr-mat.html#hat-matrix",
    "title": "Multiple Linear Regression - Matrix Approach \n",
    "section": "Hat Matrix",
    "text": "Hat Matrix\n\nIn SLR, \\(\\hat{y}_i = h_{i1}y_1 + h_{i2}y_2 + \\cdots + h_{in}y_n = {\\bf h}_i'{\\bf y}\\) where \\(h_{ij} = \\frac{1}{n} + \\frac{(x_i-\\overline{x})(x_j-\\overline{x})}{S_{xx}}\\) and \\({\\bf h}_i' = (h_{i1}, h_{i2}, \\dots, h_{in})\\). The hat matrix is \\({\\bf H} = (h_{ij})_{n \\times n}\\).\n\n\n\\[\\hat{\\bf y} = {\\bf X} {\\bf b} = {\\bf X} ({\\bf X}' {\\bf X}) ^{-1} \\bf X' \\bf y = \\bf H \\bf y\\]\n\n\\({\\bf H} = {\\bf X} ({\\bf X}' {\\bf X}) ^{-1} \\bf X'\\)\nThe vector of residuals \\(e_i = y_i - \\hat{y}_i\\) is \\[\\bf e = \\bf y - \\hat{\\bf y} = \\bf y - \\bf X \\bf b = \\bf y -\\bf H \\bf y = (\\bf I - \\bf H) \\bf y\\]\n\n\n\n\n\nBoth \\(\\bf H\\) and \\(\\bf I-H\\) are symmetric and idempotent. They are projection matrices.\n\n\\(\\bf H\\) projects \\(\\bf y\\) to \\(\\hat{\\bf y}\\) on the \\(p\\)-dimensional space spanned by columns of \\(\\bf X\\), or the column space of \\(\\bf X\\), \\(Col(\\bf X)\\).\n\n\\(\\bf I - H\\) projects \\(\\bf y\\) to \\(\\bf e\\) on the space perpendicular to \\(Col(\\bf X)\\), or \\(Col(\\bf X)^{\\bot}\\).\n\n\n\nThe vector of fitted values \\(\\hat{y}_i\\) corresponding to \\(y_i\\) is \\[\\hat{\\bf y} = \\bf X \\bf b = \\bf X (\\bf X' \\bf X) ^{-1} \\bf X' \\bf y = \\bf H \\bf y\\]\n\n\n\\(\\bf H\\) plays an important role in regression analysis.\nAnd it will be shown up many times later in many topics of this course.\nOK. It‚Äôs a little abstract. Let‚Äôs look into the geometrical interpretation of least squares little by little."
  },
  {
    "objectID": "slides/06-mlr-mat.html#geometrical-interpretation-of-least-squares",
    "href": "slides/06-mlr-mat.html#geometrical-interpretation-of-least-squares",
    "title": "Multiple Linear Regression - Matrix Approach \n",
    "section": "Geometrical Interpretation of Least Squares",
    "text": "Geometrical Interpretation of Least Squares\n\n\n\n\\(Col({\\bf X}) = \\{ {\\bf Xb}: {\\bf b} \\in {\\bf R}^p \\}\\)\n\\({\\bf y} \\notin Col({\\bf X})\\)\n\\(\\hat{{\\bf y}} = {\\bf Xb} = {\\bf H} {\\bf y} \\in Col({\\bf X})\\)\nMinimize the distance of \\(\\color{red}{A}\\) to \\(Col(\\bf X)\\): Find the point in \\(Col(\\bf X)\\) that is closest to \\(A\\).\n\n\n\n\n\n\nhttps://commons.wikimedia.org/wiki/File:OLS_geometric_interpretation.svg\n\n\n\n\n\n\n\n\n\nwhat is column space of \\(X\\)?\n\n\nThe col space of \\(X\\) is the set or the collection of all linear combinations of columns of \\(X\\). In other words, column space of \\(X\\) is the space spanned by the columns of \\(X\\).\nExample\nFor visualization purpose, suppose \\(X\\) has 2 columns, \\(x_1\\) and \\(x_2\\). The column space of \\(X\\) will be the plane spanned by the 2 columns, shown in green color.\n\n\n\n\nobservation \\(y\\).\n\n\nBasically 99% of the time, \\(Y\\) will not be in the column space of \\(X\\).\n\\(y_i = b_0 + b_1x_{i1} + ... + b_kx_{ik} + e_i\\)\n\n\\(y_i\\) is not the linear combo of columns of \\(X\\), and \\(y\\) is not a point in Col(X).\nSo \\(y\\) is point in the \\(n\\)-dimensional space, and not on the plane spanned by columns of \\(X\\).\n\n\n\n\nH (Let‚Äôs see what the projection matrix \\(H\\) is doing)\n\n\nWhen we multiply \\(H\\) by \\(y\\), we are finding the projection of \\(y\\) onto the Col(X).\nThe projection of \\(y\\) on the Col(X) is actually the fitted value \\(\\hat{y}\\).\n\\(\\hat{y}_i = b_0 + b_1x_{i1} + ... + b_kx_{ik}\\)\nThere are so many (actually infinitely many) linear combo of columns of \\(X\\), why this particular vector is our \\(\\hat{y}\\)?\nThe distance between \\(y\\) and Col(X) is actually the shortest when we do the projection of \\(y\\) onto the Col(X).\nIn other words the distance between \\(y\\) and \\(\\hat{y}\\) is the minimal distance among all the distance between \\(y\\) and any point in the Col(X)."
  },
  {
    "objectID": "slides/06-mlr-mat.html#geometrical-interpretation-of-least-squares-1",
    "href": "slides/06-mlr-mat.html#geometrical-interpretation-of-least-squares-1",
    "title": "Multiple Linear Regression - Matrix Approach \n",
    "section": "Geometrical Interpretation of Least Squares",
    "text": "Geometrical Interpretation of Least Squares\n\n\n\nThe distance is minimized when the point in the space is the foot of the line from \\(A\\) normal to the space. This is point \\(C\\). \\(\\small {\\bf e} = ({\\bf y - \\hat{\\bf y}}) = ({\\bf y - X b}) = ({\\bf I} - {\\bf H}) {\\bf y} \\perp Col({\\bf X})\\)\n\n\\(\\bf X'(y - Xb) = 0\\)\n\n\nSearching for the LS solution \\(\\bf b\\) that minimizes \\(SS_{res}\\) is the same as locating the point \\({\\bf Xb} \\in Col({\\bf X})\\) that is as close to \\(\\bf y\\) as possible!\n\n\n\n\n\n\nhttps://commons.wikimedia.org/wiki/File:OLS_geometric_interpretation.svg\n\n\n\n\n\n\n\n\n\n(I - H) Now let‚Äôs see what (I - H) is doing.\n\n\nThe residual vector is \\(y - \\hat{y}\\). Geometrically, the residual vector is this dash line vector that is perpendicular to the column space of \\(X\\). In other words,the residual vector is the normal vector of Col(X).\nThis is the result of projection. The distance between \\(y\\) and the column space is minimized when the point in the space is the foot of the line from \\(A\\) normal to the space. This is point \\(C\\).\nWe know that e = (I - H)y. So (I - H) project \\(Y\\) onto the space that is perpendicular to Col(X). And the vector in that space is the residual vector.\n\n\n\nCalculus connects to Linear Algebra"
  },
  {
    "objectID": "slides/06-mlr-mat.html#r-lab-verify-identity",
    "href": "slides/06-mlr-mat.html#r-lab-verify-identity",
    "title": "Multiple Linear Regression - Matrix Approach \n",
    "section": "\nR Lab Verify Identity",
    "text": "R Lab Verify Identity\n\n\n\\({\\bf H} = {\\bf X} ({\\bf X}' {\\bf X}) ^{-1} \\bf X'\\)\n\nH <- X %*% solve(t(X) %*% X) %*% t(X)\n\n\\(\\hat{\\bf y} = \\bf H \\bf y\\)\n\nfitted_y <- H %*% y\nfitted_y[1:4]\n\n[1] 22 10 12 10\n\ndelivery_lm$fitted.values[1:4]\n\n 1  2  3  4 \n22 10 12 10 \n\n\n\n\\(\\bf e = (\\bf I - \\bf H) \\bf y\\)\n\nn <- length(y)\nI <- diag(n)\nres <- (I - H) %*% y\nres[1:4]\n\n[1] -5.03  1.15 -0.05  4.92\n\ndelivery_lm$residuals[1:4]\n\n    1     2     3     4 \n-5.03  1.15 -0.05  4.92 \n\n## residual vector in the left null space of X\nt(X) %*% res\n\n             [,1]\n1         3.9e-13\ncases    -1.4e-12\ndistance -6.0e-11"
  },
  {
    "objectID": "slides/06-mlr-mat.html#multivariate-normal-distribution",
    "href": "slides/06-mlr-mat.html#multivariate-normal-distribution",
    "title": "Multiple Linear Regression - Matrix Approach \n",
    "section": "Multivariate Normal Distribution",
    "text": "Multivariate Normal Distribution\n\n\\({\\bf y} \\sim N(\\boldsymbol \\mu, {\\bf \\Sigma})\\), and \\({\\bf Z = By + c}\\) with a constant matrix \\({\\bf B}\\) and vector \\(\\bf c\\), then \\[{\\bf Z} \\sim N({\\bf B\\boldsymbol \\mu}, {\\bf B \\Sigma B}')\\]\n\n\\({\\bf b} = ({\\bf X}' {\\bf X}) ^{-1} \\bf X' \\bf y\\)\n\\[\\textbf{b} \\sim N\\left(\\boldsymbol \\beta, \\sigma^2 {\\bf (X'X)}^{-1} \\right)\\] \\[E(\\textbf{b}) = E\\left[ {\\bf (X'X)}^{-1} {\\bf X}' {\\bf y}\\right] = \\boldsymbol \\beta\\] \\[\\mathrm{Cov}(\\textbf{b}) = \\mathrm{Cov}\\left[{\\bf (X'X)}^{-1} {\\bf X}' {\\bf y} \\right] = \\sigma^2 {\\bf (X'X)}^{-1}\\]\n\nThe standard error of \\(b_j\\) is \\({\\sqrt{s^2C_{jj}}}\\), where \\(C_{jj}\\) is the diagonal element of \\(({\\bf X'X})^{-1}\\) corresponding to \\(b_j\\).\n\nIf \\({\\bf y} \\sim N_n(\\bsmu, {\\bf \\Sigma})\\), and \\({\\bf Z = By + c}\\) with a constant vector \\(\\bf c\\), then \\[{\\bf Z} \\sim N({\\bf B\\bsmu}, {\\bf B \\Sigma B}')\\]. \\[\\mathrm{Var}(\\textbf{b}) = E\\left[(\\textbf{b} - E(\\textbf{b}))(\\textbf{b}-E(\\textbf{b}))'\\right] = \\mathrm{Var}\\left[\\bf{(X'X)^{-1}X'y}\\right] = \\sigma^2 \\bf (X'X)^{-1}\\]\n\nThe standard error of \\(b_j\\) is \\({\\sqrt{\\hat{\\sigma}^2C_{jj}}}\\), where \\(C_{jj}\\) is the diagonal element of \\(({\\bf X'X})^{-1}\\) corresponding to \\(b_j\\)."
  },
  {
    "objectID": "slides/06-mlr-mat.html#reduced-model-vs.-full-model",
    "href": "slides/06-mlr-mat.html#reduced-model-vs.-full-model",
    "title": "Multiple Linear Regression - Matrix Approach \n",
    "section": "Reduced Model vs.¬†Full Model",
    "text": "Reduced Model vs.¬†Full Model\n\nOverall test of significance: all predictors vs.¬†Marginal \\(t\\)-test: one single predictor\nHow to test any subset of predictors?\n\n\n\n\nFull Model: \\(y = \\beta_0 + \\beta_1x_1+\\beta_2x_2 + \\beta_3x_3 + \\beta_4x_4 + \\epsilon\\)\n\n\\(H_0: \\beta_{2} = \\beta_{4} = 0\\)\n\n\n\n\n\nReduced Model (under \\(H_0\\)): \\(y = \\beta_0 + \\beta_1x_1 + \\beta_3x_3 + \\epsilon\\)\n\nLike to see if \\(x_2\\) and \\(x_4\\) can contribute significantly to the regression model when \\(x_1\\) and \\(x_3\\) are in the model.\n\nIf yes, \\(\\beta_{2} \\ne 0\\) and/or \\(\\beta_{4} \\ne 0\\). (Reject \\(H_0\\))\nOtherwise, \\(\\beta_{2} = \\beta_{4} = 0\\). (Do not reject \\(H_0\\))\n\n\n\n\n\n\nGiven \\(x_1\\) and \\(x_3\\) in the model, we examine how much extra \\(SS_R\\) is increased ( \\(SS_{res}\\) is reduced) if \\(x_2\\) and \\(x_4\\) are added to the model.\n\n\nIf much more extra \\(SS_R\\) is increased after \\(x_2\\) and \\(x_4\\) are included in the model, meaning that lots of variation in \\(y\\) that were treated as unexplained variation or variation that cannot be explained by the \\(x_1\\) and \\(x_3\\), now are absorbed or explained by \\(x_2\\) and \\(x_4\\).\n\n\\(x_2\\) and \\(x_4\\) provide some important and valuable information that \\(x_1\\) and \\(x_3\\) cannot provide, and that information helps us better predict response values and explain the variation of \\(Y\\).\nSo if we observe a significant increase in SS_R, it means that \\(x_2\\) and \\(x_4\\) are valuable and their coefficient is significantly non-zero. Therefore, we should keep the full model that includes all predictors, or reject \\(H_0\\) because the reduced model is too simple, and lose lots of useful information for explaining variation in \\(Y\\)."
  },
  {
    "objectID": "slides/06-mlr-mat.html#extra-sum-of-sqaures",
    "href": "slides/06-mlr-mat.html#extra-sum-of-sqaures",
    "title": "Multiple Linear Regression - Matrix Approach \n",
    "section": "Extra Sum-of-sqaures",
    "text": "Extra Sum-of-sqaures\n\nFull Model: \\(\\bf y = X\\boldsymbol \\beta+\\boldsymbol \\epsilon\\)\nPartition coefficient vector:\n\n\\[\\boldsymbol \\beta_{p \\times 1} = \\left[\n\\begin{array}{c}\n  \\boldsymbol \\beta_1 \\\\\n  \\hline\n  \\boldsymbol \\beta_2\n\\end{array}\n\\right]\\]\nwhere \\(\\boldsymbol \\beta_1\\) is \\((p-r) \\times 1\\) and \\(\\boldsymbol \\beta_2\\) is \\(r \\times 1\\)\n\nTest \\(H_0: \\boldsymbol \\beta_2 = {\\bf 0}\\), \\(H_1: \\boldsymbol \\beta_2 \\ne {\\bf 0}\\)\nExample:  \\(p=5\\), \\(r=2\\), \\(\\boldsymbol \\beta_1 = (\\beta_0, \\beta_1, \\beta_3)'\\), \\(\\boldsymbol \\beta_2 = (\\beta_2, \\beta_4)'\\).\n\n\nFor simplicity, we can partition the coefficient vector to beta_1 and beta_2, where beta_2 is the subset we‚Äôd like to test.\n\\(1 \\le r \\le k\\)\n\n\\[{\\bf y} = {\\bf X} \\boldsymbol \\beta+\\boldsymbol \\epsilon= {\\bf X}_1\\boldsymbol \\beta_1 + {\\bf X}_2\\boldsymbol \\beta_2 + \\boldsymbol \\epsilon\\]\n\n\\(n \\times (p-r)\\) matrix \\({\\bf X}_1\\): the columns of \\(\\bf X\\) associated with \\(\\boldsymbol \\beta_1\\)\n\\(n \\times r\\) matrix \\({\\bf X}_2\\): the columns of \\(\\bf X\\) associated with \\(\\boldsymbol \\beta_2\\)"
  },
  {
    "objectID": "slides/06-mlr-mat.html#extra-sum-of-sqaures-1",
    "href": "slides/06-mlr-mat.html#extra-sum-of-sqaures-1",
    "title": "Multiple Linear Regression - Matrix Approach \n",
    "section": "Extra Sum-of-sqaures",
    "text": "Extra Sum-of-sqaures\n\n\nFull Model: \\(\\small \\color{red}{{\\bf y} = {\\bf X}\\boldsymbol \\beta+\\boldsymbol \\epsilon= {\\bf X}_1\\boldsymbol \\beta_1 + {\\bf X}_2\\boldsymbol \\beta_2 + \\boldsymbol \\epsilon}\\)\n\n\n\\({\\bf b} = ({\\bf X}' {\\bf X}) ^{-1} {\\bf X}' {\\bf y}\\) \n\n\n\nReduced Model: \\(\\small \\color{red}{{\\bf y} = {\\bf X}_1\\boldsymbol \\beta_1+\\boldsymbol \\epsilon}\\) \\(\\color{red}{(\\boldsymbol \\beta_2 = {\\bf 0})}\\)\n\n\n\\({\\bf b}_1 = ({\\bf X}_1' {\\bf X}_1) ^{-1} {\\bf X}_1' {\\bf y}\\) \n\n\n\n\n\nTo find the contribution of the terms in \\(\\boldsymbol \\beta_2\\) to the regression, fit the model assuming that \\(H_0: \\bf \\boldsymbol \\beta_2 = 0\\) is true. \n\n\n\nThe \\(SS_R\\) due to \\(\\boldsymbol \\beta_2\\) given that \\(\\boldsymbol \\beta_1\\) is in the model is \\[\\begin{align} SS_R(\\boldsymbol \\beta_2|\\boldsymbol \\beta_1) &= SS_R(\\boldsymbol \\beta) - SS_R(\\boldsymbol \\beta_1)\\\\ &= SS_R(\\boldsymbol \\beta_1, \\boldsymbol \\beta_2) - SS_R(\\boldsymbol \\beta_1) \\end{align}\\] with \\(r\\) dfs.\nThis is the extra sum of squares due to \\(\\boldsymbol \\beta_2\\).\nIt measures the increase in the \\(SS_R\\) that results from adding regressors in \\({\\bf X}_2\\) to the model that already contains regressors in \\({\\bf X}_1\\).\n\n\nThe regression sum of squares due to \\(\\boldsymbol \\beta_2\\) given that \\(\\boldsymbol \\beta_1\\) is in the model is \\[SS_R(\\boldsymbol \\beta_2|\\boldsymbol \\beta_1) = SS_R(\\boldsymbol \\beta) - SS_R(\\boldsymbol \\beta_1)\\] with \\(p - (p-r) = r\\) dfs.\nThis is extra sum of squares due to \\(\\boldsymbol \\beta_2\\).\nIt measures the increase in the \\(SS_R\\) that results from adding \\(x_{k-r+1}, x_{k-r+2}, \\dots, x_k\\) to the model that already contains \\(x_{1}, x_{2}, \\dots, x_{k-r}\\).\nTo find the contribution of the terms in \\(\\boldsymbol \\beta_2\\) to the regression, fit the model assuming that \\(H_0: \\bf \\boldsymbol \\beta_2 = 0\\) is true."
  },
  {
    "objectID": "slides/06-mlr-mat.html#partial-f-test",
    "href": "slides/06-mlr-mat.html#partial-f-test",
    "title": "Multiple Linear Regression - Matrix Approach \n",
    "section": "Partial \\(F\\)-test",
    "text": "Partial \\(F\\)-test\n\n\\(F_{test} = \\frac{SS_R(\\boldsymbol \\beta_2|\\boldsymbol \\beta_1)/r}{SS_{res}(\\boldsymbol \\beta)/(n-p)} = \\frac{MS_R(\\boldsymbol \\beta_2|\\boldsymbol \\beta_1)}{MS_{res}}\\)\nUnder \\(H_0\\) that \\(\\boldsymbol \\beta_2 = \\bf 0\\), \\(F_{test} \\sim F_{r, n-p}\\). \\((p = k+1)\\).\nReject \\(H_0\\) if \\(F_{test} > F_{\\alpha, r, n-p}\\).\n\n\n\nGiven the regressors of \\({\\bf X}_1\\) are in the model,\n\nIf the regressors of \\({\\bf X}_2\\) contribute much, \\(SS_R(\\boldsymbol \\beta_2|\\boldsymbol \\beta_1)\\) will be large.\nA large \\(SS_R(\\boldsymbol \\beta_2|\\boldsymbol \\beta_1)\\) implies a large \\(F_{test}\\).\nA large \\(F_{test}\\) tends to reject \\(H_0\\), and conclude that \\(\\boldsymbol \\beta_2 \\ne \\bf 0\\).\n\\(\\boldsymbol \\beta_2 \\ne \\bf 0\\) means the regressors of \\({\\bf X}_2\\) provide additional explanatory and prediction power that \\({\\bf X}_1\\) cannot provide."
  },
  {
    "objectID": "slides/06-mlr-mat.html#example-delivery-data",
    "href": "slides/06-mlr-mat.html#example-delivery-data",
    "title": "Multiple Linear Regression - Matrix Approach \n",
    "section": "Example: Delivery Data",
    "text": "Example: Delivery Data\n\n\\(H_0: \\beta_2 = 0 \\qquad H_1: \\beta_2 \\ne 0\\)\nFull model: \\(y = \\beta_0 + \\beta_1x_1+\\beta_2x_2+\\epsilon\\)\n\n\n\n\nWhat is the reduced model?\n\n\n\n\nReduced model: \\(y = \\beta_0 + \\beta_1x_1 +\\epsilon\\)\n\n\n\n\n\n\\(SS_R(\\beta_2|\\beta_1, \\beta_0) = SS_R(\\beta_2, \\beta_1, \\beta_0) -SS_R(\\beta_1, \\beta_0)\\)\n\\(F_{test} = \\frac{SS_R(\\beta_2|\\beta_1, \\beta_0)/1}{SS_{res}(\\boldsymbol \\beta)/(25-3)}\\)\nReject \\(H_0\\) if \\(F_{test} > F_{\\alpha, 1, 22}\\)\n\n\n\n\n\nWhen \\(r=1\\), partial \\(F\\)-test is equivalent to marginal \\(t\\)-test."
  },
  {
    "objectID": "slides/06-mlr-mat.html#r-lab-extra-sum-of-sqaures-approach",
    "href": "slides/06-mlr-mat.html#r-lab-extra-sum-of-sqaures-approach",
    "title": "Multiple Linear Regression - Matrix Approach \n",
    "section": "\nR Lab Extra Sum-of-sqaures Approach",
    "text": "R Lab Extra Sum-of-sqaures Approach\n\nfull_lm <- delivery_lm\nreduced_lm <- lm(time ~ cases, data = delivery_data)\nanova(reduced_lm, full_lm)\n\nAnalysis of Variance Table\n\nModel 1: time ~ cases\nModel 2: time ~ cases + distance\n  Res.Df RSS Df Sum of Sq    F  Pr(>F)    \n1     23 402                              \n2     22 234  1       168 15.8 0.00063 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\nsumm_delivery$coefficients\n\n            Estimate Std. Error t value Pr(>|t|)\n(Intercept)    2.341     1.0967     2.1  4.4e-02\ncases          1.616     0.1707     9.5  3.3e-09\ndistance       0.014     0.0036     4.0  6.3e-04"
  },
  {
    "objectID": "slides/06-mlr-mat.html#standardized-regression-coefficients",
    "href": "slides/06-mlr-mat.html#standardized-regression-coefficients",
    "title": "Multiple Linear Regression - Matrix Approach \n",
    "section": "Standardized Regression Coefficients",
    "text": "Standardized Regression Coefficients\n\n\\(\\hat{y} = 5 + x_1 + 1000x_2\\). Can we say the effect of \\(x_2\\) on \\(y\\) is 1000 times larger than the effect of \\(x_1\\)?\n\n\n\nNope! If \\(x_1\\) is measured in litres and \\(x_2\\) in millilitres, although \\(b_2\\) is 1000 times larger than \\(b_1\\), both effects on \\(\\hat{y}\\) are identical.\n\n\n\n\n\nThe units of \\(\\beta_j\\) are \\(\\frac{\\text{units of } y}{\\text{units of } x_j}\\)\n\n \\(\\beta_2\\): delivery time (min) \\((y)\\) per distance (ft) walked by the driver \\((x_2)\\). \n\n\nIt is helpful to work with dimensionless or standardized coefficients.\n\ncomparison\nget rid of round-off errors in \\({\\bf (X'X)}^{-1}\\).\n\n\n\n\ninterpretation\nIt is difficult to compare coefficients because the magnitude of \\(b_j\\) reflects the units of measurement of \\(x_j\\).\n\n\n\n\nIn Intro Stats, how do we standardize a variable?"
  },
  {
    "objectID": "slides/06-mlr-mat.html#unit-normal-scaling",
    "href": "slides/06-mlr-mat.html#unit-normal-scaling",
    "title": "Multiple Linear Regression - Matrix Approach \n",
    "section": "Unit Normal Scaling",
    "text": "Unit Normal Scaling\n\n\n\\(z_{ij} = \\frac{x_{ij}-\\overline{x}_j}{s_j}, \\, i = 1, \\dots, n, \\, j = 1, \\dots, k\\), where \\(s_j\\) is the sample SD of \\(x_j\\).\n\n\\(y^*_{i} = \\frac{y_{i}-\\overline{y}}{s_y}, \\, i = 1, \\dots, n\\), where \\(s_y\\) is the sample SD of \\(y\\).\nThe scaled predictors and response have mean 0 and variance 1.\nThe new model: \\[y_i^* = \\alpha_1z_{i1} + \\alpha_2z_{i2} + \\cdots + \\alpha_kz_{ik} + \\epsilon_i\\]\n\n\n\nWhy no intercept term \\(\\alpha_0\\)?\n\n\n\nThe least-squares estimator for \\(\\boldsymbol \\alpha\\): \\[{\\bf a} = {\\bf (Z'Z)}^{-1} {\\bf Z'y}^*\\]"
  },
  {
    "objectID": "slides/06-mlr-mat.html#r-lab-standardized-coefficients",
    "href": "slides/06-mlr-mat.html#r-lab-standardized-coefficients",
    "title": "Multiple Linear Regression - Matrix Approach \n",
    "section": "\nR Lab Standardized Coefficients",
    "text": "R Lab Standardized Coefficients\n\n## unit normal scaling\nscale_data <- scale(delivery_data, center = TRUE, scale = TRUE)  ## becomes a matrix\napply(scale_data, 2, mean)\n\n    time    cases distance \n-6.2e-17  8.9e-18  9.8e-17 \n\napply(scale_data, 2, var)\n\n    time    cases distance \n       1        1        1 \n\n\n\n\n## No-intercept\nscale_lm <- lm(time ~ cases + distance - 1, data = as.data.frame(scale_data)) \nscale_lm$coefficients\n\n   cases distance \n    0.72     0.30 \n\n## With intercept\nscale_lm_0 <- lm(time ~ cases + distance, data = as.data.frame(scale_data))  \nscale_lm_0$coefficients\n\n(Intercept)       cases    distance \n   -3.3e-17     7.2e-01     3.0e-01 \n\n\n\n\n\n\n\n\nmath4780-f23.github.io/website"
  },
  {
    "objectID": "slides/13-cat-var.html#categorical-variables",
    "href": "slides/13-cat-var.html#categorical-variables",
    "title": "Categorical Variables üõ†",
    "section": "Categorical Variables",
    "text": "Categorical Variables\n\nExamine the relationship between numerical response and categorical predictors. \n\n\n Gender (Female üë©, Male üë®, Other üè≥Ô∏è‚Äçüåà) : Gender income/wage gap\n\n Country (USA üá∫üá∏, Canada üá®üá¶, UK üá¨üáß, Germany üá©üá™, Japan üáØüáµ, Korea üá∞üá∑) : Meat consumption level\n\n Political Party (Republican üî¥, Democratic üîµ, Other ‚ö´) : Donation to healthcare\n\n\n\n\nIn regression, we may also want to examine the relationship between numerical response and categorical (qualitative) predictors.\nWhen a categorical variable has many levels/categories, they‚Äôre encoded to dummy variables or indicator variables.\n\n Gender has two levels: female and male \n Blood Type: A, B, AB and O \n Education: high school, bachelor, master, doctoral \n\n\nIn reality there are lots of categorical variables, especially in social science studies.\nCategorical variables are equally important as numerical variables."
  },
  {
    "objectID": "slides/13-cat-var.html#categorical-variable-in-regression",
    "href": "slides/13-cat-var.html#categorical-variable-in-regression",
    "title": "Categorical Variables üõ†",
    "section": "Categorical Variable in Regression",
    "text": "Categorical Variable in Regression\n\n\n\n\n\n\n\n\n\n\n\n\n\nMay draw inappropriate height-weight relationship if gender factor is ignored.\nBecause female tend to be shorter and weight less.\nMoreover, the weight increased may be at different rate in gender.\n\n\n\nWith categorical variable Gender\n\n\n\n\n\n\n\n\n\n\n\n\n\nInappropriate height-weight relationship if gender factor is ignored.\nThe two groups have different \\(\\beta_0\\) and \\(\\beta_1\\).\n\n\n\nA more appropriate regression model should include the gender variable when we explain the relationship between height and weight.\nThe two groups in general will have different \\(\\beta_0\\) and \\(\\beta_1\\). And the difference is generally quite significant.\nThis example tells us not only numerical variables, it is also important to include relevant categorical variables that greatly affect the response of interest."
  },
  {
    "objectID": "slides/13-cat-var.html#example-8.1-tool-life-data-lra",
    "href": "slides/13-cat-var.html#example-8.1-tool-life-data-lra",
    "title": "Categorical Variables üõ†",
    "section": "Example 8.1 Tool Life Data (LRA)",
    "text": "Example 8.1 Tool Life Data (LRA)\nRelate the effective life (hours) of a cutting tool \\((y)\\) used on a lathe to\n\nthe lathe speed in revolutions per minute \\((x_1)\\) (Numerical)\nthe type of cutting tool used \\((x_2)\\) (Categorical)\n\n\n\n\n\n\n   hours speed type\n1     19   610    A\n2     15   950    A\n3     17   720    A\n4     15   840    A\n5     13   980    A\n6     24   530    A\n7     13   680    A\n8     23   540    A\n9     13   890    A\n10    19   730    A\n11    30   670    B\n12    27   770    B\n13    25   880    B\n14    26  1000    B\n15    33   760    B\n16    36   590    B\n17    26   910    B\n18    37   650    B\n19    35   810    B\n20    44   500    B\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nScatterplot of life against speed without considering the type of cutting tool.\nWe may think the distance between data points and the regression fit is a pure random noise, and their variation is that big.\nBut in fact those variations may be due to some other variables that are not included in the model, like the type of the tool."
  },
  {
    "objectID": "slides/13-cat-var.html#indicator-variable",
    "href": "slides/13-cat-var.html#indicator-variable",
    "title": "Categorical Variables üõ†",
    "section": "Indicator Variable",
    "text": "Indicator Variable\n\n\n\nTool type can be represented as: \\[x_2 = \\begin{cases} 0  & \\quad \\text{Tool type A}\\\\ 1  & \\quad \\text{Tool type B} \\end{cases}\\] where \\(x_2\\) is a dummy variable.\nIf a first-order model is appropriate: \\[y = \\beta_0+\\beta_1x_1+\\beta_2x_2 + \\epsilon, \\quad \\epsilon \\sim N(0, \\sigma^2)\\]\n\nAssume that the variance is the same for both levels (type A and B).\n\n\n\n\n\n\n\n\n\n\n\n\n\nWhen we label data points by their tool type, we can clearly see that given the same level of the speed, type B tends to have longer life than type A.\nSo if we simply ignore the tool type, and predict the tool life based solely on the speed, we are gonna have pretty large prediction error.\nOur fitted line is sort of between the two types, so we underestimates the life of tool B, and overestimate the life of tool A.\nTo model and describe the effect of the categorical variable, we use a so-called dummy variable, or indicator variable.\nFor example here, we consider two different types of cutting tool, A and B.\n\\(x_2 = \\begin{cases} 0 & \\quad \\text{Tool type A}\\\\ 1 & \\quad \\text{Tool type B} \\end{cases}\\)\nIf the categorical variable has two categories, we need one dummy variable.\nIn general, if a categorical variable has \\(k\\) categories, we are gonna need \\(k-1\\) dummy variables put in the regression model.\n\nfirst-order means no interaction terms as well as higher polynomial orders."
  },
  {
    "objectID": "slides/13-cat-var.html#interpretation-of-coefficients",
    "href": "slides/13-cat-var.html#interpretation-of-coefficients",
    "title": "Categorical Variables üõ†",
    "section": "Interpretation of Coefficients",
    "text": "Interpretation of Coefficients\n\nFor Tool type A \\((x_2 = 0)\\) the model becomes: \\[\\begin{align} y &= \\beta_0+\\beta_1x_1+\\beta_2(0) + \\epsilon \\\\ &= \\beta_0+\\beta_1x_1+ \\epsilon \\end{align}\\]\n\nFor Tool type B \\((x_2 = 1)\\) the model becomes: \\[\\begin{align} y &= \\beta_0+\\beta_1x_1+\\beta_2(1) + \\epsilon \\\\ &= (\\beta_0 + \\beta_2)+\\beta_1x_1+ \\epsilon \\end{align}\\]\n\nChanging from type A to B induces a change in the intercept, but the slope is unchanged and identical.\nType A is the baseline level.\n\n\nChanging from type A to B induces a change in the intercept (slope is unchanged and identical).\nThe model includes the response shifting effect.\nAssume that the variance is the same for both levels (type A and B) of the categorical variable.\nType A is the baseline level. The baseline level is the level corresponding to the dummy variable being zero.\nThe original intercept term is for the baseline level."
  },
  {
    "objectID": "slides/13-cat-var.html#parallel-regression-lines",
    "href": "slides/13-cat-var.html#parallel-regression-lines",
    "title": "Categorical Variables üõ†",
    "section": "Parallel Regression Lines",
    "text": "Parallel Regression Lines\n\nTwo parallel regression lines with a common slope \\(\\beta_1\\) and different intercepts.\n\n\\(\\beta_2\\) measures the difference in mean tool life resulting from changing from tool type A to B.\n\n\n\n\\(y = \\beta_0+\\beta_1x_1+\\beta_2x_2 + \\epsilon\\)\n\n\n\n\n\n\n\n\n\n\n\\(\\hat{y} = b_0 + b_1 x_1 + b_2 x_2\\)\n\n\n\n\n\n\n\n\n\n\n\n\nTwo parallel regression lines with a common slope \\(\\beta_1\\) and different intercepts.\n\n\\(\\beta_2\\) expresses the difference in heights between the two lines, measuring the difference in mean tool life resulting from changing from tool type A to type B."
  },
  {
    "objectID": "slides/13-cat-var.html#r-lab-model-fitting",
    "href": "slides/13-cat-var.html#r-lab-model-fitting",
    "title": "Categorical Variables üõ†",
    "section": "\nR Lab Model Fitting",
    "text": "R Lab Model Fitting\n\nThe categorical variable should be of type character or factor.\n\n\nstr(tool_data)\n\n'data.frame':   20 obs. of  3 variables:\n $ hours: num  18.7 14.5 17.4 14.5 13.4 ...\n $ speed: num  610 950 720 840 980 530 680 540 890 730 ...\n $ type : chr  \"A\" \"A\" \"A\" \"A\" ...\n\n\n\nfull_model <- lm(hours ~ speed + type, data = tool_data)\nsummary(full_model)$coef\n\n            Estimate Std. Error t value Pr(>|t|)\n(Intercept)   36.986     3.5104    10.5  7.2e-09\nspeed         -0.027     0.0045    -5.9  1.8e-05\ntypeB         15.004     1.3597    11.0  3.6e-09\n\n\n\n\\(\\hat{y} = 37 -0.027x_1 +15x_2\\)\nAll else held constant, type B tools are expected, on average, to have 15 hours longer life than the baseline.\n\n\nThe dummy variable will be created based on the factor level that is created when the categorical variable is transformed to be of type factor.\nBy default, the factor level is created in alphabetical order.\nCan specify level using argument levels in factor() function.\nIf factor has been created, can use relevel() to relevel the factor.\n\n\n\n  B\nA 0\nB 1"
  },
  {
    "objectID": "slides/13-cat-var.html#model-checking",
    "href": "slides/13-cat-var.html#model-checking",
    "title": "Categorical Variables üõ†",
    "section": "Model Checking",
    "text": "Model Checking\n\n\nSame variance of the errors for both A and B?\n\n\n\n\n\n\n\n\n\nApproximately normal"
  },
  {
    "objectID": "slides/13-cat-var.html#single-model-vs.-separate-models",
    "href": "slides/13-cat-var.html#single-model-vs.-separate-models",
    "title": "Categorical Variables üõ†",
    "section": "Single Model vs.¬†Separate Models",
    "text": "Single Model vs.¬†Separate Models\n\nTwo separate models, one for each type, could have been fit to the data. \\[y^A = \\beta_0^A+\\beta_1x_1^A+ \\epsilon^A, \\quad \\epsilon^A \\sim N(0, \\sigma^2)\\] \\[y^B = \\beta_0^B+\\beta_1x_1^B+ \\epsilon^B, \\quad \\epsilon^B \\sim N(0, \\sigma^2)\\]\n\n\n\nIf performing well, the single-model approach with dummy variables is preferred.\n\n\n\nOnly one equation to work with, \\(y = \\beta_0+\\beta_1x_1+\\beta_2x_2 + \\epsilon\\), a simpler practical result.\n\n\n\n\nBoth lines are assumed to have the same slope \\(\\beta_1\\) and error variance \\(\\sigma^2\\).\n\nCombine the data to produce a single estimate of the common parameters.\nUse more data to estimate the parameters, and the estimation quality would be better.\n\n\n\n\nSince two regression lines are expected to model the relationship between tool life and lathe speed, we could have fit two separate models with no dummy variables to the data."
  },
  {
    "objectID": "slides/13-cat-var.html#difference-in-slope",
    "href": "slides/13-cat-var.html#difference-in-slope",
    "title": "Categorical Variables üõ†",
    "section": "Difference in Slope",
    "text": "Difference in Slope\n\nIf we expect the slopes to differ, include an interaction term between the variables: \\[y = \\beta_0+\\beta_1x_1+\\beta_2x_2 + \\color{blue}{\\beta_3x_1x_2} + \\epsilon\\]\n\n\n\n\nTool type A \\((x_2 = 0)\\): \\[\\begin{align} y &= \\beta_0+\\beta_1x_1+\\beta_2(0) + \\beta_3x_1(0) + \\epsilon \\\\ &= \\beta_0+\\beta_1x_1+ \\epsilon \\end{align}\\]\n\nTool type B \\((x_2 = 1)\\): \\[\\begin{align} y &= \\beta_0+\\beta_1x_1+\\beta_2(1) + \\beta_3x_1(1) + \\epsilon \\\\ &= (\\beta_0+ \\beta_2) + (\\beta_1+\\beta_3)x_1+ \\epsilon \\end{align}\\]\n\n\n\\(\\beta_2\\) is the change in the intercept caused by changing from type A to type B.\n\n\\(\\beta_3\\) is the change in the slope caused by changing from type A to type B.\n\n\n\n\nIf we expect the slopes to differ, we can model this phenomenon by including an interaction term between the variables.\nBack to the tool life data, and say we believe there may be different slopes for the two tools. The model we can fit to account for the change in slope is: \\[y = \\beta_0+\\beta_1x_1+\\beta_2x_2 + \\beta_3x_1x_2 + \\epsilon\\]\n\nTool type A : \\[\\begin{align} y &= \\beta_0+\\beta_1x_1+\\beta_2(0) + \\beta_3x_1(0) + \\epsilon \\\\ &= \\beta_0+\\beta_1x_1+ \\epsilon \\end{align}\\]\n\nTool type B : \\[\\begin{align} y &= \\beta_0+\\beta_1x_1+\\beta_2(1) + \\beta_3x_1(1) + \\epsilon \\\\ &= (\\beta_0+ \\beta_2) + (\\beta_1+\\beta_3)x_1+ \\epsilon \\end{align}\\]"
  },
  {
    "objectID": "slides/13-cat-var.html#response-function-for-the-tool-life-example",
    "href": "slides/13-cat-var.html#response-function-for-the-tool-life-example",
    "title": "Categorical Variables üõ†",
    "section": "Response Function for the Tool Life Example",
    "text": "Response Function for the Tool Life Example\n\n\n\\(y = \\beta_0+\\beta_1x_1+\\beta_2x_2 + \\beta_3x_1x_2 + \\epsilon\\) defines two regression lines with different slopes and intercepts.   \n\n\n\n\n\\(y = \\beta_0+\\beta_1x_1+\\beta_2x_2 + \\beta_3x_1x_2+\\epsilon\\)\n\n\n\n\n\n\n\n\n\n\n\\(\\hat{y} = b_0 + b_1 x_1 + b_2x_2 + b_3 x_1 x_2\\)\n\n\n\n\n\n\n\n\n\n\n\n\nType A is the baseline level"
  },
  {
    "objectID": "slides/13-cat-var.html#two-models",
    "href": "slides/13-cat-var.html#two-models",
    "title": "Categorical Variables üõ†",
    "section": "Two Models",
    "text": "Two Models\n\nThe model \\(y = \\beta_0+\\beta_1x_1+\\beta_2x_2 + \\beta_3x_1x_2 + \\epsilon\\) is equivalent to fitting two separate regressions:\n\n\\(y = \\beta_0+\\beta_1x_1+ \\epsilon\\)\n\n\\(y = \\alpha_0 + \\alpha_1x_1+ \\epsilon\\), \\(\\quad \\alpha_0 = \\beta_0 + \\beta_2\\), \\(\\quad \\alpha_1 = \\beta_1 + \\beta_3\\).\n\n\n\n\nHow do we test if the 2 regressions are identical?\n\n\n\nCan use the extra sum of squares method by comparing the full and reduced models.\n\\(H_0: \\beta_2 = \\beta_3 = 0 \\quad H_1: \\beta_2 \\ne 0 \\text{ and(or) } \\beta_3 \\ne 0\\)\n\n\ntest if the categorical variable affect intercept and or slope.\nIn other words, we can test if the tool type affects the effect of tool speed on the tool life."
  },
  {
    "objectID": "slides/13-cat-var.html#r-lab-regression-model-with-interaction",
    "href": "slides/13-cat-var.html#r-lab-regression-model-with-interaction",
    "title": "Categorical Variables üõ†",
    "section": "\nR Lab Regression Model with Interaction",
    "text": "R Lab Regression Model with Interaction\n\n(full_model <- lm(hours ~ speed + type + speed:type, data = tool_data))\n\n\nCall:\nlm(formula = hours ~ speed + type + speed:type, data = tool_data)\n\nCoefficients:\n(Intercept)        speed        typeB  speed:typeB  \n    32.7748      -0.0210      23.9706      -0.0119  \n\n\n\n\n(full_model <- lm(hours ~ speed*type, data = tool_data))\n\n\nCall:\nlm(formula = hours ~ speed * type, data = tool_data)\n\nCoefficients:\n(Intercept)        speed        typeB  speed:typeB  \n    32.7748      -0.0210      23.9706      -0.0119  \n\n\n\n\nThe fitted model is \\[\\hat{y} = 32.8-0.02x_1+23.97x_2 -0.01x_1x_2\\]"
  },
  {
    "objectID": "slides/13-cat-var.html#r-lab-regression-lines",
    "href": "slides/13-cat-var.html#r-lab-regression-lines",
    "title": "Categorical Variables üõ†",
    "section": "\nR Lab Regression Lines",
    "text": "R Lab Regression Lines\n\n\n\\(y = \\beta_0+\\beta_1x_1+\\beta_2x_2 + \\epsilon\\)\n\n\n\n\n\n\n\n\n\n\\(y = \\beta_0+\\beta_1x_1+\\beta_2x_2 + \\beta_3x_1x_2+\\epsilon\\)"
  },
  {
    "objectID": "slides/13-cat-var.html#r-lab-test-effect-of-tool-type",
    "href": "slides/13-cat-var.html#r-lab-test-effect-of-tool-type",
    "title": "Categorical Variables üõ†",
    "section": "\nR Lab Test Effect of Tool Type",
    "text": "R Lab Test Effect of Tool Type\n\n\\(H_0: \\beta_2 = \\beta_3 = 0 \\quad H_1: \\beta_2 \\ne 0 \\text{ and(or) } \\beta_3 \\ne 0\\)\n\n\nreduced_model <- lm(hours ~ speed, data = tool_data)\nanova(reduced_model, full_model)\n\nAnalysis of Variance Table\n\nModel 1: hours ~ speed\nModel 2: hours ~ speed * type\n  Res.Df  RSS Df Sum of Sq    F  Pr(>F)    \n1     18 1282                              \n2     16  141  2      1141 64.8 2.1e-08 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n\\(F_{test} = \\frac{SS_R(\\beta_2, \\beta_3 |\\beta_1, \\beta_0)/2}{MS_{res}} = \\frac{1141/2}{141/16} = 64.75 > F_{\\alpha, 2, 20-4}\\)\nThe two regression lines are not identical."
  },
  {
    "objectID": "slides/13-cat-var.html#more-than-2-categories",
    "href": "slides/13-cat-var.html#more-than-2-categories",
    "title": "Categorical Variables üõ†",
    "section": "More than 2 Categories",
    "text": "More than 2 Categories\n\nFor a categorical predictor with \\(m\\) levels, we need \\(m-1\\) dummies.\nThree tool types, A, B, and C. Then two indicators \\(x_2\\) and \\(x_3\\) will be needed:\n\n\n\nTool Type\n\\(x_2\\)\n\\(x_3\\)\n\n\n\nA\n0\n0\n\n\nB\n1\n0\n\n\nC\n0\n1\n\n\n\n\nThe regression model (common slope) is \\[y = \\beta_0+\\beta_1x_1+\\beta_2x_2 + \\beta_3x_3 + \\epsilon\\]\n\nType A is the baseline level.\n\n\nType A is the baseline level. The base line level is the category corresponding value zero for all dummies.\n\n\\(\\beta_0\\) is the intercept when A is used.\n\n\\(\\beta_2\\) is the change in the intercept when the tool changes from A to B.\n\n\\(\\beta_3\\) is the change in the intercept when the tool changes from A to C.\nWhat is the change in the intercept when the tool changes from B to C."
  },
  {
    "objectID": "slides/13-cat-var.html#example-8.3-more-than-2-levels-lra",
    "href": "slides/13-cat-var.html#example-8.3-more-than-2-levels-lra",
    "title": "Categorical Variables üõ†",
    "section": "Example 8.3: More Than 2 Levels (LRA)",
    "text": "Example 8.3: More Than 2 Levels (LRA)\n\nAn electric utility is investigating the effect of the size of a single family house \\((x_1)\\) and the type of air conditioning used on the total electricity consumption \\((y)\\).\n\n\n\nType of Air Conditioning\n\\(x_2\\)\n\\(x_3\\)\n\\(x_4\\)\n\n\n\nNo air conditioning\n0\n0\n0\n\n\nWindow units\n1\n0\n0\n\n\nHeat pump\n0\n1\n0\n\n\nCentral air conditioning\n0\n0\n1\n\n\n\n\nWhich type is the baseline level?\n\n\nAn electric utility is investigating the effect of the size of a single family house \\((x_1)\\) and the type of air conditioning used in the house on the total electricity consumption during summer months \\((y)\\).\nSince there are 4 categories, 3 dummy variables are used to differentiate their effect on \\(y\\).\nNo air conditioning is the baseline level"
  },
  {
    "objectID": "slides/13-cat-var.html#example-8.3-dummy-variables",
    "href": "slides/13-cat-var.html#example-8.3-dummy-variables",
    "title": "Categorical Variables üõ†",
    "section": "Example 8.3: Dummy Variables",
    "text": "Example 8.3: Dummy Variables\nThe regression model is \\(y = \\beta_0+\\beta_1x_1+\\beta_2x_2 + \\beta_3x_3 + \\beta_4x_4 + \\epsilon\\)\n\n2 predictors, but 4 x‚Äôs, because x2, x3 and x4 are for the variable Type of Air\n\n\n\n\n‚ÄúNo air conditioning‚Äù is the baseline level.\n\n\n\nType of Air Conditioning\n\\(x_2\\)\n\\(x_3\\)\n\\(x_4\\)\n\n\n\nNo air conditioning\n0\n0\n0\n\n\nWindow units\n1\n0\n0\n\n\nHeat pump\n0\n1\n0\n\n\nCentral air conditioning\n0\n0\n1\n\n\n\n\n\nIf the house has\n\nno air conditioning,\n\n\\[y = \\beta_0+\\beta_1x_1 + \\epsilon\\]\n\nwindow units,\n\n\\[y = (\\beta_0 + \\beta_2)+\\beta_1x_1 + \\epsilon\\]\n\na heat pump,\n\n\\[y = (\\beta_0 + \\beta_3) +\\beta_1x_1 + \\epsilon\\]\n\ncentral air conditioning,\n\n\\[y = (\\beta_0 + \\beta_4) +\\beta_1x_1 + \\epsilon\\]\n\n\n\n\nThe type ‚ÄúNo air conditioning‚Äù is the baseline level.\nThe model assumes that the relationship between electricity consumption and the size of the house is linear and the slope does not depend on the type of air conditioning system employed.\nThe parameters Œ≤ 2 , Œ≤ 3 , and Œ≤ 4 modify the height (or intercept) of the regression model for the different types of air conditioning systems.\nThat is, Œ≤ 2 , Œ≤ 3 , and Œ≤ 4 measure the effect of window units, a heat pump, and a central air conditioning system, respectively, compared to no air conditioning.\nFurthermore, other effects can be determined by directly comparing the appropriate regression coefficients. For example, Œ≤ 3 ‚àí Œ≤ 4 reflects the relative efficiency of a heat pump compared to central air conditioning.\nNegative means saving more energy."
  },
  {
    "objectID": "slides/13-cat-var.html#example-8.3-interaction",
    "href": "slides/13-cat-var.html#example-8.3-interaction",
    "title": "Categorical Variables üõ†",
    "section": "Example 8.3: Interaction",
    "text": "Example 8.3: Interaction\n\nDo you think the model \\(y = \\beta_0+\\beta_1x_1+\\beta_2x_2 + \\beta_3x_3 + \\beta_4x_4 + \\epsilon\\) is reasonable?\n\n\n\nIt seems unrealistic to assume that the slope \\(\\beta_1\\) relating mean electricity consumption to the house size does NOT depend on air-conditioning type.\nThe consumption increases with the house size.\nThe rate of increase should be different because a more efficient central air conditioning system should have a consumption rate lower than window units.\nAdd interaction between the house size and the type of air conditioning: \\[y = \\beta_0+\\beta_1x_1+\\beta_2x_2 + \\beta_3x_3 + \\beta_4x_4 + \\beta_5x_1x_2 + \\beta_6 x_1x_3 + \\beta_7 x_1x_4 + \\epsilon\\]\n\n\n\nIt would seem unrealistic to assume that the slope of the regression function relating mean electricity consumption to the size of the house does not depend on the type of air conditioning system.\nWe would expect the mean electricity consumption to increase with the size of the house, but the rate of increase should be different for a central air conditioning system than for window units because central air conditioning should be more efficient than window units for larger houses.\nThere should be an interaction between the size of the house and the type of air conditioning system: \\[y = \\beta_0+\\beta_1x_1+\\beta_2x_2 + \\beta_3x_3 + \\beta_4x_4 + \\beta_5x_1x_2 + \\beta_6 x_1x_3 + \\beta_7 x_1x_4 + \\epsilon\\]\n\nThe assumption that the variance of energy consumption does not depend on the type of air conditioning system used may be inappropriate."
  },
  {
    "objectID": "slides/13-cat-var.html#example-8.3-unique-slope-and-intercept",
    "href": "slides/13-cat-var.html#example-8.3-unique-slope-and-intercept",
    "title": "Categorical Variables üõ†",
    "section": "Example 8.3: Unique Slope and Intercept",
    "text": "Example 8.3: Unique Slope and Intercept\n\\(y = \\beta_0+\\beta_1x_1+\\beta_2x_2 + \\beta_3x_3 + \\beta_4x_4 + \\beta_5x_1x_2 + \\beta_6 x_1x_3 + \\beta_7 x_1x_4 + \\epsilon\\)\n\n\n\n\n‚ÄúNo air conditioning‚Äù is the baseline level.\n\n\n\nType of Air Conditioning\n\\(x_2\\)\n\\(x_3\\)\n\\(x_4\\)\n\n\n\nNo air conditioning\n0\n0\n0\n\n\nWindow units\n1\n0\n0\n\n\nHeat pump\n0\n1\n0\n\n\nCentral air conditioning\n0\n0\n1\n\n\n\n\n\n\nNo air conditioning: \\[y = \\beta_0+\\beta_1x_1 + \\epsilon\\]\nWindow units: \\[y = (\\beta_0 + \\beta_2)+(\\beta_1+\\beta_5)x_1 + \\epsilon\\]\nHeat pump: \\[y = (\\beta_0 + \\beta_3) +(\\beta_1+\\beta_6)x_1 + \\epsilon\\]\nCentral air conditioning: \\[y  = (\\beta_0 + \\beta_4) +(\\beta_1+\\beta_7)x_1 + \\epsilon\\]\n\n\n\n\n\n\n\\(\\beta_5\\) is the effect of window units on the slope, comparing to the slope when no air conditioning is used."
  },
  {
    "objectID": "slides/13-cat-var.html#more-than-one-categorical-variable-model",
    "href": "slides/13-cat-var.html#more-than-one-categorical-variable-model",
    "title": "Categorical Variables üõ†",
    "section": "More Than One Categorical Variable: Model",
    "text": "More Than One Categorical Variable: Model\n\nFrequently several categorical variables must be incorporated into the model.\nSuppose in the cutting tool life example a second categorical variable, the type of cutting oil used, must be considered.\nAssuming that the variable has two levels: \\[x_3 = \\begin{cases} 0  & \\quad \\text{low-viscosity oil used}\\\\ 1  & \\quad \\text{medium-viscosity oil used} \\end{cases}\\]\n\nA regression model relating tool life \\((y)\\) to cutting speed \\((x_1)\\), tool type \\((x_2)\\), and type of cutting oil \\((x_3)\\) is \\[y = \\beta_0+\\beta_1x_1+\\beta_2x_2 + \\beta_3x_3 + \\epsilon\\]\n\n\n\n\n‚ùó The model has the same expression as the model with only one categorical variable having 3 categories. But the meaning is totally different!\n\n\nClearly the slope Œ≤ 1 of the regression model relating tool life to cutting speed does not depend on either the type of tool or the type of cutting oil. The intercept of the regression line depends on these factors in an additive fashion."
  },
  {
    "objectID": "slides/13-cat-var.html#more-than-one-categorical-variable-interaction",
    "href": "slides/13-cat-var.html#more-than-one-categorical-variable-interaction",
    "title": "Categorical Variables üõ†",
    "section": "More Than One Categorical Variable: Interaction",
    "text": "More Than One Categorical Variable: Interaction\n\nAdd interactions between cutting speed \\(x_1\\) and the two categorical variables: \\[y = \\beta_0+\\beta_1x_1+\\beta_2x_2 + \\beta_3x_3 + \\color{blue}{\\beta_4x_1x_2} + \\color{blue}{\\beta_5x_1x_3} + \\epsilon\\]\n\n\n\n\n\n\n\n\n\nTool Type\nCutting Oil\nRegression Model\n\n\n\nA \\(\\small (x_2 = 0)\\)\n\nLow-viscosity \\(\\small (x_3 = 0)\\)\n\n\\(\\small y = \\beta_0+\\beta_1x_1 + \\epsilon\\)\n\n\nB \\(\\small (x_2 = 1)\\)\n\nLow-viscosity \\(\\small (x_3 = 0)\\)\n\n\\(\\small y = (\\beta_0+ \\beta_2) + (\\beta_1+\\beta_4)x_1 + \\epsilon\\)\n\n\nA \\(\\small (x_2 = 0)\\)\n\nMedium-viscosity \\(\\small (x_3 = 1)\\)\n\n\\(\\small y = (\\beta_0+ \\beta_3) + (\\beta_1+\\beta_5)x_1 + \\epsilon\\)\n\n\nB \\(\\small (x_2 = 1)\\)\n\nMedium-viscosity \\(\\small (x_3 = 1)\\)\n\n\\(\\small y = (\\beta_0+ \\beta_2 + \\beta_3) + (\\beta_1+\\beta_4 + \\beta_5)x_1 + \\epsilon\\)\n\n\n\n\nEach combination results in a separate regression line with different slopes and intercepts.\nChanging from low to medium-viscosity cutting oil changes the intercept by \\(\\beta_3\\) and the slope by \\(\\beta_5\\) regardless of the type of tool used.\n\nNo interaction between the 2 categorical variables.\n\n\nEach combination of tool type and cutting oil results in a separate regression line, with different slopes and intercepts.\nChanging from low to medium-viscosity cutting oil changes the intercept by \\(\\beta_3\\) and the slope by \\(\\beta_5\\) regardless of the type of tool used.  The model is still additive with respect to the levels of the indicator variables."
  },
  {
    "objectID": "slides/13-cat-var.html#interaction-between-categorical-variables",
    "href": "slides/13-cat-var.html#interaction-between-categorical-variables",
    "title": "Categorical Variables üõ†",
    "section": "Interaction between Categorical Variables",
    "text": "Interaction between Categorical Variables\n\nAdd a cross-product term involving the two dummy variables \\(x_2\\) and \\(x_3\\) to the model: \\[y = \\beta_0+\\beta_1x_1+\\beta_2x_2 + \\beta_3x_3 + \\beta_4x_1x_2 + \\beta_5x_1x_3 + \\color{blue}{\\beta_6x_2x_3} + \\epsilon\\]\n\n\n\n\n\n\n\n\n\nTool Type\nCutting Oil\nRegression Model\n\n\n\nA \\(\\small (x_2 = 0)\\)\n\nLow-viscosity \\(\\small (x_3 = 0)\\)\n\n\\(\\small y = \\beta_0+\\beta_1x_1 + \\epsilon\\)\n\n\nB \\(\\small (x_2 = 1)\\)\n\nLow-viscosity \\(\\small (x_3 = 0)\\)\n\n\\(\\small y = (\\beta_0+ \\beta_2) + (\\beta_1+\\beta_4)x_1 + \\epsilon\\)\n\n\nA \\(\\small (x_2 = 0)\\)\n\nMedium-viscosity \\(\\small (x_3 = 1)\\)\n\n\\(\\small y = (\\beta_0+ \\beta_3) + (\\beta_1+\\beta_5)x_1 + \\epsilon\\)\n\n\nB \\(\\small (x_2 = 1)\\)\n\nMedium-viscosity \\(\\small (x_3 = 1)\\)\n\n\\(\\small y = (\\beta_0+ \\beta_2 + \\beta_3 + \\beta_6) + (\\beta_1+\\beta_4 + \\beta_5)x_1 + \\epsilon\\)\n\n\n\n\nChanging from low to medium-viscosity cutting oil changes the intercept by \\(\\beta_3\\) if tool type A is used.\nThe same change in cutting oil changes the intercept by \\(\\beta_3 + \\beta_6\\) if tool type B is used.\n\n\nThe addition of the cross-product term \\(\\beta_6x_2x_3\\) results in the effect of one indicator variable on the intercept depending on the level of the other indicator variable."
  },
  {
    "objectID": "slides/13-cat-var.html#comparing-regression-models",
    "href": "slides/13-cat-var.html#comparing-regression-models",
    "title": "Categorical Variables üõ†",
    "section": "Comparing Regression Models",
    "text": "Comparing Regression Models\n\nConsider simple linear regression where the \\(n\\) observations can be formed into \\(M\\) groups, with the \\(m\\)-th group having \\(n_m\\) observations.\nThe most general model consists of \\(M\\) separate equations: \\[y = \\beta_{0m} + \\beta_{1m}x + \\epsilon, \\quad m = 1, 2, \\dots, M\\]\n\nWe are interested in comparing this general model to a more restrictive one."
  },
  {
    "objectID": "slides/13-cat-var.html#parallel-lines-example-8.1-where-m-2",
    "href": "slides/13-cat-var.html#parallel-lines-example-8.1-where-m-2",
    "title": "Categorical Variables üõ†",
    "section": "Parallel Lines (Example 8.1 where \\(M = 2\\))",
    "text": "Parallel Lines (Example 8.1 where \\(M = 2\\))\nAll \\(M\\) slopes are identical \\(H_0: \\beta_{11} = \\beta_{12} = \\cdots = \\beta_{1M} = \\beta_1\\)\n\nFull Model \\((F)\\): \\(y = \\beta_{0m} + \\beta_{1m}x + \\epsilon, \\quad m = 1, 2, \\dots, M\\).\nReduced Model \\((R)\\): \\(y = \\beta_0 + \\beta_1x + \\color{blue}{\\beta_2D_1 + \\beta_3D_2 + \\cdots + \\beta_{M-1}D_{M-1}}+\\epsilon\\), where \\(D_1, \\dots, D_{M-1}\\) are dummies.\n\n\n\n\n\n\\(F_{test} = \\frac{(SS_{res}(R) - SS_{res}(F))/(df_{R} - df_{F})}{SS_{res}(F)/df_{F}}\\)\n\n\\(df_{R} = n - (M+1)\\)\n\\(df_{F} = n - 2M\\)\n\n\\(SS_{res}(F)\\) is the sum of \\(SS_{res}\\) from each separate regression\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nUse the extra-sum-of-squares method to test the hypothesis\nAll \\(M\\) slopes are identical, \\(\\beta_{11} = \\beta_{12} = \\cdots = \\beta_{1M}\\), but the intercepts may differ.\n\nReduced Model \\((RM)\\): \\(y = \\beta_0 + \\beta_1x + \\beta_2D_1 + \\beta_3D_2 + \\cdots + \\beta_{M-1}D_{M-1}+\\epsilon\\), where \\(D_1, \\dots, D_{M-1}\\) are dummy variables.\n\n\\(F_{test} = \\dfrac{(SS_{res}(RM) - SS_{res}(FM))/(df_{RM} - df_{FM})}{SS_{res}(FM)/df_{FM}}\\)\n\n\\(df_{RM} = n - (M+1)\\)\n\\(df_{FM} = \\sum_{m=1}^M(n_m - 2) = n - 2M\\)\n\n\\(SS_{res}(FM)\\) is the sum of \\(SS_{res}\\) from each separate regression\n\n\nIf the reduced model is as satisfactory as the full model, then \\(F_{test}\\) will be small compared to \\(F_{\\alpha,df_{RM}‚àídf_{FM}, df_{FM}}.\\)"
  },
  {
    "objectID": "slides/13-cat-var.html#concurrent-lines",
    "href": "slides/13-cat-var.html#concurrent-lines",
    "title": "Categorical Variables üõ†",
    "section": "Concurrent Lines",
    "text": "Concurrent Lines\n\nAll \\(M\\) intercepts are equal, \\(H_0: \\beta_{01} = \\beta_{02} = \\cdots = \\beta_{0M}= \\beta_0\\)\n\n\nReduced model: \\(y = \\beta_0 + \\beta_1x + \\color{blue}{\\beta_2xD_1 + \\beta_3xD_2 + \\cdots + \\beta_{M-1}xD_{M-1}}+\\epsilon\\).\n\n\\(df_{R} = n - (M+1)\\)\n\n\nAssume concurrence at the origin.\n\n\n\nAll \\(M\\) intercepts are equal, \\(H_0: \\beta_{01} = \\beta_{02} = \\cdots = \\beta_{0M}\\), but the slopes may differ.\n\nReduced model: \\(y = \\beta_0 + \\beta_1x +\\beta_2xD_1 + \\beta_3xD_2 + \\cdots + \\beta_{M-1}xD_{M-1}+\\epsilon\\).\n\n\\(df_{R} = n - (M+1)\\)\n\n\nAssume concurrence at the origin.\n\nAll \\(M\\) intercepts are equal, \\(H_0: \\beta_{01} = \\beta_{02} = \\cdots = \\beta_{0M}\\), but the slopes may differ.\n\nReduced model: \\(y = \\beta_0 + \\beta_1x +\\beta_2xD_1 + \\beta_3xD_2 + \\cdots + \\beta_{M-1}xD_{M-1}+\\epsilon\\).\n\n\\(df_{R} = n - (M+1)\\)\n\n\nAssume concurrence at the origin."
  },
  {
    "objectID": "slides/13-cat-var.html#coincident-lines",
    "href": "slides/13-cat-var.html#coincident-lines",
    "title": "Categorical Variables üõ†",
    "section": "Coincident Lines",
    "text": "Coincident Lines\n\nBoth the \\(M\\) slopes and the \\(M\\) intercepts are the same,\\(H_0: \\beta_{11} = \\beta_{12} = \\cdots = \\beta_{1M} = \\beta_1\\), \\(\\beta_{01} = \\beta_{02} = \\cdots = \\beta_{0M} = \\beta_0\\)\n\n\nReduced model: \\(y = \\beta_0 + \\beta_1x+\\epsilon\\)\n\n\\(df_{R} = n - 2\\)\n\n\nDummy variables are not necessary in the test of coincidence.\n\n\n\n\n\n\n\n\n\n\n\n\nmath4780-f23.github.io/website"
  },
  {
    "objectID": "slides/02-overview.html#relationship-as-functions",
    "href": "slides/02-overview.html#relationship-as-functions",
    "title": "Overview of Regression üìñ",
    "section": "Relationship as Functions",
    "text": "Relationship as Functions\n\nRepresent relationships between variables using functions \\(y = f(x)\\).\n\nPlug in the inputs and receive the output.\n\n\\(y = f(x) = 3x + 7\\) is a function with input \\(x\\) and output \\(y\\).\nIf \\(x = 5\\), \\(y = 3 \\times 5 + 7 = 22\\).\n\n\n\n\n\nIn mathematics, how do we describe a Relationship Between Variables? We use a function. Right.\nThe function \\(y = f(x)\\) gives us the relationship between an output \\(Y\\) and one or more inputs \\(x\\).\n\nYou plug in the values of inputs and receive back the output value.\nFor example, the formula \\(y = f(x) = 3x + 7\\) is a function with input \\(x\\) and output \\(y\\). If \\(x = 5\\), \\(y = 3 \\times 5 + 7 = 22\\).\n\n\nBecause this is a linear function, we know that x and y are linearly related.\nWith a value of \\(x\\), I can give you 100% correct value of \\(y\\), which is right on this straight line. right. In other words, their relationship is 100% deterministic."
  },
  {
    "objectID": "slides/02-overview.html#different-relationships",
    "href": "slides/02-overview.html#different-relationships",
    "title": "Overview of Regression üìñ",
    "section": "Different Relationships",
    "text": "Different Relationships\n\n\n\nCan you come up with any real-world examples describing relationships between variables deterministically?\n\n\nThe relationship between x and y can be more than linear.\nThe relationship can be also quadratic, cubic or any other possible relationship."
  },
  {
    "objectID": "slides/02-overview.html#different-relationships-1",
    "href": "slides/02-overview.html#different-relationships-1",
    "title": "Overview of Regression üìñ",
    "section": "Different Relationships",
    "text": "Different Relationships\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHere I give your two examples. The first example is the conversion of F and C degrees. Their relationship is linear and F = 32 + 1.8 C.\nSo you give me a C degree, I can tell you its corresponding F degree fro sure. Right.\nThe second example comes from physics. the displacement of an object is a quadratic function of time.\nSo here s(t) = v0 * t + 0.5 * a * t^2. v0 is the initial velocity, and a is acceleration, and t is time.\nAgain the relationship between displacement and time is 100% deterministic.\nA value of time corresponds to an unique value of displacement given v0 and a."
  },
  {
    "objectID": "slides/02-overview.html#relationship-between-variables-is-not-perfect",
    "href": "slides/02-overview.html#relationship-between-variables-is-not-perfect",
    "title": "Overview of Regression üìñ",
    "section": "Relationship between Variables is Not Perfect",
    "text": "Relationship between Variables is Not Perfect\n\n\n\nCan you provide some real examples that the variables are related each other, but not perfectly related?\n\n\nBut unfortunately, in reality, most relationships between variables we are interested are not Perfect.\nIn fact, there is almost no perfect relationship between two variables because everything in the world is connected each other. And so any two variables are affected by any other variables.\nFor example, the displacement of an object is also affected by airflow and humidity, which is not considered in the formula.\nEven we the two variables are perfectly related, there are always some measurement errors or noises when we are recording and collecting their data. right. For example, there may be some measurement errors when we measure the displacement of an object for any given time, right? So their quadratic relationship is there, but the data we collect are not exactly on the quadratic curve.\nSo that‚Äôs why the Relationship between variables always involves some uncertainty."
  },
  {
    "objectID": "slides/02-overview.html#relationship-between-variables-is-not-perfect-1",
    "href": "slides/02-overview.html#relationship-between-variables-is-not-perfect-1",
    "title": "Overview of Regression üìñ",
    "section": "Relationship between Variables is Not Perfect",
    "text": "Relationship between Variables is Not Perfect\n\n\nüíµ In general, one with more years of education earns more.\nüíµ Any two with the same years of education may have different annual income.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHere I give you a simple example: the relationship between income and years of education.\nüíµ In general, one with more years of education earns more.\nüíµ Any two with the same years of education may have different annual income.\nBecause your income level depends on so many other factors, not just years of education.\nSo when you plot the scatter plot of the two variables, you will find that there is some trend, but the data are sort of scattered or jittered or variated around some function that describes the relationship between income the years of education.\n\nRed dots are observed values or the years of education and income pairs."
  },
  {
    "objectID": "slides/02-overview.html#variation-around-the-functionmodel",
    "href": "slides/02-overview.html#variation-around-the-functionmodel",
    "title": "Overview of Regression üìñ",
    "section": "Variation around the Function/Model",
    "text": "Variation around the Function/Model\n\n\nWhat are the unexplained variation coming from?\n\n\n\n\n\n\nOther factors accounting for parts of variability of income.\n\nAdding more explanatory variables to a model can reduce the variation size around the model.\n\n\nPure measurement error.\nJust that randomness plays a big role. ü§î\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWhat other factors (variables) may affect a person‚Äôs income?\n\n\n\nyour income = f(years of education, major, GPA, college, parent's income, ...)\n\nAnd the data Variation around the Function, or in general the regression model is just as important as the model, if not more!\n\nBasically, what statistics does is explain variation in the context of what remains unexplained.\nThe scatter plot suggests that there might be other factors that account for large parts of variability.\nIf that is the case, adding more explanatory variables ( \\(X\\)s ) to a model can sometimes usefully reduce the size of the scatter around the model.\nPerhaps just that randomness plays a big role."
  },
  {
    "objectID": "slides/02-overview.html#regression-model",
    "href": "slides/02-overview.html#regression-model",
    "title": "Overview of Regression üìñ",
    "section": "Regression Model",
    "text": "Regression Model\n\n\n\\(Y\\): response, outcome, label, dependent variable, e.g., income\n\n\n\\(X\\): predictor, covariate, feature, regressor, explanatory/ independent variable, e.g., years of education, which is known and fixed.\n\n\n\nExplain the relationship between \\(X\\) and \\(Y\\) and make predictions through a model \\[Y = f(X) + \\epsilon\\]\n\n\n\\(\\epsilon\\): irreducible random error\n\nindependent of \\(X\\)\n\n\nmean zero with some variance.\n\n\n\n\\(f(\\cdot)\\): fixed but unknown function describing the relationship between \\(X\\) and the mean of \\(Y\\). \n\n\n\n\n\nIn Intro Stats, what is the form of \\(f\\) and what assumptions you made on the random error \\(\\epsilon\\) ?\n\n\n\n\n\n\\(f(X) = \\beta_0 + \\beta_1X\\) with unknown parameters \\(\\beta_0\\) and \\(\\beta_1\\).\n\n\\(\\epsilon \\sim N(0, \\sigma^2)\\).\n\n\nOK. Now after collecting the data of the variables we are interested, we know their relationship, most of the time, is not perfect, and stochastic in some way and in some sense.\nAnd how do we model such stochastic relationship? Well the answer is a regression model.\nSuppose we are interested in the relationship between two variables, call \\(X\\) and \\(Y\\). In particular, we like to know how changes of \\(X\\) affect value of \\(Y\\), or we want to use \\(X\\) to predict \\(Y\\).\nIn this sense, \\(Y\\) is called response, outcome, label, dependent variable, e.g., income\n\n\n\\(X\\) is called predictor, covariate, feature, regressor, explanatory or independent variable, e.g., years of education, which is known and fixed.\nExplain the relationship between \\(X\\) and \\(Y\\) and make predictions through a model \\(Y = f(X) + \\epsilon\\). This is a very general regression model we can built to learn the relationship b/w x and y.\n\n\\(f(\\cdot)\\) is fixed but unknown and describes the true relationship between \\(X\\) and \\(Y\\). \n\n\n\\(\\epsilon\\) is a irreducible random error which is assumed to be independent of \\(X\\) and has mean zero with some variance.\n\n\\(\\epsilon\\) is used to represent those measurement errors or the variation that cannot be explained or captured by the predictor X.\nIntro Stats:\n\n\n\\(f(X) = \\beta_0 + \\beta_1X\\) with unknown parameters \\(\\beta_0\\) and \\(\\beta_1\\).\n\n\\(\\epsilon \\sim N(0, \\sigma^2)\\).\n\n\n\n\\(X\\) and \\(Y\\) are assumed to be linearly related, which may not be correct.\nNext week, we will learn simple linear regression from the scratch and in much more detail. Here I just give you an overview."
  },
  {
    "objectID": "slides/02-overview.html#true-unknown-function-f-of-the-model-y-fx-epsilon",
    "href": "slides/02-overview.html#true-unknown-function-f-of-the-model-y-fx-epsilon",
    "title": "Overview of Regression üìñ",
    "section": "True Unknown Function \\(f\\) of the Model \\(Y = f(X) + \\epsilon\\)\n",
    "text": "True Unknown Function \\(f\\) of the Model \\(Y = f(X) + \\epsilon\\)\n\n\n\n\nBlue curve: true underlying relationship between (the mean) income and years of education.\n\nBlack lines: error associated with each observation\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLet‚Äôs go back to the income-education example. Red dots are observed values or the years of education and income pairs. Suppose we like to use a regression model \\(y = f(x) + \\epsilon\\) to describe the relationship between income and education.\nAnd the *Blue** curve on the right shows the true underlying relationship between income and years of education, which is the function \\(f\\) in our regression model.\nAnd each Black vertical line indicates an error associated with each observation.\nSo again, each red dot or observation is the value of the function \\(f(x)\\) plus some random error with its magnitude shown in a black vertical line.\nAgain, in regression, we assume years of education is fixed. It is income level that varies around the function \\(f\\).\n\n\nBig problem: \\(f(x)\\) is unknown and needs to be estimated.\n\n\nAnd a big problem is, in reality, we don‚Äôt know this true function \\(f\\). And we need to estimate it.\nestimating this unknown \\(f\\) is a main job of regression analysis. We are trying to uncover the underlying true process or function \\(f\\), given the data that are noisy or with some unknown random errors."
  },
  {
    "objectID": "slides/02-overview.html#why-estimate-f-prediction-for-y",
    "href": "slides/02-overview.html#why-estimate-f-prediction-for-y",
    "title": "Overview of Regression üìñ",
    "section": "Why Estimate \\(f\\)? Prediction for \\(Y\\)\n",
    "text": "Why Estimate \\(f\\)? Prediction for \\(Y\\)\n\n\n\nPrediction: Inputs \\(X\\) are available, but the output \\(Y\\) cannot be easily obtained. We predict \\(Y\\) using \\[ \\hat{Y} = \\hat{f}(X), \\] where \\(\\hat{f}\\) is our estimate of \\(f\\), and \\(\\hat{Y}\\) represents the resulting prediction for \\(Y\\).\n\n\nNow we know our goal is to estimate the unknown regression function \\(f\\). But why do we need that? Any benefits we can have after estimating \\(f\\)?\nWell, the two main benefits of doing regression are\n\nfirst, we are able to predict y given a value of x\nthe second benefit is, we learn how x affects y, which may be our research interest.\n\n\nLet‚Äôs discuss prediction first.\nRegression can be a great tool for prediction, especially when inputs \\(X\\) are available, but the output \\(Y\\) cannot be easily obtained.\nFor example, we usually know people‚Äôs years of education, but we rarely know their income level, right? because income is a private and personal information.\nAfter we estimate \\(f\\), we can use the estimate of \\(f\\) to predict \\(y\\),\nSo we predict \\(Y\\) using the relationship between X and Y that we learn from our data that\n\n\nIn Intro Stats, what is our estimated regression function \\(\\hat{f}\\)?\n\n\n\n\n\n\\(\\hat{Y} = \\hat{\\beta}_0 + \\hat{\\beta}_1X\\).\n\n\n\n\n\n\\(\\hat{f}\\) is often treated as a black box.\n\n\n\n\n\n\n\n\n\n\nWhen the goal is prediction, \\(\\hat{f}\\) is often treated as a black box. Basically we don‚Äôt care much about how \\(f\\) looks like, or whether the shape \\(f\\) is meaningful. All we want is that predicted values are as close as the true response values."
  },
  {
    "objectID": "slides/02-overview.html#why-estimate-f-inference-for-f",
    "href": "slides/02-overview.html#why-estimate-f-inference-for-f",
    "title": "Overview of Regression üìñ",
    "section": "Why Estimate \\(f\\)? Inference for \\(f\\)\n",
    "text": "Why Estimate \\(f\\)? Inference for \\(f\\)\n\n\n\nInference: Understand how \\(Y\\) is affected by \\(X\\).\n\n\\(\\hat{f}\\) cannot be treated as a black box. We want to know the exact form of \\(f\\).\n\n\nWe are interested in\n\n\n Which covariates are associated with the response? \nüëâ Do age, education level, gender, etc affect salary?\n\n\n\n\n\n What is the relationship between the response and each covariate? \nüëâ How much salary increases/decreases as age increases one unit?\n\n\n\n\n\n Can the relationship be adequately summarized using any equation? \nüëâ The relationship between salary and age is linear, quadratic or more complicated?\n\n\nAnother benefit of estimating y is that we can do inference for the regression function \\(f\\).\nWe estimate \\(f\\) so that we understand how \\(Y\\) is affected as \\(X\\) changes.\nWhen the goal is inference, \\(\\hat{f}\\) cannot be treated as a black box.\nWe want to know its exact form of \\(f\\).\nWhen doing inference, we are interested in:\n\n\nWhich covariates are associated with the response? e.g.¬†Do age, education level, gender, etc affect salary?\n\n\nWhat is the relationship between the response and each covariate? e.g.¬†How much salary increases/decreases as age increases one unit?\n\n\nCan the relationship between \\(Y\\) and each covariate be adequately summarized using a linear equation, or is the relationship more complicated? e.g.¬†The relationship between salary and age is linear, quadratic or more complicated?\n\n\n\n\nAll those questions may be your research questions, and these can be answered by regression analysis."
  },
  {
    "objectID": "slides/02-overview.html#how-to-estimate-f",
    "href": "slides/02-overview.html#how-to-estimate-f",
    "title": "Overview of Regression üìñ",
    "section": "How to Estimate \\(f\\)?",
    "text": "How to Estimate \\(f\\)?\n\nObservations \\(\\{(x_1, y_1), (x_2, y_2), \\dots, (x_n,y_n)\\}\\): training data to train or teach our model to learn \\(f\\).\nUse test data to test or evaluate how well the model makes inference or prediction.\n\n\n\nModels are characterized as either parametric or nonparametric.\n\n\nWe learn why we estimate \\(f\\). But How do we Estimate \\(f\\)?\nWell, that‚Äôs is why we have this course. we will learn how to estimate f starting next week.\nThe basic idea is that We have observed values \\(\\{(x_1, y_1), (x_2, y_2), \\dots, (x_n,y_n)\\}\\) called training data in machine learning that is used to train or teach our model to learn \\(f\\).\n\n\n\n\n\nParametric methods involve a two-step model-based approach:\n\n1Ô∏è‚É£ Make an assumption about the functional form, or shape of \\(f\\), e.g.¬†linear regression \\[ f(X) = \\beta_0 + \\beta_1X_1 \\]\n\n2Ô∏è‚É£ Use the training data to fit or train the model, e.g., estimate the parameters \\(\\beta_0, \\beta_1\\) using (ordinary) least squares.\n\n\n\n\nGenerally, regression models can be characterized as either parametric or non-parametric.\nAnd parametric methods involve a two-step model-based approach:\n\n[1] First we make an assumption about the functional form, or shape of \\(f\\). For example, we can assume the function form or the relationship between y and predictors is linear, so the function \\(f(X)\\) is a linear function $f(X) = _0 + _1X_1 + _2X_2 + , _pX_p $.\n[2] And then we use the training data to fit or train the model. For example, estimate the parameters \\(\\beta_0, \\beta_1, \\beta_2 \\dots, \\beta_p\\) using (ordinary) least squares.\n\n\n\nNonparametric methods, on the other hand, do not make assumptions about the functional form of \\(f\\).\nThey basically seek an estimate of \\(f\\) that gets as close to the data points as possible without being too rough or wiggly. So the idea is, we try to make the function looks like data scatter pattern, and the function is a smoothed version of the data.\n\n\n\n\n\nNonparametric methods do not make assumptions about the shape of \\(f\\).\n\nSeek an estimate of \\(f\\) that gets close to the data points without being too rough or wiggly."
  },
  {
    "objectID": "slides/02-overview.html#parametric-vs.-nonparametric-models",
    "href": "slides/02-overview.html#parametric-vs.-nonparametric-models",
    "title": "Overview of Regression üìñ",
    "section": "Parametric vs.¬†Nonparametric Models",
    "text": "Parametric vs.¬†Nonparametric Models\n\n\nParametric (Linear regression) \n\n\n\n\n\n\n\n\n\nNonparametric (LOESS) \n\n\n\n\n\n\n\n\n\n\n\nSo here see the difference between parametric and nonparameteric models.\nFor the parameteric model, we actually use linear regression, assuming the relationship between Income and Years of Education is linear.\nFor the nonparameteric model, we learning the regression function \\(f\\) from the data without putting strict constraints on the shape of \\(f\\).\nRemember that the fitted function here is an estimated one, not the true one.\nWhich one is better than the other? We‚Äôll learn how to compare different models later."
  },
  {
    "objectID": "slides/02-overview.html#parametric-vs.-nonparametric-models-1",
    "href": "slides/02-overview.html#parametric-vs.-nonparametric-models-1",
    "title": "Overview of Regression üìñ",
    "section": "Parametric vs.¬†Nonparametric Models",
    "text": "Parametric vs.¬†Nonparametric Models\n\n\nParametric: Assumptions on \\(f\\) with unknown parameters.\n\nNonparametric: No assumptions on \\(f\\) (may have no closed form).\n\n\n\nIn sum, regression can be either parametric or non-parametric.\n\nParametric models put some assumptions on \\(f\\) with unknown parameters.\n\nNonParametric models have no assumptions on the shape of \\(f\\)."
  },
  {
    "objectID": "slides/02-overview.html#linear-vs.-nonlinear-models",
    "href": "slides/02-overview.html#linear-vs.-nonlinear-models",
    "title": "Overview of Regression üìñ",
    "section": "Linear vs.¬†Nonlinear Models",
    "text": "Linear vs.¬†Nonlinear Models\n\n\nLinear Regression: \\(Y\\) is linear in unknown parameters, NOT predictors.\n\n\\(Y = \\beta_0 + \\beta_1X + \\epsilon\\)\n\\(Y = \\beta_0 + \\beta_1X + \\beta_2X^2 + \\epsilon\\)\n\\(Y = \\beta_0 + \\beta_1X + \\beta_2X^2 + \\beta_3\\sqrt{X} + \\epsilon\\)\n\n\n\n\nIn fact, Parametric models can be either a linear model or a nonlinear model\nWhen we say linear regression, it means \\(Y\\) is linear in unknown parameters, NOT predictors.\nNote that for all three cases here, y is linear in \\(\\beta\\), and so they all belong to linear regression.\nA nonlinear relationship between \\(X\\) and \\(Y\\) can be modeled as a linear regression.\n\n\nA nonlinear relationship between \\(X\\) and \\(Y\\) can be modeled using a linear regression.\n\n\n\n\n\nNonlinear Regression: \\(Y\\) is NOT linear in unknown parameters.\n\n\\(Y = \\frac{\\beta_0}{1 + e ^{-\\beta_1X}} + \\epsilon\\)\n\\(Y = \\beta_0e ^{-\\beta_1X}\\cdot\\epsilon\\)\n\n\n\n\nWhen we say Nonlinear Regression, it means that \\(Y\\) is NOT linear in unknown parameters.\n\n\n\nüëâ Some nonlinear models can be transformed to an equivalent linear model.\n\nActually Some nonlinear models can be transformed to an equivalent linear model.\nSo we know linear models can describe nonlinear relationships between variables, and some nonlinear models can be transformed into linear models, and linear regression is much easier to deal with.\nThat‚Äôs why linear regression or linear model in general plays an important role in statistical modeling.\nWe love to start with a linear model when doing data analysis.\nIf we can get our job done using a simple model, why use a complex one.\n\n\n\n\nWhich nonlinear model above can be transformed into a linear model?"
  },
  {
    "objectID": "slides/02-overview.html#linear-vs.-nonlinear-models-1",
    "href": "slides/02-overview.html#linear-vs.-nonlinear-models-1",
    "title": "Overview of Regression üìñ",
    "section": "Linear vs.¬†Nonlinear Models",
    "text": "Linear vs.¬†Nonlinear Models\n\n\n\n\n\n\n\n\n\nOK. So Regression can be divided into Linear and Nonlinear Regression.\n\n\n\nmath4780-f23.github.io/website"
  },
  {
    "objectID": "slides/07-diag-unusual.html#outliers-unusual-y",
    "href": "slides/07-diag-unusual.html#outliers-unusual-y",
    "title": "Regression Diagnostics: Unusual Data üë©‚Äçüíª",
    "section": "Outliers: Unusual \\(y\\)\n",
    "text": "Outliers: Unusual \\(y\\)\n\n\nAn outlier is a case whose response value is unusual given the value of the regressors.\n\n\n\nNeither the x nor y value of the outlier is individually unusual, as it is clear from the marginal distribution of each variable."
  },
  {
    "objectID": "slides/07-diag-unusual.html#leverage-unusual-x",
    "href": "slides/07-diag-unusual.html#leverage-unusual-x",
    "title": "Regression Diagnostics: Unusual Data üë©‚Äçüíª",
    "section": "Leverage: Unusual \\(x\\)\n",
    "text": "Leverage: Unusual \\(x\\)\n\n\nPoints that are away from the center of the data cloud of \\({\\bf x}\\) are called leverage points.\n\n\n\n\n\n\n   time cases distance\n1  16.7     7      560\n2  11.5     3      220\n3  12.0     3      340\n4  14.9     4       80\n5  13.8     6      150\n6  18.1     7      330\n7   8.0     2      110\n8  17.8     7      210\n9  79.2    30     1460\n10 21.5     5      605\n11 40.3    16      688\n12 21.0    10      215\n13 13.5     4      255\n14 19.8     6      462\n15 24.0     9      448\n16 29.0    10      776\n17 15.3     6      200\n18 19.0     7      132\n19  9.5     3       36\n20 35.1    17      770\n21 17.9    10      140\n22 52.3    26      810\n23 18.8     9      450\n24 19.8     8      635\n25 10.8     4      150\n\n\n\n\n\n\n\n\n\n\n\n\n\nObservation 9 and 22 are away from the center of \\({\\bf x}\\) space, and are likely leverage points.\n\n\n\n\nleverage certainly will have a dramatic effect on the model summary statistics such as \\(R^2\\) and the standard errors of the regression coefficients"
  },
  {
    "objectID": "slides/07-diag-unusual.html#influence-unusual-x-and-y",
    "href": "slides/07-diag-unusual.html#influence-unusual-x-and-y",
    "title": "Regression Diagnostics: Unusual Data üë©‚Äçüíª",
    "section": "Influence: Unusual \\(x\\) and \\(y\\)\n",
    "text": "Influence: Unusual \\(x\\) and \\(y\\)\n\n\nAn influential point has great influence on estimated regression coefficients. \\[\\text{Influence} = \\text{Leverage} \\times \\text{Outlyingness}\\]\n\n\n\n\nWhat kind of data points have such great influence power?\nWell if a point is an outlier with high leverage, then it becomes a influential point.\nWe got to be very careful about influential points because the estimated regression coefficients change a lot with and without these points.\nOur analysis conclusion may be completely opposite => explain the figure."
  },
  {
    "objectID": "slides/07-diag-unusual.html#unusual-data-outliers-leverage-and-influence",
    "href": "slides/07-diag-unusual.html#unusual-data-outliers-leverage-and-influence",
    "title": "Regression Diagnostics: Unusual Data üë©‚Äçüíª",
    "section": "Unusual Data: Outliers, Leverage and Influence",
    "text": "Unusual Data: Outliers, Leverage and Influence"
  },
  {
    "objectID": "slides/07-diag-unusual.html#measuring-leverage-hat-values",
    "href": "slides/07-diag-unusual.html#measuring-leverage-hat-values",
    "title": "Regression Diagnostics: Unusual Data üë©‚Äçüíª",
    "section": "Measuring Leverage: Hat Values",
    "text": "Measuring Leverage: Hat Values\n\n\\({\\bf H} = {\\bf X(X'X)}^{-1} {\\bf X}'\\).\nThe hat value \\(h_{ii}\\) is the \\(i\\)th diagonal element of \\(\\bf H\\).\n\\(h_{ii} \\in \\left[\\frac{1}{n}, 1\\right]\\) is a standardized measure of the distance of the \\(i\\)th case from the centroid of the \\(\\bf x\\) space, taking into account the correlational structure of the \\(x\\)s.\nThe larger the \\(h_{ii}\\) is, the farther the point \\({\\bf x}_i\\) lies to the centroid of the \\(\\bf x\\) space.\n\n\n\nThe contours show the same or constant leverage in the 2-dim data ellipses.\nThe two black points are equally unusual and have equally large hat values.\nexample: one point at inner contour, the other outer contour"
  },
  {
    "objectID": "slides/07-diag-unusual.html#measuring-leverage-hat-values-1",
    "href": "slides/07-diag-unusual.html#measuring-leverage-hat-values-1",
    "title": "Regression Diagnostics: Unusual Data üë©‚Äçüíª",
    "section": "Measuring Leverage: Hat Values",
    "text": "Measuring Leverage: Hat Values\n\nIn simple linear regression, \\[h_{ii} = \\frac{1}{n} + \\frac{\\left(x_i - \\bar{x}\\right)^2}{S_{xx}}\\]\n\\(\\hat{y}_j = b_0 + b_1 ~x_j = h_{j1}y_1 + h_{j2}y_2 + \\cdots + h_{jn}y_n\\)\n\\(h_{ii} = \\sum_{j=1}^nh_{ji}^2\\) summarizes the influence (the leverage) of \\(y_i\\) on all the fitted values \\(\\hat{y}_j\\), \\(j = 1, \\dots, n\\).\n\\(\\bar{h} = \\frac{\\sum_{i=1}^n h_{ii}}{n} = p/n\\)\n\n\nA point with \\(h_{ii} > 2\\bar{h}\\) is considered a leverage point.\n\n\n\\(\\hat{y}_i = b_0 + b_1 ~x_i = h_{i1}y_1 + h_{i2}y_2 + \\cdots + h_{in}y_n\\)\n\n\\(h_{ii} = \\sum_{j=1}^nh_{ji}^2\\) summarizes the potential influence (the leverage) of \\(y_i\\) on all the fitted values \\(\\hat{y}_j\\), \\(j = 1, \\dots, n\\).\n\n\\(2\\bar{h}\\) may be greater than 1, which is not applicable.\nA point with \\(h_{ii} > 2\\bar{h}\\) is remote enough to be considered a leverage point.\n2p/n corresponds to 5% of the data when x is multivariate normal and n and p are large.\nUse 3p/n when n small."
  },
  {
    "objectID": "slides/07-diag-unusual.html#r-lab-delivery-time-leverage-points",
    "href": "slides/07-diag-unusual.html#r-lab-delivery-time-leverage-points",
    "title": "Regression Diagnostics: Unusual Data üë©‚Äçüíª",
    "section": "\nR Lab Delivery Time Leverage Points",
    "text": "R Lab Delivery Time Leverage Points\n\nhat_i <- hatvalues(delivery_lm)\nsort(hat_i, decreasing = TRUE)\n\n    9    22    10    16    21    24    12     1    20     3    19    18    11 \n0.498 0.392 0.196 0.166 0.165 0.121 0.114 0.102 0.102 0.099 0.096 0.096 0.086 \n    4     7    14     5     2    25     8    13    17     6    23    15 \n0.085 0.082 0.078 0.075 0.071 0.067 0.064 0.061 0.059 0.043 0.041 0.041 \n\n\n\n\n\np <- 3\nn <- dim(delivery_data)[1]  ## n = 25\n2 * p/n\n\n[1] 0.24\n\nhat_i[hat_i > 2 * p/n]\n\n   9   22 \n0.50 0.39 \n\n\n\nObservation 9 and 22 are identified as high-leverage points.\n\n\n\n\n\n\n\n\n\n\n\n\n\nMost cutoff works better for large sample data\nproblem-specific unit change may be large after removing the influential point, even though the measures are not greater than the cutoff."
  },
  {
    "objectID": "slides/07-diag-unusual.html#detecting-outliers",
    "href": "slides/07-diag-unusual.html#detecting-outliers",
    "title": "Regression Diagnostics: Unusual Data üë©‚Äçüíª",
    "section": "Detecting Outliers",
    "text": "Detecting Outliers\n\nMeasure the conditional unusualness of \\(y\\) given \\({\\bf x}\\).\n‚ùå Can‚Äôt just use residual \\(e_i\\) to measure the unusualness of \\(y_i\\)!\nIf the point is also a leverage point (large \\(h_{ii}\\)), it‚Äôs residual tends to be small.\n\n\nCan‚Äôt just use residual \\(e_i\\) to measure the unusualness of \\(y_i\\)! If the point is also a leverage point (large \\(h_{ii}\\)), it‚Äôs residual tends to be small. -> Check plot: these points force the regression line or surface to be close to them.\n\n\n\nR-student residual: \\[t_i = \\frac{e_i}{\\sqrt{s^2_{(i)}(1-h_{ii})}}\\] where \\(s^2_{(i)}\\) estimates \\(\\sigma^2\\) based on the data with the \\(i\\)th point removed.\n\n\n\n\n\\(t_i \\sim t_{n-p-1}\\) and a formal testing procedure can be used for detecting outliers.\nA point with \\(|t_i| > 2\\) needs to be examined with care.\n\n\n\n(Eq (4.12) in LRA)\nThere are lots variants of residuals that are for several different purposes.\nThe so-called R-student residual is recommended for detecting outliers.\nBonferroni correction is better for testing n residuals simultaneously.\nA point with \\(|t_i| > 2\\) needs to be examined with care ( \\(5\\%\\) of the data)."
  },
  {
    "objectID": "slides/07-diag-unusual.html#r-lab-detecting-outliers---residual-plot",
    "href": "slides/07-diag-unusual.html#r-lab-detecting-outliers---residual-plot",
    "title": "Regression Diagnostics: Unusual Data üë©‚Äçüíª",
    "section": "\nR Lab Detecting Outliers - Residual Plot",
    "text": "R Lab Detecting Outliers - Residual Plot\n\nr_student <- rstudent(delivery_lm)\nround(sort(r_student, decreasing = TRUE), 2)\n\n    9     4    18    10    11    19     8     2    14    13     7    15    17 \n 4.31  1.64  1.12  0.81  0.71  0.57  0.36  0.36  0.33  0.32  0.26  0.21  0.13 \n    3    25     6     5    12    16    21    23    22    24     1    20 \n-0.02 -0.07 -0.09 -0.14 -0.19 -0.22 -0.87 -1.48 -1.49 -1.54 -1.70 -2.00"
  },
  {
    "objectID": "slides/07-diag-unusual.html#measuring-influence",
    "href": "slides/07-diag-unusual.html#measuring-influence",
    "title": "Regression Diagnostics: Unusual Data üë©‚Äçüíª",
    "section": "Measuring Influence",
    "text": "Measuring Influence\n\nThe influence measures are those that measure the effect of deleting the \\(i\\)th observation.\n\n\n \\(DFBETAS_{j, i}\\)  measures the effect on coefficient \\({b_j}\\) when the \\(i\\)th observation is removed.\n\n Cook‚Äôs Distance \\(D_i\\)  measures the effect of the \\(i\\)th observation on coefficient vector \\({\\bf b}\\).\n\n \\(DFFITS_{i}\\)  measures the effect on fitted value \\(\\hat{y}_i\\).\n\n \\(COVRATIO_{i}\\)  measures the effect on the precision of estimates (variance).\n\n\n\n\nNow we are going to learn some measures of influence.\nWith all these measures, we can quantify how large impact a point can make on the model coefficients or model fitting in general.\nAnd the idea is to check the difference between model coefficients or model fitting results when the point is in the data and those when the point is deleted."
  },
  {
    "objectID": "slides/07-diag-unusual.html#dfbetas_j-i",
    "href": "slides/07-diag-unusual.html#dfbetas_j-i",
    "title": "Regression Diagnostics: Unusual Data üë©‚Äçüíª",
    "section": "\\(DFBETAS_{j, i}\\)",
    "text": "\\(DFBETAS_{j, i}\\)\n\n\n\\(DFBETAS_{j, i}\\) measures how much the regression coefficient \\(b_j\\) changes in standard error units if the \\(i\\)th observation is removed. \\[DFBETAS_{j, i} = \\frac{b_j - b_{j(i)}}{\\sqrt{s_{(i)}^2C_{jj}}} = \\frac{b_j - b_{j(i)}}{se_{(i)}(b_j)}\\] where\n\n\\(b_{j(i)}\\) is the \\(j\\)th coefficient estimate computed without the \\(i\\)th observation.\n\n\\(C_{jj}\\) is the diagonal element of \\(( {\\bf X}'{\\bf X})^{-1}\\) corresponding to \\(b_j\\).\n\n\n\nThe \\(i\\)th point is considered influential on \\(j\\)th coefficient if \\(|DFBETAS_{j, i}| > 2/\\sqrt{n}\\).\nOne issue: there are \\(np\\) DFBETAS measures.\n\n\n\nDF means difference; S means standardized. \\[DFBETAS_{j, i} := \\frac{b_j - b_{j(i)}}{\\sqrt{S_{(i)}^2C_{jj}}} = \\frac{r_{j, i}}{\\sqrt{{\\bf r}_j'{\\bf r}_j}}{\\frac{1}{\\sqrt{1-h_{ii}}}}t_i\\]\n\nThe denominator provides a standardization since it estimates the standard error of \\(b_j\\). \n\n\n\\(DFBETAS_{j, i}\\) represents the combination of leverage measures and the impact of errors in the \\(y\\) direction.\nThe \\(n\\) elements in \\({\\bf r}_j'\\) produce the leverage that the \\(n\\) observations have on \\(b_j\\).\n\n\\(\\frac{r_{j, i}}{\\sqrt{{\\bf r}_j'{\\bf r}_j}}\\) is a normalized measure of the impact of the \\(i\\)th observation on the \\(j\\)th coefficient.\nThe measure is swelled by the leverage score \\(h_{ii}\\).\nPoint \\(i\\) is considered influential on \\(j\\)th coefficient if \\(|DFBETAS_{j, i}| > 2/\\sqrt{n}\\).\n\n\n\\(b_{j(i)}\\): the \\(j\\)th coefficient computed without the \\(i\\)th observation\n\n\\(S_{(i)}^2\\): the estimate of \\(\\sigma^2\\) based on the data with no the \\(i\\)th point (Eq. (4.12))\n\n\\(C_{jj}\\): the \\(j\\)th diagonal element of \\({\\bf(X'X)}^{-1}\\)\n\n\n\\({\\bf r}_j'\\): the \\(j\\)th row of the \\(p \\times n\\) matrix \\({\\bf R = (X'X)^{-1}X'}\\) \\(({\\bf b = Ry})\\)\n\n\n\\(r_{j, i}\\): the \\(ji\\)th element of \\({\\bf R}\\)\n\n\n\\(t_i\\): the \\(i\\)th R-student residual The n elements in the j th row of R produce the leverage that the n observations in the sample have on ÀÜŒ≤j"
  },
  {
    "objectID": "slides/07-diag-unusual.html#r-lab-dfbetas_j-i",
    "href": "slides/07-diag-unusual.html#r-lab-dfbetas_j-i",
    "title": "Regression Diagnostics: Unusual Data üë©‚Äçüíª",
    "section": "\nR Lab \\(DFBETAS_{j, i}\\)\n",
    "text": "R Lab \\(DFBETAS_{j, i}\\)\n\n\n\n\n\nround(dfbeta <- dfbetas(delivery_lm), 2)\n\n   (Intercept) cases distance\n1        -0.19  0.41    -0.43\n2         0.09 -0.05     0.01\n3         0.00  0.00     0.00\n4         0.45  0.09    -0.27\n5        -0.03 -0.01     0.02\n6        -0.01  0.00     0.00\n7         0.08 -0.02    -0.01\n8         0.07  0.03    -0.05\n9        -2.58  0.93     1.51\n10        0.11 -0.34     0.34\n11       -0.03  0.09     0.00\n12       -0.03 -0.05     0.05\n13        0.07 -0.04     0.01\n14        0.05 -0.07     0.06\n15        0.02  0.00     0.01\n16        0.00  0.06    -0.08\n17        0.03  0.01    -0.02\n18        0.25  0.19    -0.27\n19        0.17  0.02    -0.10\n20        0.17 -0.21    -0.09\n21       -0.16 -0.30     0.34\n22        0.40 -1.03     0.57\n23       -0.16  0.04    -0.05\n24       -0.12  0.40    -0.47\n25       -0.02  0.00     0.01\n\n\n\n\n\n\n## cut-off = 0.4\napply(dfbeta, 2, \n      function(x) x[abs(x) >  2 / sqrt(n)])\n\n$`(Intercept)`\n    4     9 \n 0.45 -2.58 \n\n$cases\n    1     9    22    24 \n 0.41  0.93 -1.03  0.40 \n\n$distance\n    1     9    22    24 \n-0.43  1.51  0.57 -0.47 \n\n\n\n\nPoint 9 is influential, as its deletion results in a displacement of every coefficient by at least 0.9 standard deviation of \\(b_j\\)."
  },
  {
    "objectID": "slides/07-diag-unusual.html#cooks-distance-d_i",
    "href": "slides/07-diag-unusual.html#cooks-distance-d_i",
    "title": "Regression Diagnostics: Unusual Data üë©‚Äçüíª",
    "section": "Cook‚Äôs Distance \\(D_i\\)\n",
    "text": "Cook‚Äôs Distance \\(D_i\\)\n\n\n\n\\(D_i\\) measures the squared distance that the vector of fitted values moves when the \\(i\\)th observation is deleted.\n\n\n\\[D_i = \\frac{(\\hat{{\\bf y}}_{(i)} - \\hat{{\\bf y}})'(\\hat{{\\bf y}}_{(i)}-\\hat{{\\bf y}})}{p s^2} = \\frac{r_i^2}{p}\\frac{h_{ii}}{1-h_{ii}}, \\quad i = 1, 2, \\dots, n\\] where \\(r_i = \\frac{e_i}{\\sqrt{s^2(1-h_{ii})}}\\) is the Studentized residuals.\n\nWhat contributes to \\(D_i\\):\n\nHow well the model fits the \\(i\\)th observation (larger \\(r_i^2\\) for poorer fit)\nHow far the point is away from the remaining data (larger \\(h_{ii}\\) for higher leverage)\n\\(\\text{Influence} = \\text{Leverage} \\times \\text{Outlyingness}\\)\n\n\n\n\nConsider points with \\(D_{i} > 1\\) to be influential.\n\n\n\n\\(D_i > 1\\) may risk missing unusual data. Use \\(4/(n-p)\\) instead. \\[\\begin{align} D_i& := \\frac{\\left( {\\bf b}_{(i)} - {\\bf b} \\right)' {\\bf X'X} \\left( {\\bf b}_{(i)} - {\\bf b} \\right)}{pMS_{res}}\\\\ &=\\frac{r_i^2}{p}\\frac{\\mathrm{Var}(\\hat{y}_i)}{\\mathrm{Var}(\\hat{e}_i)} =\\frac{r_i^2}{p}\\frac{h_{ii}}{1-h_{ii}}, \\quad i = 1, 2, \\dots, n \\end{align}\\]\n\nView \\(D_i\\) as the standardized distance between \\(b\\) and \\(b_{-i}\\).\n\n\\(D_i > 0\\) since \\(X'X\\) is positive definite.\nThe magnitude of \\(D_i\\) is usually assessed by comparing it to \\(F_{\\alpha, p, n-p}\\).\nIf \\(D_i = F_{0.5, p , n ‚àí p}\\), then deleting point i would move \\(b_{-i}\\) to the boundary of an approximate 50% confidence region for \\(\\beta\\) based on the complete data set.\n\n\\(\\frac{h_{ii}}{1-h_{ii}}\\) This ratio can be shown to be the distance from the vector \\(x_i\\) to the centroid of the remaining data.\nIt is the squared Euclidean distance (apart from pMS_Res) that the vector of fitted values moves when the \\(i\\)th observation is deleted.\nD is a scale-invariant measure of the influence of the ith case on all of the regression coeffcients, that is the distance between b and b(-i)."
  },
  {
    "objectID": "slides/07-diag-unusual.html#r-lab-cooks-distance-d_i",
    "href": "slides/07-diag-unusual.html#r-lab-cooks-distance-d_i",
    "title": "Regression Diagnostics: Unusual Data üë©‚Äçüíª",
    "section": "\nR Lab Cook‚Äôs Distance \\(D_i\\)\n",
    "text": "R Lab Cook‚Äôs Distance \\(D_i\\)\n\n\nDii <- cooks.distance(delivery_lm)\nDii[Dii > 1]\n\n  9 \n3.4 \n\nround(sort(Dii, decreasing = TRUE), 3)\n\n    9    22    20    24     1     4    10    21    18    23    11    19     2 \n3.419 0.451 0.132 0.102 0.100 0.078 0.054 0.051 0.044 0.030 0.016 0.012 0.003 \n   14    16     8    13     7    12    15     5    17     6    25     3 \n0.003 0.003 0.003 0.002 0.002 0.002 0.001 0.001 0.000 0.000 0.000 0.000 \n\n\n\nObservation 9 is influential using the cutoff of one, and the point 22 is not. \n\n\n\n\\(D_i > 1\\) may risk missing unusual data. Use \\(4/(n-p)\\) instead.\n\n\nround(pf(Dii, df1 = 3, 22), 3)\n\n    1     2     3     4     5     6     7     8     9    10    11    12    13 \n0.041 0.000 0.000 0.029 0.000 0.000 0.000 0.000 0.965 0.017 0.003 0.000 0.000 \n   14    15    16    17    18    19    20    21    22    23    24    25 \n0.000 0.000 0.000 0.000 0.013 0.002 0.060 0.016 0.281 0.007 0.042 0.000 \n\npf(3.41835, 3, 22)\n\n[1] 0.96\n\npf(0.45106, 3, 22)\n\n[1] 0.28\n\n\nThe magnitude of D i is usually assessed by comparing it to F Œ± , p , n ‚àí p . If D i = F 0.5, p , n ‚àí p , then deleting point i would move ÀÜb(i) to the boundary of an approximate 50% confidence region for Œ≤ based on the complete data set."
  },
  {
    "objectID": "slides/07-diag-unusual.html#dffits_i",
    "href": "slides/07-diag-unusual.html#dffits_i",
    "title": "Regression Diagnostics: Unusual Data üë©‚Äçüíª",
    "section": "\\(DFFITS_{i}\\)",
    "text": "\\(DFFITS_{i}\\)\n\n\n\\(DFFITS_{i}\\) measures the influence of the \\(i\\)th observation on the \\(i\\)th fitted value, again in standard deviation units. \\[DFFITS_{i} := \\frac{\\hat{y}_i - \\hat{y}_{(i)}}{\\sqrt{s_{(i)}^2h_{ii}}} = \\left(\\frac{h_{ii}}{1-h_{ii}}\\right)^{1/2}t_i\\] where \\(\\hat{y}_{(i)}\\) is the fitted value of \\(y_i\\) computed without the \\(i\\)th observation.\nThe denominator provides a standardization since \\(\\mathrm{Var}\\left( \\hat{y}_i\\right) = \\sigma^2h_{ii}\\). \n\n\n\\(DFFITS_{i}\\)is essentially the R-student residual scaled by the leverage \\([h_{ii}/(1-h_{ii})]^{1/2}\\).\n\n\nA point with \\(|DFFITS_{i}| > 2\\sqrt{p/n}\\) needs attention.\n\n\nChatterjee and Hadi (1988): cutoff \\(2\\sqrt{p/(n-p)}\\)"
  },
  {
    "objectID": "slides/07-diag-unusual.html#r-lab-dffits_i",
    "href": "slides/07-diag-unusual.html#r-lab-dffits_i",
    "title": "Regression Diagnostics: Unusual Data üë©‚Äçüíª",
    "section": "\nR Lab \\(DFFITS_{i}\\)\n",
    "text": "R Lab \\(DFFITS_{i}\\)\n\n\nround(dffit <- dffits(delivery_lm), 2)\n\n    1     2     3     4     5     6     7     8     9    10    11    12    13 \n-0.57  0.10 -0.01  0.50 -0.04 -0.02  0.08  0.09  4.30  0.40  0.22 -0.07  0.08 \n   14    15    16    17    18    19    20    21    22    23    24    25 \n 0.10  0.04 -0.10  0.03  0.37  0.19 -0.67 -0.39 -1.20 -0.31 -0.57 -0.02 \n\n## cut-off = 0.69\ndffit[abs(dffit) > 2 * sqrt(p/n)]  \n\n   9   22 \n 4.3 -1.2 \n\n\n\nDeleting point 9 displaces the predicted response \\(\\hat{y}_{(i)}\\) by over four standard deviations of \\(\\hat{y}_i.\\)"
  },
  {
    "objectID": "slides/07-diag-unusual.html#covratio_i",
    "href": "slides/07-diag-unusual.html#covratio_i",
    "title": "Regression Diagnostics: Unusual Data üë©‚Äçüíª",
    "section": "\\(COVRATIO_{i}\\)",
    "text": "\\(COVRATIO_{i}\\)\n\n\\(DFFITS_{i}\\) and \\(DFBETAS_{j, i}\\) reflect influence on \\(\\hat{y}_i\\) and \\(b_j\\), but do not indicate whether or not the presence of the \\(i\\)th point appreciably sharpened the estimation of the coefficient. \nA scalar measure of precision, called generalized variance of \\({\\bf b}\\) is \\[GV({\\bf b}) = \\det\\left( \\mathrm{Cov}({\\bf b}) \\right) = \\det\\left( \\sigma^2 ({\\bf X'X})^{-1} \\right) \\]\n\nTo express the role of the \\(i\\)th observation on the precision of estimation, we use \\[COVRATIO_{i} = \\frac{ \\det \\left( \\left( {\\bf X}_{(i)}' {\\bf X}_{(i)} \\right)^{-1} s_{(i)}^2 \\right) }{\\det \\left(  \\left( {\\bf X'X} \\right)^{-1} s^2\\right)} = \\frac{ \\left( s_{(i)}^2 \\right)^p}{\\left( s^2 \\right) ^ p}\\left( \\frac{1}{1-h_{ii}}\\right)\\]\n\n\n\\({\\bf X}_{(i)}\\) denotes the \\((n-1) \\times p\\) data matrix with the \\(i\\)th observation eliminated."
  },
  {
    "objectID": "slides/07-diag-unusual.html#covratio_i-1",
    "href": "slides/07-diag-unusual.html#covratio_i-1",
    "title": "Regression Diagnostics: Unusual Data üë©‚Äçüíª",
    "section": "\\(COVRATIO_{i}\\)",
    "text": "\\(COVRATIO_{i}\\)\n\\[\\small COVRATIO_{i} = \\frac{ \\det \\left( \\left( {\\bf X}_{(i)}' {\\bf X}_{(i)} \\right)^{-1} s_{(i)}^2 \\right) }{\\det \\left(  \\left( {\\bf X'X} \\right)^{-1} s^2 \\right)} = \\frac{ \\left( s_{(i)}^2 \\right)^p}{\\left( s^2 \\right) ^ p}\\left( \\frac{1}{1-h_{ii}}\\right)\\]\n\nIf \\(COVRATIO_{i} > 1\\), the \\(i\\)th case improves the precision.\nIf \\(COVRATIO_{i} < 1\\), the \\(i\\)th case degrades the precision.\nHigher leverage \\(h_{ii}\\) leads to larger \\(COVRATIO_{i}\\) and improves the precision unless the point is an outlier in \\(y\\) space.\nIf \\(i\\)th point is an outlier, \\(\\frac{s_{(i)}^2}{s^2} < 1\\).\n\n\nCutoffs:\n\n\n\\(COVRATIO_{i} > 1 + 3p/n\\) or\n\n\\(COVRATIO_{i} < 1 - 3p/n\\) provided that \\(n > 3p\\).\n\n\n\n\n\\(\\frac{1}{1-h_{ii}}\\) is the ratio of \\(\\small \\det \\left( \\left( {\\bf X}_{(i)}' {\\bf X}_{(i)} \\right)^{-1} \\right)\\) to \\(\\small \\det \\left( \\left( {\\bf X}' {\\bf X} \\right)^{-1} \\right)\\)"
  },
  {
    "objectID": "slides/07-diag-unusual.html#r-lab-covratio_i",
    "href": "slides/07-diag-unusual.html#r-lab-covratio_i",
    "title": "Regression Diagnostics: Unusual Data üë©‚Äçüíª",
    "section": "\nR Lab \\(COVRATIO_{i}\\)\n",
    "text": "R Lab \\(COVRATIO_{i}\\)\n\n\n(covra <- covratio(delivery_lm))\n\n   1    2    3    4    5    6    7    8    9   10   11   12   13   14   15   16 \n0.87 1.21 1.28 0.88 1.24 1.20 1.24 1.21 0.34 1.31 1.17 1.29 1.21 1.23 1.19 1.37 \n  17   18   19   20   21   22   23   24   25 \n1.22 1.07 1.22 0.76 1.24 1.40 0.89 0.95 1.23 \n\n## cutoff 1.36\ncovra[covra > (1 + 3*p/n)]  \n\n 16  22 \n1.4 1.4 \n\n## cutoff 0.64\ncovra[covra < (1 - 3*p/n)] \n\n   9 \n0.34 \n\n\n\nSince \\(COVRATIO_{9} < 1\\), this observation degrades precision of estimation.\nSince \\(COVRATIO_{22} > 1\\), this observation tends to improve precision of estimation.\nPoint 22 and 16 barely exceed the cutoff, so their influence is small."
  },
  {
    "objectID": "slides/07-diag-unusual.html#jointly-influential-points",
    "href": "slides/07-diag-unusual.html#jointly-influential-points",
    "title": "Regression Diagnostics: Unusual Data üë©‚Äçüíª",
    "section": "Jointly Influential Points",
    "text": "Jointly Influential Points\n\nSubset of cases can be jointly influential or can offset each other‚Äôs influence.\n\n\n\nSometimes subset of cases can be jointly influential or can offset each other‚Äôs influence. One single point may not be that influential, but with another point, the fitting result changes a lot. Or one single point may be influential, but with another point, the influence power disappears.\nOften, influential subsets of cases or multiple outliers can be identified by applying single -case deletion diagnostics sequentially.\n\ntwo jointly influential cases located close to one another. Deletion of both cases has a much greater impact than deletion of only one.\n\n\ntwo jointly influential cases located on opposite sides of the data. Deletion of both cases has a much greater impact than deletion of only one.\n\n\ncases that offset one another: the regression with both cases deleted is nearlythe same as for the whole data set."
  },
  {
    "objectID": "slides/07-diag-unusual.html#detecting-joint-influence-added-variable-plot",
    "href": "slides/07-diag-unusual.html#detecting-joint-influence-added-variable-plot",
    "title": "Regression Diagnostics: Unusual Data üë©‚Äçüíª",
    "section": "Detecting Joint Influence: Added-Variable Plot",
    "text": "Detecting Joint Influence: Added-Variable Plot\n\n\n\\(y_i= \\beta_0 + \\beta_1x_{i1} + \\beta_2x_{i2} + \\dots + \\beta_kx_{ik} + \\epsilon_i\\).\n\nTo create the added-variable plot for \\(x_1\\),\n\n\nStep 1:  Regress \\(y\\) on all predictors except \\(x_1\\) and obtain residuals  \\[\\small \\begin{align}\\hat{y}_i(x_{(1)}) &= b_0^{(1)} + b_2^{(1)}x_{i2} + \\dots + b_k^{(1)}x_{ik}\\\\e_i(y \\mid x_{(1)}) &= y_i - \\hat{y}_i(x_{(1)}) \\end{align}\\]\n\n\n\n\n\nStep 2:  Regress \\(x_1\\) on all other predictors and obtain residuals \\[\\small \\begin{align}\\hat{x}_{i1}(x_{(1)}) &= a_0^{(1)} + a_2^{(1)}x_{i2} + \\dots + a_k^{(1)}x_{ik}\\\\e_i(x_1 \\mid x_{(1)}) &= x_{i1} - \\hat{x}_{i1}(x_{(1)}) \\end{align}\\]\n\n\n\n\n\n\nStep 3:  Plot \\(e_i(y \\mid x_{(1)})\\) vs.¬†\\(e_i(x_1 \\mid x_{(1)}), \\quad i = 1, \\dots, n\\)\n\n\n\nAdded-Variable Plot is a quite useful plot. We can use it for many other purposes, not just detecting joint influence. We‚Äôll talk about that later.\nFor each predictor, we can create its own Added-Variable Plot, so if we have k predictors, we can have k Added-Variable Plots.\nThe idea of the plot is that we would like to see the pure effect of a predictor on the response when all other predictors are in the model.\nSo how do we get the pure effect of \\(x_1\\)?\nStep 1: Regress y on all predictors except x1 and obtain residuals. The residuals are the variation of y that cannot be explained by x2 to xk. So if later we see the variation further goes down, that must be due to the inclusion of x1.\nStep 2: Regress x1 on all other predictors. The residuals are the variation of x1 that cannot be explained by x2 to xk. This gives us the\n\n\n\n\nFor any \\(x_j\\), \\(e_i(y \\mid x_{(j)})\\) vs.¬†\\(e_i(x_j \\mid x_{(j)})\\)\n\n\n\nThe collection of of added-variable plots for x1, ‚Ä¶, xk convert the graph for multiple regression into a sequence of 2-dimensional plot.\nplotting \\(e_i(y \\mid x_{(j)})\\) vs.¬†\\(e_i(x_j \\mid x_{(j)}), \\quad i = 1, \\dots, n\\) allows us to examine the leverage and influence of the cases on \\(b_j\\)."
  },
  {
    "objectID": "slides/07-diag-unusual.html#r-lab-duncans-occupational-prestige-data",
    "href": "slides/07-diag-unusual.html#r-lab-duncans-occupational-prestige-data",
    "title": "Regression Diagnostics: Unusual Data üë©‚Äçüíª",
    "section": "\nR Lab Duncan‚Äôs Occupational Prestige Data",
    "text": "R Lab Duncan‚Äôs Occupational Prestige Data\n\n\n\nlibrary(carData)\ncarData::Duncan[sample(dim(Duncan)[1], 16), ]\n\n                   type income education prestige\nplumber              bc     44        25       29\nchemist            prof     64        86       90\ngas.stn.attendant    bc     15        29       10\nstore.manager      prof     42        44       45\nteacher            prof     48        91       73\ninsurance.agent      wc     55        71       41\nbookkeeper           wc     29        72       39\nauthor             prof     55        90       76\nelectrician          bc     47        39       53\nlawyer             prof     76        98       89\npoliceman            bc     34        47       41\nmail.carrier         wc     48        55       34\nsoda.clerk           bc     12        30        6\nmachinist            bc     36        32       57\nbanker             prof     78        82       92\nstreetcar.motorman   bc     42        26       19\n\n\n\n\n\ntype: Type of occupation\n\n\nprof (professional and managerial)\n\nwc (white-collar)\n\nbc (blue-collar)\n\n\nincome: Percentage who earned $3,500 or more per year\neducation: Percentage who were high school graduates\nprestige: Percentage of respondents in a survey who rated the occupation as ‚Äúgood‚Äù or better in prestige\n\n\n\n\n1950 data"
  },
  {
    "objectID": "slides/07-diag-unusual.html#r-lab-added-valued-plot",
    "href": "slides/07-diag-unusual.html#r-lab-added-valued-plot",
    "title": "Regression Diagnostics: Unusual Data üë©‚Äçüíª",
    "section": "\nR Lab Added-Valued Plot",
    "text": "R Lab Added-Valued Plot\n\nThe slope of the least square simple regression line of \\(e_i(y \\mid x_{(1)})\\) on \\(e_i(x_1 \\mid x_{(1)})\\) is the same as the slope \\(b_1\\) for \\(x_1\\) in the full multiple regression.\n\n\n\nduncan_fit <- lm(prestige ~ income + education, data = Duncan)\ncar::avPlot(model = duncan_fit, variable = \"income\", id = list(method = \"mahal\", n = 3), \n            xlab = \"e(Income | Education)\", ylab = \"e(Prestige | Education)\", pch = 16)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMinister‚Äôs income is low given its education level.\nRailroad engineer and railroad conductor‚Äôs income is high given their education level. \n\nThe points for minister and conductor appear to be working jointly to decrease the income slope.\n\n\n\nidentifies the 3 points with the largest Mahalanobis distances from the center of the data."
  },
  {
    "objectID": "slides/07-diag-unusual.html#r-lab-bubble-plot",
    "href": "slides/07-diag-unusual.html#r-lab-bubble-plot",
    "title": "Regression Diagnostics: Unusual Data üë©‚Äçüíª",
    "section": "\nR Lab Bubble Plot",
    "text": "R Lab Bubble Plot\n\n\n\nEach point is plotted as a circle with area proportional to Cook‚Äôs distance.\nHorizontal lines are drawn R-Student residuals of 0 and \\(\\pm 2\\).\nThe vertical lines at \\(2\\bar{h}\\) and \\(3\\bar{h}\\).\n\n\n\ncar::influencePlot(duncan_fit)\n\n\n\n\n\n\n\n            StudRes   Hat CookD\nminister       3.13 0.173 0.566\nreporter      -2.40 0.054 0.099\nconductor     -1.70 0.195 0.224\nRR.engineer    0.81 0.269 0.081"
  },
  {
    "objectID": "slides/07-diag-unusual.html#treatment-of-unusual-data",
    "href": "slides/07-diag-unusual.html#treatment-of-unusual-data",
    "title": "Regression Diagnostics: Unusual Data üë©‚Äçüíª",
    "section": "Treatment of Unusual Data",
    "text": "Treatment of Unusual Data\n\nShould unusual data be discarded?\n\n\n\nFirst investigate why data are unusual.\n\nError in recording:\n\nIf the typo can be corrected, correct it and keep it.\nIf it cannot be corrected, discard it.\n\n\n\n\n\n\nIf the unusual point is known to be correct, understand why.\n\nOutliers or influential data may also motivate model respecification.\n\nThe pattern of outlying data may suggest introducing additional regressors.\n\n\n\nIt is tempting to remove outliers. Don‚Äôt do this without a very good reason. Models that ignore exceptional (and interesting) cases often perform poorly. For instance, if a financial firm ignored the largest market swings ‚Äì the ‚Äúoutliers‚Äù ‚Äì they would soon go bankrupt by making poorly thought-out investments. A compromise approach (Advanced): - Use robust estimation that downweights observations in proportion to residual magnitude or influence. - A highly influential observation will receive less weight than it would in a least-squares fit.\n\n\n\nmath4780-f23.github.io/website"
  },
  {
    "objectID": "supplemental/mlr-matrix.html",
    "href": "supplemental/mlr-matrix.html",
    "title": "Matrix Notation for Multiple Linear Regression",
    "section": "",
    "text": "Note\n\n\n\nThe following supplemental notes were created by Dr.¬†Maria Tackett for STA 210. They are provided for students who want to dive deeper into the mathematics behind regression and reflect some of the material covered in STA 211: Mathematics of Regression. Additional supplemental notes will be added throughout the semester.\nThis document provides the details for the matrix notation for multiple linear regression. We assume the reader has familiarity with some linear algebra. Please see Chapter 1 of An Introduction to Statistical Learning for a brief review of linear algebra."
  },
  {
    "objectID": "supplemental/mlr-matrix.html#introduction",
    "href": "supplemental/mlr-matrix.html#introduction",
    "title": "Matrix Notation for Multiple Linear Regression",
    "section": "Introduction",
    "text": "Introduction\nSuppose we have \\(n\\) observations. Let the \\(i^{th}\\) be \\((x_{i1}, \\ldots, x_{ip}, y_i)\\), such that \\(x_{i1}, \\ldots, x_{ip}\\) are the explanatory variables (predictors) and \\(y_i\\) is the response variable. We assume the data can be modeled using the least-squares regression model, such that the mean response for a given combination of explanatory variables follows the form in Equation¬†1.\n\\[\ny = \\beta_0 + \\beta_1 x_1 + \\dots + \\beta_p x_p\n\\tag{1}\\]\nWe can write the response for the \\(i^{th}\\) observation as shown in Equation¬†2\n\\[\ny_i = \\beta_0 + \\beta_1 x_{i1} + \\dots + \\beta_p x_{ip} + \\epsilon_i\n\\tag{2}\\]\nsuch that \\(\\epsilon_i\\) is the amount \\(y_i\\) deviates from \\(\\mu\\{y|x_{i1}, \\ldots, x_{ip}\\}\\), the mean response for a given combination of explanatory variables. We assume each \\(\\epsilon_i \\sim N(0,\\sigma^2)\\), where \\(\\sigma^2\\) is a constant variance for the distribution of the response \\(y\\) for any combination of explanatory variables \\(x_1, \\ldots, x_p\\)."
  },
  {
    "objectID": "supplemental/mlr-matrix.html#matrix-representation-for-the-regression-model",
    "href": "supplemental/mlr-matrix.html#matrix-representation-for-the-regression-model",
    "title": "Matrix Notation for Multiple Linear Regression",
    "section": "Matrix Representation for the Regression Model",
    "text": "Matrix Representation for the Regression Model\nWe can represent the Equation¬†1 and Equation¬†2 using matrix notation. Let\n\\[\n\\mathbf{Y} = \\begin{bmatrix}y_1 \\\\ y_2 \\\\ \\vdots \\\\y_n\\end{bmatrix}\n\\hspace{15mm}\n\\mathbf{X} = \\begin{bmatrix}x_{11} & x_{12} & \\dots & x_{1p} \\\\\nx_{21} & x_{22} & \\dots & x_{2p} \\\\\n\\vdots & \\vdots & \\ddots & \\vdots\\\\\nx_{n1} & x_{n2} & \\dots & x_{np} \\end{bmatrix}\n\\hspace{15mm}\n\\boldsymbol{\\beta}= \\begin{bmatrix}\\beta_0 \\\\ \\beta_1 \\\\ \\vdots \\\\ \\beta_p \\end{bmatrix}\n\\hspace{15mm}\n\\boldsymbol{\\epsilon}= \\begin{bmatrix}\\epsilon_1 \\\\ \\epsilon_2 \\\\ \\vdots \\\\ \\epsilon_n \\end{bmatrix}\n\\tag{3}\\]\nThus,\n\\[\\mathbf{Y} = \\mathbf{X}\\boldsymbol{\\beta} + \\mathbf{\\epsilon}\\]\nTherefore the estimated response for a given combination of explanatory variables and the associated residuals can be written as\n\\[\n\\hat{\\mathbf{Y}} = \\mathbf{X}\\hat{\\boldsymbol{\\beta}} \\hspace{10mm} \\mathbf{e} = \\mathbf{Y} - \\mathbf{X}\\hat{\\boldsymbol{\\beta}}\n\\tag{4}\\]"
  },
  {
    "objectID": "supplemental/mlr-matrix.html#estimating-the-coefficients",
    "href": "supplemental/mlr-matrix.html#estimating-the-coefficients",
    "title": "Matrix Notation for Multiple Linear Regression",
    "section": "Estimating the Coefficients",
    "text": "Estimating the Coefficients\nThe least-squares model is the one that minimizes the sum of the squared residuals. Therefore, we want to find the coefficients, \\(\\hat{\\boldsymbol{\\beta}}\\) that minimizes\n\\[\n\\sum\\limits_{i=1}^{n} e_{i}^2 = \\mathbf{e}^T\\mathbf{e} = (\\mathbf{Y} - \\mathbf{X}\\hat{\\boldsymbol{\\beta}})^T(\\mathbf{Y} - \\mathbf{X}\\hat{\\boldsymbol{\\beta}})\n\\tag{5}\\]\nwhere \\(\\mathbf{e}^T\\), the transpose of the matrix \\(\\mathbf{e}\\).\n\\[\n(\\mathbf{Y} - \\mathbf{X}\\hat{\\boldsymbol{\\beta}})^T(\\mathbf{Y} - \\mathbf{X}\\hat{\\boldsymbol{\\beta}}) = (\\mathbf{Y}^T\\mathbf{Y} -\n\\mathbf{Y}^T \\mathbf{X}\\hat{\\boldsymbol{\\beta}} - (\\hat{\\boldsymbol{\\beta}}{}^{T}\\mathbf{X}^T\\mathbf{Y} +\n\\hat{\\boldsymbol{\\beta}}{}^{T}\\mathbf{X}^T\\mathbf{X}\n\\hat{\\boldsymbol{\\beta}})\n\\tag{6}\\]\nNote that \\((\\mathbf{Y^T}\\mathbf{X}\\hat{\\boldsymbol{\\beta}})^T = \\hat{\\boldsymbol{\\beta}}{}^{T}\\mathbf{X}^T\\mathbf{Y}\\). Since these are both constants (i.e.¬†\\(1\\times 1\\) vectors), \\(\\mathbf{Y^T}\\mathbf{X}\\hat{\\boldsymbol{\\beta}} = \\hat{\\boldsymbol{\\beta}}{}^{T}\\mathbf{X}^T\\mathbf{Y}\\). Thus, Equation¬†7 becomes\n\\[\n\\mathbf{Y}^T\\mathbf{Y} - 2 \\mathbf{X}^T\\hat{\\boldsymbol{\\beta}}{}^{T}\\mathbf{Y} + \\hat{\\boldsymbol{\\beta}}{}^{T}\\mathbf{X}^T\\mathbf{X}\n\\hat{\\boldsymbol{\\beta}}\n\\tag{7}\\]\nSince we want to find the \\(\\hat{\\boldsymbol{\\beta}}\\) that minimizes Equation¬†5, will find the value of \\(\\hat{\\boldsymbol{\\beta}}\\) such that the derivative with respect to \\(\\hat{\\boldsymbol{\\beta}}\\) is equal to 0.\n\\[\n\\begin{aligned}\n\\frac{\\partial \\mathbf{e}^T\\mathbf{e}}{\\partial \\hat{\\boldsymbol{\\beta}}} & = \\frac{\\partial}{\\partial \\hat{\\boldsymbol{\\beta}}}(\\mathbf{Y}^T\\mathbf{Y} - 2 \\mathbf{X}^T\\hat{\\boldsymbol{\\beta}}{}^T\\mathbf{Y} + \\hat{\\boldsymbol{\\beta}}{}^{T}\\mathbf{X}^T\\mathbf{X}\\hat{\\boldsymbol{\\beta}}) = 0 \\\\\n&\\Rightarrow - 2 \\mathbf{X}^T\\mathbf{Y} + 2 \\mathbf{X}^T\\mathbf{X}\\hat{\\boldsymbol{\\beta}} = 0 \\\\\n& \\Rightarrow 2 \\mathbf{X}^T\\mathbf{Y} = 2 \\mathbf{X}^T\\mathbf{X}\\hat{\\boldsymbol{\\beta}} \\\\\n& \\Rightarrow \\mathbf{X}^T\\mathbf{Y} = \\mathbf{X}^T\\mathbf{X}\\hat{\\boldsymbol{\\beta}} \\\\\n& \\Rightarrow (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{Y} = (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{X}\\hat{\\boldsymbol{\\beta}} \\\\\n& \\Rightarrow (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{Y} = \\mathbf{I}\\hat{\\boldsymbol{\\beta}}\n\\end{aligned}\n\\tag{8}\\]\nThus, the estimate of the model coefficients is \\(\\hat{\\boldsymbol{\\beta}} = (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{Y}\\)."
  },
  {
    "objectID": "supplemental/mlr-matrix.html#variance-covariance-matrix-of-the-coefficients",
    "href": "supplemental/mlr-matrix.html#variance-covariance-matrix-of-the-coefficients",
    "title": "Matrix Notation for Multiple Linear Regression",
    "section": "Variance-covariance matrix of the coefficients",
    "text": "Variance-covariance matrix of the coefficients\nWe will use two properties to derive the form of the variance-covariance matrix of the coefficients:\n\n\\(E[\\boldsymbol{\\epsilon}\\boldsymbol{\\epsilon}^T] = \\sigma^2I\\)\n\\(\\hat{\\boldsymbol{\\beta}} = \\boldsymbol{\\beta} + (\\mathbf{X}^T\\mathbf{X})^{-1}\\epsilon\\)\n\nFirst, we will show that \\(E[\\boldsymbol{\\epsilon}\\boldsymbol{\\epsilon}^T] = \\sigma^2I\\)\n\\[\n\\begin{aligned}\nE[\\boldsymbol{\\epsilon}\\boldsymbol{\\epsilon}^T] &= E \\begin{bmatrix}\\epsilon_1  & \\epsilon_2 & \\dots & \\epsilon_n \\end{bmatrix}\\begin{bmatrix}\\epsilon_1 \\\\ \\epsilon_2 \\\\ \\vdots \\\\ \\epsilon_n \\end{bmatrix}  \\\\\n& = E \\begin{bmatrix} \\epsilon_1^2  & \\epsilon_1 \\epsilon_2 & \\dots & \\epsilon_1 \\epsilon_n \\\\\n\\epsilon_2 \\epsilon_1 & \\epsilon_2^2 & \\dots & \\epsilon_2 \\epsilon_n \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\n\\epsilon_n \\epsilon_1 & \\epsilon_n \\epsilon_2 & \\dots & \\epsilon_n^2\n\\end{bmatrix} \\\\\n& = \\begin{bmatrix} E[\\epsilon_1^2]  & E[\\epsilon_1 \\epsilon_2] & \\dots & E[\\epsilon_1 \\epsilon_n] \\\\\nE[\\epsilon_2 \\epsilon_1] & E[\\epsilon_2^2] & \\dots & E[\\epsilon_2 \\epsilon_n] \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\nE[\\epsilon_n \\epsilon_1] & E[\\epsilon_n \\epsilon_2] & \\dots & E[\\epsilon_n^2]\n\\end{bmatrix}\n\\end{aligned}\n\\tag{9}\\]\nRecall, the regression assumption that the errors \\(\\epsilon_i's\\) are Normally distributed with mean 0 and variance \\(\\sigma^2\\). Thus, \\(E(\\epsilon_i^2) = Var(\\epsilon_i) = \\sigma^2\\) for all \\(i\\). Additionally, recall the regression assumption that the errors are uncorrelated, i.e.¬†\\(E(\\epsilon_i \\epsilon_j) = Cov(\\epsilon_i, \\epsilon_j) = 0\\) for all \\(i,j\\). Using these assumptions, we can write Equation¬†9 as\n\\[\nE[\\mathbf{\\epsilon}\\mathbf{\\epsilon}^T]  = \\begin{bmatrix} \\sigma^2  & 0 & \\dots & 0 \\\\\n0 & \\sigma^2  & \\dots & 0 \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\n0 & 0 & \\dots & \\sigma^2\n\\end{bmatrix} = \\sigma^2 \\mathbf{I}\n\\tag{10}\\]\nwhere \\(\\mathbf{I}\\) is the \\(n \\times n\\) identity matrix.\nNext, we show that \\(\\hat{\\boldsymbol{\\beta}} = \\boldsymbol{\\beta} + (\\mathbf{X}^T\\mathbf{X})^{-1}\\epsilon\\).\nRecall that the \\(\\hat{\\boldsymbol{\\beta}} = (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{Y}\\) and \\(\\mathbf{Y} = \\mathbf{X}\\mathbf{\\beta} + \\mathbf{\\epsilon}\\). Then,\n\\[\n\\begin{aligned}\n\\hat{\\boldsymbol{\\beta}} &= (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{Y} \\\\\n&= (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T(\\mathbf{X}\\boldsymbol{\\beta} + \\boldsymbol{\\epsilon}) \\\\\n&= \\boldsymbol{\\beta} + (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T \\mathbf{\\epsilon} \\\\\n\\end{aligned}\n\\tag{11}\\]\nUsing these two properties, we derive the form of the variance-covariance matrix for the coefficients. Note that the covariance matrix is \\(E[(\\hat{\\boldsymbol{\\beta}} - \\boldsymbol{\\beta})(\\hat{\\boldsymbol{\\beta}} - \\boldsymbol{\\beta})^T]\\)\n\\[\n\\begin{aligned}\nE[(\\hat{\\boldsymbol{\\beta}} - \\boldsymbol{\\beta})(\\hat{\\boldsymbol{\\beta}} - \\boldsymbol{\\beta})^T] &= E[(\\boldsymbol{\\beta} + (\\mathbf{X}^T\\mathbf{X})^{-1} \\mathbf{X}^T \\boldsymbol{\\epsilon} - \\boldsymbol{\\beta})(\\boldsymbol{\\beta} + (\\mathbf{X}^T\\mathbf{X})^{-1} \\mathbf{X}^T \\boldsymbol{\\epsilon} - \\boldsymbol{\\beta})^T]\\\\\n& = E[(\\mathbf{X}^T\\mathbf{X})^{-1} \\mathbf{X}^T \\boldsymbol{\\epsilon}\\boldsymbol{\\epsilon}^T\\mathbf{X}(\\mathbf{X}^T\\mathbf{X})^{-1}] \\\\\n& = (\\mathbf{X}^T\\mathbf{X})^{-1} \\mathbf{X}^T E[\\boldsymbol{\\epsilon}\\boldsymbol{\\epsilon}^T]\\mathbf{X}(\\mathbf{X}^T\\mathbf{X})^{-1}\\\\\n& = (\\mathbf{X}^T\\mathbf{X})^{-1} \\mathbf{X}^T (\\sigma^2\\mathbf{I})\\mathbf{X}(\\mathbf{X}^T\\mathbf{X})^{-1}\\\\\n&= \\sigma^2\\mathbf{I}(\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{X}(\\mathbf{X}^T\\mathbf{X})^{-1}\\\\\n& = \\sigma^2\\mathbf{I}(\\mathbf{X}^T\\mathbf{X})^{-1}\\\\\n&  = \\sigma^2(\\mathbf{X}^T\\mathbf{X})^{-1} \\\\\n\\end{aligned}\n\\tag{12}\\]"
  },
  {
    "objectID": "supplemental/model-diagnostics-matrix.html",
    "href": "supplemental/model-diagnostics-matrix.html",
    "title": "Model Diagnostics",
    "section": "",
    "text": "Note\n\n\n\nThe following supplemental notes were created by Dr.¬†Maria Tackett for STA 210. They are provided for students who want to dive deeper into the mathematics behind regression and reflect some of the material covered in STA 211: Mathematics of Regression. Additional supplemental notes will be added throughout the semester.\nThis document discusses some of the mathematical details of the model diagnostics - leverage, standardized residuals, and Cook‚Äôs distance. We assume the reader knowledge of the matrix form for multiple linear regression. Please see Matrix Form of Linear Regression for a review."
  },
  {
    "objectID": "supplemental/model-diagnostics-matrix.html#introduction",
    "href": "supplemental/model-diagnostics-matrix.html#introduction",
    "title": "Model Diagnostics",
    "section": "Introduction",
    "text": "Introduction\nSuppose we have \\(n\\) observations. Let the \\(i^{th}\\) be \\((x_{i1}, \\ldots, x_{ip}, y_i)\\), such that \\(x_{i1}, \\ldots, x_{ip}\\) are the explanatory variables (predictors) and \\(y_i\\) is the response variable. We assume the data can be modeled using the least-squares regression model, such that the mean response for a given combination of explanatory variables follows the form in Equation¬†1.\n\\[\ny = \\beta_0 + \\beta_1 x_1 + \\dots + \\beta_p x_p\n\\tag{1}\\]\nWe can write the response for the \\(i^{th}\\) observation as shown in Equation¬†2.\n\\[\ny_i = \\beta_0 + \\beta_1 x_{i1} + \\dots + \\beta_p x_{ip} + \\epsilon_i\n\\tag{2}\\]\nsuch that \\(\\epsilon_i\\) is the amount \\(y_i\\) deviates from \\(\\mu\\{y|x_{i1}, \\ldots, x_{ip}\\}\\), the mean response for a given combination of explanatory variables. We assume each \\(\\epsilon_i \\sim N(0,\\sigma^2)\\), where \\(\\sigma^2\\) is a constant variance for the distribution of the response \\(y\\) for any combination of explanatory variables \\(x_1, \\ldots, x_p\\)."
  },
  {
    "objectID": "supplemental/model-diagnostics-matrix.html#matrix-form-for-the-regression-model",
    "href": "supplemental/model-diagnostics-matrix.html#matrix-form-for-the-regression-model",
    "title": "Model Diagnostics",
    "section": "Matrix Form for the Regression Model",
    "text": "Matrix Form for the Regression Model\nWe can represent the Equation¬†1 and Equation¬†2 using matrix notation. Let\n\\[\n\\mathbf{Y} = \\begin{bmatrix}y_1 \\\\ y_2 \\\\ \\vdots \\\\y_n\\end{bmatrix}\n\\hspace{15mm}\n\\mathbf{X} = \\begin{bmatrix}x_{11} & x_{12} & \\dots & x_{1p} \\\\\nx_{21} & x_{22} & \\dots & x_{2p} \\\\\n\\vdots & \\vdots & \\ddots & \\vdots\\\\\nx_{n1} & x_{n2} & \\dots & x_{np} \\end{bmatrix}\n\\hspace{15mm}\n\\boldsymbol{\\beta}= \\begin{bmatrix}\\beta_0 \\\\ \\beta_1 \\\\ \\vdots \\\\ \\beta_p \\end{bmatrix}\n\\hspace{15mm}\n\\boldsymbol{\\epsilon}= \\begin{bmatrix}\\epsilon_1 \\\\ \\epsilon_2 \\\\ \\vdots \\\\ \\epsilon_n \\end{bmatrix}\n\\tag{3}\\]\nThus,\n\\[\\mathbf{Y} = \\mathbf{X}\\boldsymbol{\\beta} + \\mathbf{\\epsilon}\\]\nTherefore the estimated response for a given combination of explanatory variables and the associated residuals can be written as\n\\[\n\\hat{\\mathbf{Y}} = \\mathbf{X}\\hat{\\boldsymbol{\\beta}} \\hspace{10mm} \\mathbf{e} = \\mathbf{Y} - \\mathbf{X}\\hat{\\boldsymbol{\\beta}}\n\\tag{4}\\]"
  },
  {
    "objectID": "supplemental/model-diagnostics-matrix.html#hat-matrix-leverage",
    "href": "supplemental/model-diagnostics-matrix.html#hat-matrix-leverage",
    "title": "Model Diagnostics",
    "section": "Hat Matrix & Leverage",
    "text": "Hat Matrix & Leverage\nRecall from the notes Matrix Form of Linear Regression that \\(\\hat{\\boldsymbol{\\beta}}\\) can be written as the following:\n\\[\n\\hat{\\boldsymbol{\\beta}} = (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{Y}\n\\tag{5}\\]\nCombining Equation¬†4 and Equation¬†5, we can write \\(\\hat{\\mathbf{Y}}\\) as the following:\n\\[\n\\begin{aligned}\n\\hat{\\mathbf{Y}} &= \\mathbf{X}\\hat{\\boldsymbol{\\beta}} \\\\[10pt]\n&= \\mathbf{X}(\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{Y}\\\\\n\\end{aligned}\n\\tag{6}\\]\nWe define the hat matrix as an \\(n \\times n\\) matrix of the form \\(\\mathbf{H} = \\mathbf{X}(\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\). Thus Equation¬†6 becomes\n\\[\n\\hat{\\mathbf{Y}} = \\mathbf{H}\\mathbf{Y}\n\\tag{7}\\]\nThe diagonal elements of the hat matrix are a measure of how far the predictor variables of each observation are from the means of the predictor variables. For example, \\(h_{ii}\\) is a measure of how far the values of the predictor variables for the \\(i^{th}\\) observation, \\(x_{i1}, x_{i2}, \\ldots, x_{ip}\\), are from the mean values of the predictor variables, \\(\\bar{x}_1, \\bar{x}_2, \\ldots, \\bar{x}_p\\). In the case of simple linear regression, the \\(i^{th}\\) diagonal, \\(h_{ii}\\), can be written as\n\\[\nh_{ii} =  \\frac{1}{n} + \\frac{(x_i - \\bar{x})^2}{\\sum_{j=1}^{n}(x_j-\\bar{x})^2}\n\\]\nWe call these diagonal elements, the leverage of each observation.\nThe diagonal elements of the hat matrix have the following properties:\n\n\\(0 \\leq h_ii \\leq 1\\)\n\\(\\sum\\limits_{i=1}^{n} h_{ii} = p+1\\), where \\(p\\) is the number of predictor variables in the model.\nThe mean hat value is \\(\\bar{h} = \\frac{\\sum\\limits_{i=1}^{n} h_{ii}}{n} = \\frac{p+1}{n}\\).\n\nUsing these properties, we consider a point to have high leverage if it has a leverage value that is more than 2 times the average. In other words, observations with leverage greater than \\(\\frac{2(p+1)}{n}\\) are considered to be high leverage points, i.e.¬†outliers in the predictor variables. We are interested in flagging high leverage points, because they may have an influence on the regression coefficients.\nWhen there are high leverage points in the data, the regression line will tend towards those points; therefore, one property of high leverage points is that they tend to have small residuals. We will show this by rewriting the residuals from Equation¬†4 using Equation¬†7.\n\\[\n\\begin{aligned}\n\\mathbf{e} &= \\mathbf{Y} - \\hat{\\mathbf{Y}} \\\\[10pt]\n& = \\mathbf{Y} - \\mathbf{H}\\mathbf{Y} \\\\[10pt]\n&= (1-\\mathbf{H})\\mathbf{Y}\n\\end{aligned}\n\\tag{8}\\]\nNote that the identity matrix and hat matrix are idempotent, i.e.¬†\\(\\mathbf{I}\\mathbf{I} = \\mathbf{I}\\), \\(\\mathbf{H}\\mathbf{H} = \\mathbf{H}\\). Thus, \\((\\mathbf{I} - \\mathbf{H})\\) is also idempotent. These matrices are also symmetric. Using these properties and Equation¬†8, we have that the variance-covariance matrix of the residuals \\(\\boldsymbol{e}\\), is\n\\[\n\\begin{aligned}\nVar(\\mathbf{e}) &= \\mathbf{e}\\mathbf{e}^T \\\\[10pt]\n&=  (1-\\mathbf{H})Var(\\mathbf{Y})^T(1-\\mathbf{H})^T \\\\[10pt]\n&= (1-\\mathbf{H})\\hat{\\sigma}^2(1-\\mathbf{H})^T  \\\\[10pt]\n&= \\hat{\\sigma}^2(1-\\mathbf{H})(1-\\mathbf{H})  \\\\[10pt]\n&= \\hat{\\sigma}^2(1-\\mathbf{H})\n\\end{aligned}\n\\tag{9}\\]\nwhere \\(\\hat{\\sigma}^2 = \\frac{\\sum_{i=1}^{n}e_i^2}{n-p-1}\\) is the estimated regression variance. Thus, the variance of the \\(i^{th}\\) residual is \\(Var(e_i) = \\hat{\\sigma}^2(1-h_{ii})\\). Therefore, the higher the leverage, the smaller the variance of the residual. Because the expected value of the residuals is 0, we conclude that points with high leverage tend to have smaller residuals than points with lower leverage."
  },
  {
    "objectID": "supplemental/model-diagnostics-matrix.html#standardized-residuals",
    "href": "supplemental/model-diagnostics-matrix.html#standardized-residuals",
    "title": "Model Diagnostics",
    "section": "Standardized Residuals",
    "text": "Standardized Residuals\nIn general, we standardize a value by shifting by the expected value and rescaling by the standard deviation (or standard error). Thus, the \\(i^{th}\\) standardized residual takes the form\n\\[\nstd.res_i = \\frac{e_i - E(e_i)}{SE(e_i)}\n\\]\nThe expected value of the residuals is 0, i.e.¬†\\(E(e_i) = 0\\). From Equation¬†9), the standard error of the residual is \\(SE(e_i) = \\hat{\\sigma}\\sqrt{1-h_{ii}}\\). Therefore,\n\\[\nstd.res_i = \\frac{e_i}{\\hat{\\sigma}\\sqrt{1-h_{ii}}}\n\\tag{10}\\]"
  },
  {
    "objectID": "supplemental/model-diagnostics-matrix.html#cooks-distance",
    "href": "supplemental/model-diagnostics-matrix.html#cooks-distance",
    "title": "Model Diagnostics",
    "section": "Cook‚Äôs Distance",
    "text": "Cook‚Äôs Distance\nCook‚Äôs distance is a measure of how much each observation influences the model coefficients, and thus the predicted values. The Cook‚Äôs distance for the \\(i^{th}\\) observation can be written as\n\\[\nD_i = \\frac{(\\hat{\\mathbf{Y}} -\\hat{\\mathbf{Y}}_{(i)})^T(\\hat{\\mathbf{Y}} -\\hat{\\mathbf{Y}}_{(i)})}{(p+1)\\hat{\\sigma}}\n\\tag{11}\\]\nwhere \\(\\hat{\\mathbf{Y}}_{(i)}\\) is the vector of predicted values from the model fitted when the \\(i^{th}\\) observation is deleted. Cook‚Äôs Distance can be calculated without deleting observations one at a time, since Equation¬†12 below is mathematically equivalent to Equation¬†11.\n\\[\nD_i = \\frac{1}{p+1}std.res_i^2\\Bigg[\\frac{h_{ii}}{(1-h_{ii})}\\Bigg] = \\frac{e_i^2}{(p+1)\\hat{\\sigma}^2(1-h_{ii})}\\Bigg[\\frac{h_{ii}}{(1-h_{ii})}\\Bigg]\n\\tag{12}\\]"
  },
  {
    "objectID": "supplemental/slr-derivations.html",
    "href": "supplemental/slr-derivations.html",
    "title": "Deriving the Least-Squares Estimates for Simple Linear Regression",
    "section": "",
    "text": "Note\n\n\n\nThe following supplemental notes were created by Dr.¬†Maria Tackett for STA 210. They are provided for students who want to dive deeper into the mathematics behind regression and reflect some of the material covered in STA 211: Mathematics of Regression. Additional supplemental notes will be added throughout the semester.\n\n\nThis document contains the mathematical details for deriving the least-squares estimates for slope (\\(\\beta_1\\)) and intercept (\\(\\beta_0\\)). We obtain the estimates, \\(\\hat{\\beta}_1\\) and \\(\\hat{\\beta}_0\\) by finding the values that minimize the sum of squared residuals, as shown in Equation¬†1.\n\\[\nSSR = \\sum\\limits_{i=1}^{n}[y_i - \\hat{y}_i]^2 = [y_i - (\\hat{\\beta}_0 + \\hat{\\beta}_1 x_i)]^2 = [y_i - \\hat{\\beta}_0 - \\hat{\\beta}_1 x_i]^2\n\\tag{1}\\]\nRecall that we can find the values of \\(\\hat{\\beta}_1\\) and \\(\\hat{\\beta}_0\\) that minimize /eq-ssr by taking the partial derivatives of Equation¬†1 and setting them to 0. Thus, the values of \\(\\hat{\\beta}_1\\) and \\(\\hat{\\beta}_0\\) that minimize the respective partial derivative also minimize the sum of squared residuals. The partial derivatives are shown in Equation¬†2.\n\\[\n\\begin{aligned}\n\\frac{\\partial \\text{SSR}}{\\partial \\hat{\\beta}_1} &= -2 \\sum\\limits_{i=1}^{n}x_i(y_i - \\hat{\\beta}_0 - \\hat{\\beta}_1 x_i)  \\\\\n\\frac{\\partial \\text{SSR}}{\\partial \\hat{\\beta}_0} &= -2 \\sum\\limits_{i=1}^{n}(y_i - \\hat{\\beta}_0 - \\hat{\\beta}_1 x_i)\n\\end{aligned}\n\\tag{2}\\]\nThe derivation of deriving \\(\\hat{\\beta}_0\\) is shown in Equation¬†3.\n\\[\n\\begin{aligned}\\frac{\\partial \\text{SSR}}{\\partial \\hat{\\beta}_0} &= -2 \\sum\\limits_{i=1}^{n}(y_i - \\hat{\\beta}_0 - \\hat{\\beta}_1 x_i) = 0 \\\\&\\Rightarrow -\\sum\\limits_{i=1}^{n}(y_i + \\hat{\\beta}_0 + \\hat{\\beta}_1 x_i) = 0 \\\\&\\Rightarrow - \\sum\\limits_{i=1}^{n}y_i + n\\hat{\\beta}_0 + \\hat{\\beta}_1\\sum\\limits_{i=1}^{n}x_i = 0 \\\\&\\Rightarrow n\\hat{\\beta}_0  = \\sum\\limits_{i=1}^{n}y_i - \\hat{\\beta}_1\\sum\\limits_{i=1}^{n}x_i \\\\&\\Rightarrow \\hat{\\beta}_0  = \\frac{1}{n}\\Big(\\sum\\limits_{i=1}^{n}y_i - \\hat{\\beta}_1\\sum\\limits_{i=1}^{n}x_i\\Big)\\\\&\\Rightarrow \\hat{\\beta}_0  = \\bar{y} - \\hat{\\beta}_1 \\bar{x} \\\\\\end{aligned}\n\\tag{3}\\]\nThe derivation of \\(\\hat{\\beta}_1\\) using the \\(\\hat{\\beta}_0\\) we just derived is shown in Equation¬†4.\n\\[\n\\begin{aligned}&\\frac{\\partial \\text{SSR}}{\\partial \\hat{\\beta}_1} = -2 \\sum\\limits_{i=1}^{n}x_i(y_i - \\hat{\\beta}_0 - \\hat{\\beta}_1 x_i) = 0  \\\\&\\Rightarrow -\\sum\\limits_{i=1}^{n}x_iy_i + \\hat{\\beta}_0\\sum\\limits_{i=1}^{n}x_i + \\hat{\\beta}_1\\sum\\limits_{i=1}^{n}x_i^2 = 0 \\\\\\text{(Fill in }\\hat{\\beta}_0\\text{)}&\\Rightarrow -\\sum\\limits_{i=1}^{n}x_iy_i + (\\bar{y} - \\hat{\\beta}_1\\bar{x})\\sum\\limits_{i=1}^{n}x_i + \\hat{\\beta}_1\\sum\\limits_{i=1}^{n}x_i^2 = 0 \\\\&\\Rightarrow  (\\bar{y} - \\hat{\\beta}_1\\bar{x})\\sum\\limits_{i=1}^{n}x_i + \\hat{\\beta}_1\\sum\\limits_{i=1}^{n}x_i^2 = \\sum\\limits_{i=1}^{n}x_iy_i \\\\&\\Rightarrow \\bar{y}\\sum\\limits_{i=1}^{n}x_i - \\hat{\\beta}_1\\bar{x}\\sum\\limits_{i=1}^{n}x_i + \\hat{\\beta}_1\\sum\\limits_{i=1}^{n}x_i^2 = \\sum\\limits_{i=1}^{n}x_iy_i \\\\&\\Rightarrow n\\bar{y}\\bar{x} - \\hat{\\beta}_1n\\bar{x}^2 + \\hat{\\beta}_1\\sum\\limits_{i=1}^{n}x_i^2 = \\sum\\limits_{i=1}^{n}x_iy_i \\\\&\\Rightarrow \\hat{\\beta}_1\\sum\\limits_{i=1}^{n}x_i^2 - \\hat{\\beta}_1n\\bar{x}^2  = \\sum\\limits_{i=1}^{n}x_iy_i - n\\bar{y}\\bar{x} \\\\&\\Rightarrow \\hat{\\beta}_1\\Big(\\sum\\limits_{i=1}^{n}x_i^2 -n\\bar{x}^2\\Big)  = \\sum\\limits_{i=1}^{n}x_iy_i - n\\bar{y}\\bar{x} \\\\ &\\hat{\\beta}_1 = \\frac{\\sum\\limits_{i=1}^{n}x_iy_i - n\\bar{y}\\bar{x}}{\\sum\\limits_{i=1}^{n}x_i^2 -n\\bar{x}^2}\\end{aligned}\n\\tag{4}\\]\nTo write \\(\\hat{\\beta}_1\\) in a form that‚Äôs more recognizable, we will use the following:\n\\[\n\\sum x_iy_i - n\\bar{y}\\bar{x} = \\sum(x - \\bar{x})(y - \\bar{y}) = (n-1)\\text{Cov}(x,y)\n\\tag{5}\\]\n\\[\n\\sum x_i^2 - n\\bar{x}^2 - \\sum(x - \\bar{x})^2 = (n-1)s_x^2\n\\tag{6}\\]\nwhere \\(\\text{Cov}(x,y)\\) is the covariance of \\(x\\) and \\(y\\), and \\(s_x^2\\) is the sample variance of \\(x\\) (\\(s_x\\) is the sample standard deviation).\nThus, applying Equation¬†5 and Equation¬†6, we have\n\\[\n\\begin{aligned}\\hat{\\beta}_1 &= \\frac{\\sum\\limits_{i=1}^{n}x_iy_i - n\\bar{y}\\bar{x}}{\\sum\\limits_{i=1}^{n}x_i^2 -n\\bar{x}^2} \\\\&= \\frac{\\sum\\limits_{i=1}^{n}(x-\\bar{x})(y-\\bar{y})}{\\sum\\limits_{i=1}^{n}(x-\\bar{x})^2}\\\\&= \\frac{(n-1)\\text{Cov}(x,y)}{(n-1)s_x^2}\\\\&= \\frac{\\text{Cov}(x,y)}{s_x^2}\\end{aligned}\n\\tag{7}\\]\nThe correlation between \\(x\\) and \\(y\\) is \\(r = \\frac{\\text{Cov}(x,y)}{s_x s_y}\\). Thus, \\(\\text{Cov}(x,y) = r s_xs_y\\). Plugging this into Equation¬†7, we have\n\\[\n\\hat{\\beta}_1 = \\frac{\\text{Cov}(x,y)}{s_x^2} = r\\frac{s_ys_x}{s_x^2} = r\\frac{s_y}{s_x}\n\\tag{8}\\]"
  },
  {
    "objectID": "supplemental/log-transformations.html",
    "href": "supplemental/log-transformations.html",
    "title": "Log Transformations in Linear Regression",
    "section": "",
    "text": "Note\n\n\n\nThe following supplemental notes were created by Dr.¬†Maria Tackett for STA 210. They are provided for students who want to dive deeper into the mathematics behind regression and reflect some of the material covered in STA 211: Mathematics of Regression. Additional supplemental notes will be added throughout the semester.\nThis document provides details about the model interpretation when the predictor and/or response variables are log-transformed. For simplicity, we will discuss transformations for the simple linear regression model as shown in Equation¬†1.\n\\[\n\\label{orig}\ny = \\beta_0 + \\beta_1 x\n\\tag{1}\\]\nAll results and interpretations can be easily extended to transformations in multiple regression models.\nNote: log refers to the natural logarithm."
  },
  {
    "objectID": "supplemental/log-transformations.html#log-transformation-on-the-response-variable",
    "href": "supplemental/log-transformations.html#log-transformation-on-the-response-variable",
    "title": "Log Transformations in Linear Regression",
    "section": "Log-transformation on the response variable",
    "text": "Log-transformation on the response variable\nSuppose we fit a linear regression model with \\(\\log(y)\\), the log-transformed \\(y\\), as the response variable. Under this model, we assume a linear relationship exists between \\(x\\) and \\(\\log(y)\\), such that \\(\\log(y) \\sim N(\\beta_0 + \\beta_1 x, \\sigma^2)\\) for some \\(\\beta_0\\), \\(\\beta_1\\) and \\(\\sigma^2\\). In other words, we can model the relationship between \\(x\\) and \\(\\log(y)\\) using the model in Equation¬†2.\n\\[\n\\log(y) = \\beta_0 + \\beta_1 x\n\\tag{2}\\]\nIf we interpret the model in terms of \\(\\log(y)\\), then we can use the usual interpretations for slope and intercept. When reporting results, however, it is best to give all interpretations in terms of the original response variable \\(y\\), since interpretations using log-transformed variables are often more difficult to truly understand.\nIn order to get back on the original scale, we need to use the exponential function (also known as the anti-log), \\(\\exp\\{x\\} = e^x\\). Therefore, we use the model in Equation¬†2 for interpretations and predictions, we will use Equation¬†3 to state our conclusions in terms of \\(y\\).\n\\[\n\\begin{aligned}\n&\\exp\\{\\log(y)\\} = \\exp\\{\\beta_0 + \\beta_1 x\\} \\\\[10pt]\n\\Rightarrow &y = \\exp\\{\\beta_0 + \\beta_1 x\\} \\\\[10pt]\n\\Rightarrow &y = \\exp\\{\\beta_0\\}\\exp\\{\\beta_1 x\\}\n\\end{aligned}\n\\tag{3}\\]\nIn order to interpret the slope and intercept, we need to first understand the relationship between the mean, median and log transformations.\n\nMean, Median, and Log Transformations\nSuppose we have a dataset y that contains the following observations:\n\n\n[1] 3 5 6 7 8\n\n\nIf we log-transform the values of y then calculate the mean and median, we have\n\n\n\n\n\nmean_log_y\nmedian_log_y\n\n\n\n\n1.70503\n1.79176\n\n\n\n\n\nIf we calculate the mean and median of y, then log-transform the mean and median, we have\n\n\n\n\n\nlog_mean\nlog_median\n\n\n\n\n1.75786\n1.79176\n\n\n\n\n\nThis is a simple illustration to show\n\n\\(\\text{Mean}[{\\log(y)}] \\neq \\log[\\text{Mean}(y)]\\) - the mean and log are not commutable\n\\(\\text{Median}[\\log(y)] = \\log[\\text{Median}(y)]\\) - the median and log are commutable\n\n\n\nInterpretaton of model coefficients\nUsing Equation¬†2, the mean \\(\\log(y)\\) for any given value of \\(x\\) is \\(\\beta_0 + \\beta_1 x\\); however, this does not indicate that the mean of \\(y = \\exp\\{\\beta_0 + \\beta_1 x\\}\\) (see previous section). From the assumptions of linear regression, we assume that for any given value of \\(x\\), the distribution of \\(\\log(y)\\) is Normal, and therefore symmetric. Thus the median of \\(\\log(y)\\) is equal to the mean of \\(\\log(y)\\), i.e \\(\\text{Median}(\\log(y)) = \\beta_0 + \\beta_1 x\\).\nSince the log and the median are commutable, \\(\\text{Median}(\\log(y)) = \\beta_0 + \\beta_1 x \\Rightarrow \\text{Median}(y) = \\exp\\{\\beta_0 + \\beta_1 x\\}\\). Thus, when we log-transform the response variable, the interpretation of the intercept and slope are in terms of the effect on the median of \\(y\\).\nIntercept: The intercept is expected median of \\(y\\) when the predictor variable equals 0. Therefore, when \\(x=0\\),\n\\[\n\\begin{aligned}\n&\\log(y) = \\beta_0 + \\beta_1 \\times 0 = \\beta_0 \\\\[10pt]\n\\Rightarrow &y = \\exp\\{\\beta_0\\}\n\\end{aligned}\n\\]\nInterpretation: When \\(x=0\\), the median of \\(y\\) is expected to be \\(\\exp\\{\\beta_0\\}\\).\nSlope: The slope is the expected change in the median of \\(y\\) when \\(x\\) increases by 1 unit. The change in the median of \\(y\\) is\n\\[\n\\exp\\{[\\beta_0 + \\beta_1 (x+1)] - [\\beta_0 + \\beta_1 x]\\} = \\frac{\\exp\\{\\beta_0 + \\beta_1 (x+1)\\}}{\\exp\\{\\beta_0 + \\beta_1 x\\}} = \\frac{\\exp\\{\\beta_0\\}\\exp\\{\\beta_1 x\\}\\exp\\{\\beta_1\\}}{\\exp\\{\\beta_0\\}\\exp\\{\\beta_1 x\\}} = \\exp\\{\\beta_1\\}\n\\]\nThus, the median of \\(y\\) for \\(x+1\\) is \\(\\exp\\{\\beta_1\\}\\) times the median of \\(y\\) for \\(x\\).\nInterpretation: When \\(x\\) increases by one unit, the median of \\(y\\) is expected to multiply by a factor of \\(\\exp\\{\\beta_1\\}\\)."
  },
  {
    "objectID": "supplemental/log-transformations.html#log-transformation-on-the-predictor-variable",
    "href": "supplemental/log-transformations.html#log-transformation-on-the-predictor-variable",
    "title": "Log Transformations in Linear Regression",
    "section": "Log-transformation on the predictor variable",
    "text": "Log-transformation on the predictor variable\nSuppose we fit a linear regression model with \\(\\log(x)\\), the log-transformed \\(x\\), as the predictor variable. Under this model, we assume a linear relationship exists between \\(\\log(x)\\) and \\(y\\), such that \\(y \\sim N(\\beta_0 + \\beta_1 \\log(x), \\sigma^2)\\) for some \\(\\beta_0\\), \\(\\beta_1\\) and \\(\\sigma^2\\). In other words, we can model the relationship between \\(\\log(x)\\) and \\(y\\) using the model in #eq-log-x.\n\\[\ny = \\beta_0 + \\beta_1 \\log(x)\n\\tag{4}\\]\nIntercept: The intercept is the mean of \\(y\\) when \\(\\log(x) = 0\\), i.e.¬†\\(x = 1\\).\nInterpretation: When \\(x = 1\\) \\((\\log(x) = 0)\\), the mean of \\(y\\) is expected to be \\(\\beta_0\\).\nSlope: The slope is interpreted in terms of the change in the mean of \\(y\\) when \\(x\\) is multiplied by a factor of \\(C\\), since \\(\\log(Cx) = \\log(x) + \\log(C)\\). Thus, when \\(x\\) is multiplied by a factor of \\(C\\), the change in the mean of \\(y\\) is\n\\[\n\\begin{aligned}\n[\\beta_0 + \\beta_1 \\log(Cx)] - [\\beta_0 + \\beta_1 \\log(x)] &= \\beta_1 [\\log(Cx) - \\log(x)] \\\\[10pt]\n& = \\beta_1[\\log(C) + \\log(x) - \\log(x)] \\\\[10pt]\n& = \\beta_1 \\log(C)\n\\end{aligned}\n\\]\nThus the mean of \\(y\\) changes by \\(\\beta_1 \\log(C)\\) units.\nInterpretation: When \\(x\\) is multiplied by a factor of \\(C\\), the mean of \\(y\\) is expected to change by \\(\\beta_1 \\log(C)\\) units. For example, if \\(x\\) is doubled, then the mean of \\(y\\) is expected to change by \\(\\beta_1 \\log(2)\\) units."
  },
  {
    "objectID": "supplemental/log-transformations.html#log-transformation-on-the-the-response-and-predictor-variable",
    "href": "supplemental/log-transformations.html#log-transformation-on-the-the-response-and-predictor-variable",
    "title": "Log Transformations in Linear Regression",
    "section": "Log-transformation on the the response and predictor variable",
    "text": "Log-transformation on the the response and predictor variable\nSuppose we fit a linear regression model with \\(\\log(x)\\), the log-transformed \\(x\\), as the predictor variable and \\(\\log(y)\\), the log-transformed \\(y\\), as the response variable. Under this model, we assume a linear relationship exists between \\(\\log(x)\\) and \\(\\log(y)\\), such that \\(\\log(y) \\sim N(\\beta_0 + \\beta_1 \\log(x), \\sigma^2)\\) for some \\(\\beta_0\\), \\(\\beta_1\\) and \\(\\sigma^2\\). In other words, we can model the relationship between \\(\\log(x)\\) and \\(\\log(y)\\) using the model in Equation¬†5.\n\\[\n\\log(y) = \\beta_0 + \\beta_1 \\log(x)\n\\tag{5}\\]\nBecause the response variable is log-transformed, the interpretations on the original scale will be in terms of the median of \\(y\\) (see the section on the log-transformed response variable for more detail).\nIntercept: The intercept is the mean of \\(y\\) when \\(\\log(x) = 0\\), i.e.¬†\\(x = 1\\). Therefore, when \\(\\log(x) = 0\\),\n\\[\n\\begin{aligned}\n&\\log(y) = \\beta_0 + \\beta_1 \\times 0 = \\beta_0 \\\\[10pt]\n\\Rightarrow &y = \\exp\\{\\beta_0\\}\n\\end{aligned}\n\\]\nInterpretation: When \\(x = 1\\) \\((\\log(x) = 0)\\), the median of \\(y\\) is expected to be \\(\\exp\\{\\beta_0\\}\\).\nSlope: The slope is interpreted in terms of the change in the median \\(y\\) when \\(x\\) is multiplied by a factor of \\(C\\), since \\(\\log(Cx) = \\log(x) + \\log(C)\\). Thus, when \\(x\\) is multiplied by a factor of \\(C\\), the change in the median of \\(y\\) is\n\\[\n\\begin{aligned}\n\\exp\\{[\\beta_0 + \\beta_1 \\log(Cx)] - [\\beta_0 + \\beta_1 \\log(x)]\\} &=\n\\exp\\{\\beta_1 [\\log(Cx) - \\log(x)]\\} \\\\[10pt]\n& = \\exp\\{\\beta_1[\\log(C) + \\log(x) - \\log(x)]\\} \\\\[10pt]\n& = \\exp\\{\\beta_1 \\log(C)\\} = C^{\\beta_1}\n\\end{aligned}\n\\]\nThus, the median of \\(y\\) for \\(Cx\\) is \\(C^{\\beta_1}\\) times the median of \\(y\\) for \\(x\\).\nInterpretation: When \\(x\\) is multiplied by a factor of \\(C\\), the median of \\(y\\) is expected to multiple by a factor of \\(C^{\\beta_1}\\). For example, if \\(x\\) is doubled, then the median of \\(y\\) is expected to multiply by \\(2^{\\beta_1}\\)."
  },
  {
    "objectID": "supplemental/model-selection-criteria.html",
    "href": "supplemental/model-selection-criteria.html",
    "title": "Model Selection Criteria: AIC & BIC",
    "section": "",
    "text": "Note\n\n\n\nThe following supplemental notes were created by Dr.¬†Maria Tackett for STA 210. They are provided for students who want to dive deeper into the mathematics behind regression and reflect some of the material covered in STA 211: Mathematics of Regression. Additional supplemental notes will be added throughout the semester.\nThis document discusses some of the mathematical details of Akaike‚Äôs Information Criterion (AIC) and Schwarz‚Äôs Bayesian Information Criterion (BIC). We assume the reader knowledge of the matrix form for multiple linear regression.Please see Matrix Notation for Multiple Linear Regression for a review."
  },
  {
    "objectID": "supplemental/model-selection-criteria.html#maximum-likelihood-estimation-of-boldsymbolbeta-and-sigma",
    "href": "supplemental/model-selection-criteria.html#maximum-likelihood-estimation-of-boldsymbolbeta-and-sigma",
    "title": "Model Selection Criteria: AIC & BIC",
    "section": "Maximum Likelihood Estimation of \\(\\boldsymbol{\\beta}\\) and \\(\\sigma\\)",
    "text": "Maximum Likelihood Estimation of \\(\\boldsymbol{\\beta}\\) and \\(\\sigma\\)\nTo understand the formulas for AIC and BIC, we will first briefly explain the likelihood function and maximum likelihood estimates for regression.\nLet \\(\\mathbf{Y}\\) be \\(n \\times 1\\) matrix of responses, \\(\\mathbf{X}\\), the \\(n \\times (p+1)\\) matrix of predictors, and \\(\\boldsymbol{\\beta}\\), \\((p+1) \\times 1\\) matrix of coefficients. If the multiple linear regression model is correct then,\n\\[\n\\mathbf{Y} \\sim N(\\mathbf{X}\\boldsymbol{\\beta}, \\sigma^2)\n\\tag{1}\\]\nWhen we do linear regression, our goal is to estimate the unknown parameters \\(\\boldsymbol{\\beta}\\) and \\(\\sigma^2\\) from Equation¬†1. In Matrix Notation for Multiple Linear Regression, we showed a way to estimate these parameters using matrix alegbra. Another approach for estimating \\(\\boldsymbol{\\beta}\\) and \\(\\sigma^2\\) is using maximum likelihood estimation.\nA likelihood function is used to summarise the evidence from the data in support of each possible value of a model parameter. Using Equation¬†1, we will write the likelihood function for linear regression as\n\\[\nL(\\mathbf{X}, \\mathbf{Y}|\\boldsymbol{\\beta}, \\sigma^2) = \\prod\\limits_{i=1}^n (2\\pi \\sigma^2)^{-\\frac{1}{2}} \\exp\\bigg\\{-\\frac{1}{2\\sigma^2}\\sum\\limits_{i=1}^n(Y_i - \\mathbf{X}_i \\boldsymbol{\\beta})^T(Y_i - \\mathbf{X}_i \\boldsymbol{\\beta})\\bigg\\}\n\\tag{2}\\]\nwhere \\(Y_i\\) is the \\(i^{th}\\) response and \\(\\mathbf{X}_i\\) is the vector of predictors for the \\(i^{th}\\) observation. One approach estimating \\(\\boldsymbol{\\beta}\\) and \\(\\sigma^2\\) is to find the values of those parameters that maximize the likelihood in Equation¬†2, i.e.¬†maximum likelhood estimation. To make the calculations more manageable, instead of maximizing the likelihood function, we will instead maximize its logarithm, i.e.¬†the log-likelihood function. The values of the parameters that maximize the log-likelihood function are those that maximize the likelihood function. The log-likelihood function we will maximize is\n\\[\n\\begin{aligned}\n\\log L(\\mathbf{X}, \\mathbf{Y}|\\boldsymbol{\\beta}, \\sigma^2) &= \\sum\\limits_{i=1}^n -\\frac{1}{2}\\log(2\\pi\\sigma^2) -\\frac{1}{2\\sigma^2}\\sum\\limits_{i=1}^n(Y_i - \\mathbf{X}_i \\boldsymbol{\\beta})^T(Y_i - \\mathbf{X}_i \\boldsymbol{\\beta}) \\\\\n&= -\\frac{n}{2}\\log(2\\pi\\sigma^2) - \\frac{1}{2\\sigma^2}(\\mathbf{Y} - \\mathbf{X} \\boldsymbol{\\beta})^T(\\mathbf{Y} - \\mathbf{X} \\boldsymbol{\\beta})\\\\\n\\end{aligned}\n\\tag{3}\\]\n\nThe maximum likelihood estimate of \\(\\boldsymbol{\\beta}\\) and \\(\\sigma^2\\) are \\[\n\\hat{\\boldsymbol{\\beta}} = (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{Y} \\hspace{10mm} \\hat{\\sigma}^2 = \\frac{1}{n}(\\mathbf{Y} - \\mathbf{X} \\boldsymbol{\\beta})^T(\\mathbf{Y} - \\mathbf{X} \\boldsymbol{\\beta}) = \\frac{1}{n}RSS\n\\tag{4}\\]\nwhere \\(RSS\\) is the residual sum of squares. Note that the maximum likelihood estimate is not exactly equal to the estimate of \\(\\sigma^2\\) we typically use \\(\\frac{RSS}{n-p-1}\\). This is because the maximum likelihood estimate of \\(\\sigma^2\\) in Equation¬†4 is a biased estimator of \\(\\sigma^2\\). When \\(n\\) is much larger than the number of predictors \\(p\\), then the differences in these two estimates are trivial."
  },
  {
    "objectID": "supplemental/model-selection-criteria.html#aic",
    "href": "supplemental/model-selection-criteria.html#aic",
    "title": "Model Selection Criteria: AIC & BIC",
    "section": "AIC",
    "text": "AIC\nAkaike‚Äôs Information Criterion (AIC) is\n\\[\nAIC = -2 \\log L + 2(p+1)\n\\tag{5}\\]\nwhere \\(\\log L\\) is the log-likelihood. This is the general form of AIC that can be applied to a variety of models, but for now, let‚Äôs focus on AIC for mutliple linear regression.\n\\[\n\\begin{aligned}\nAIC &= -2 \\log L + 2(p+1) \\\\\n&= -2\\bigg[-\\frac{n}{2}\\log(2\\pi\\sigma^2) - \\frac{1}{2\\sigma^2}(\\mathbf{Y} - \\mathbf{X} \\boldsymbol{\\beta})^T(\\mathbf{Y} - \\mathbf{X} \\boldsymbol{\\beta})\\bigg] + 2(p+1) \\\\\n&= n\\log\\big(2\\pi\\frac{RSS}{n}\\big) + \\frac{1}{RSS/n}RSS \\\\\n&= n\\log(2\\pi) + n\\log(RSS) - n\\log(n) + 2(p+1)\n\\end{aligned}\n\\tag{6}\\]"
  },
  {
    "objectID": "supplemental/model-selection-criteria.html#bic",
    "href": "supplemental/model-selection-criteria.html#bic",
    "title": "Model Selection Criteria: AIC & BIC",
    "section": "BIC",
    "text": "BIC\n[To be added.]"
  },
  {
    "objectID": "computing-pipelines.html",
    "href": "computing-pipelines.html",
    "title": "Pipelines",
    "section": "",
    "text": "library(palmerpenguins)\nlibrary(tidyverse)\n\n‚îÄ‚îÄ Attaching packages ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ tidyverse 1.3.1 ‚îÄ‚îÄ\n\n\n‚úì ggplot2 3.3.5     ‚úì purrr   0.3.4\n‚úì tibble  3.1.6     ‚úì dplyr   1.0.7\n‚úì tidyr   1.1.4     ‚úì stringr 1.4.0\n‚úì readr   2.1.1     ‚úì forcats 0.5.1\n\n\n‚îÄ‚îÄ Conflicts ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ tidyverse_conflicts() ‚îÄ‚îÄ\nx dplyr::filter() masks stats::filter()\nx dplyr::lag()    masks stats::lag()\n\nlibrary(tidymodels)\n\nRegistered S3 method overwritten by 'tune':\n  method                   from   \n  required_pkgs.model_spec parsnip\n\n\n‚îÄ‚îÄ Attaching packages ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ tidymodels 0.1.4 ‚îÄ‚îÄ\n\n\n‚úì broom        0.7.10         ‚úì rsample      0.1.1     \n‚úì dials        0.0.10         ‚úì tune         0.1.6     \n‚úì infer        1.0.1.9000     ‚úì workflows    0.2.4     \n‚úì modeldata    0.1.1          ‚úì workflowsets 0.1.0     \n‚úì parsnip      0.1.7          ‚úì yardstick    0.0.9     \n‚úì recipes      0.2.0          \n\n\n‚îÄ‚îÄ Conflicts ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ tidymodels_conflicts() ‚îÄ‚îÄ\nx scales::discard() masks purrr::discard()\nx dplyr::filter()   masks stats::filter()\nx recipes::fixed()  masks stringr::fixed()\nx dplyr::lag()      masks stats::lag()\nx yardstick::spec() masks readr::spec()\nx recipes::step()   masks stats::step()\n‚Ä¢ Search for functions across packages at https://www.tidymodels.org/find/\n\nlibrary(knitr)"
  },
  {
    "objectID": "computing-pipelines.html#simple-linear-regression",
    "href": "computing-pipelines.html#simple-linear-regression",
    "title": "Pipelines",
    "section": "Simple linear regression",
    "text": "Simple linear regression\n\nModel fitting\nFit model:\n\npenguins_fit <- linear_reg() %>%\n  set_engine(\"lm\") %>%\n  fit(body_mass_g ~ flipper_length_mm, data = penguins)\n\nTidy model output:\n\ntidy(penguins_fit)\n\n# A tibble: 2 √ó 5\n  term              estimate std.error statistic   p.value\n  <chr>                <dbl>     <dbl>     <dbl>     <dbl>\n1 (Intercept)        -5781.     306.       -18.9 5.59e- 55\n2 flipper_length_mm     49.7      1.52      32.7 4.37e-107\n\n\nFormat model output as table:\n\ntidy(penguins_fit) %>%\n  kable(digits = 3)\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n-5780.831\n305.815\n-18.903\n0\n\n\nflipper_length_mm\n49.686\n1.518\n32.722\n0\n\n\n\n\n\nAugment data with model:\n\naugment(penguins_fit$fit)\n\n# A tibble: 342 √ó 9\n   .rownames body_mass_g flipper_length_‚Ä¶ .fitted  .resid    .hat .sigma .cooksd\n   <chr>           <int>            <int>   <dbl>   <dbl>   <dbl>  <dbl>   <dbl>\n 1 1                3750              181   3212.  538.   0.00881   394. 8.34e-3\n 2 2                3800              186   3461.  339.   0.00622   394. 2.33e-3\n 3 3                3250              195   3908. -658.   0.00344   393. 4.83e-3\n 4 5                3450              193   3808. -358.   0.00385   394. 1.60e-3\n 5 6                3650              190   3659.   -9.43 0.00469   395. 1.35e-6\n 6 7                3625              181   3212.  413.   0.00881   394. 4.91e-3\n 7 8                4675              195   3908.  767.   0.00344   393. 6.56e-3\n 8 9                3475              193   3808. -333.   0.00385   394. 1.39e-3\n 9 10               4250              190   3659.  591.   0.00469   394. 5.31e-3\n10 11               3300              186   3461. -161.   0.00622   395. 5.23e-4\n# ‚Ä¶ with 332 more rows, and 1 more variable: .std.resid <dbl>\n\n\n\n\nStatistical inference"
  },
  {
    "objectID": "course-faq.html",
    "href": "course-faq.html",
    "title": "FAQ",
    "section": "",
    "text": "Go to your Files tab, check the box next to the file you want to download, then click on the blue gear icon on the Files tab to reveal the drop down menu, and select Export‚Ä¶ If you have selected multiple files to export, RStudio will zip them up into a single zip file for you. If you‚Äôve selected just a single file, it will only download that. The downloaded file will go to wherever files you download off the internet goes on your computer (usually your Downloads folder)."
  },
  {
    "objectID": "course-faq.html#how-can-i-submit-my-assignment-to-gradescope",
    "href": "course-faq.html#how-can-i-submit-my-assignment-to-gradescope",
    "title": "FAQ",
    "section": "How can I submit my assignment to Gradescope?",
    "text": "How can I submit my assignment to Gradescope?\nThe instructions for submitting your assignment to Gradescope can be found here. In a nutshell, you‚Äôll upload your PDF and them mark the page(s) where each question can be found. It‚Äôs OK if a question spans multiple pages, just mark them all. It‚Äôs also OK if a page includes multiple questions."
  },
  {
    "objectID": "course-faq.html#can-i-use-a-local-install-of-r-and-rstudio-instead-of-using-the-rstudio-containers",
    "href": "course-faq.html#can-i-use-a-local-install-of-r-and-rstudio-instead-of-using-the-rstudio-containers",
    "title": "FAQ",
    "section": "Can I use a local install of R and RStudio instead of using the RStudio containers?",
    "text": "Can I use a local install of R and RStudio instead of using the RStudio containers?\nThe short answer is, I‚Äôd rather you didn‚Äôt, to save yourself some headache. But, the long answer is, sure! But you will need to install a specific versions of R and RStudio for everything to work as expected. You will also need to install the R packages we‚Äôre using as well as have Git installed on your computer. These are not extremely challenging things to get right, but they are not trivial either, particularly on certain operating systems. Myself and the TAs are always happy to provide help with any computational questions when you‚Äôre working in the containers we have provided for you. If you‚Äôre working on your local setup, we can‚Äôt guarantee being able to resolve your issues, though we‚Äôre happy to try.\nIf you want to take this path, here is what you need to do:\n\nDownload and install R 4.1.2: https://cran.r-project.org/\nDownload and install a daily build of RStudio: https://dailies.rstudio.com/\nInstall Quarto CLI: https://quarto.org/docs/getting-started/installation.html\nInstall Git: https://happygitwithr.com/install-git.html\nInstall any necessary packages with install.packages(\"___\")\n\nAnd I‚Äôd like to reiterate again that successful installation of these software is not a learning goal of this course. So if any of this seems tedious or intimidating in any way, just use the computing environment we have set up for you. More on that here."
  },
  {
    "objectID": "exercise/ex-7-nonpar.html",
    "href": "exercise/ex-7-nonpar.html",
    "title": "Exercise 7: Nonparametric Regression",
    "section": "",
    "text": "Note"
  },
  {
    "objectID": "exercise/ex-7-nonpar.html#nonparametric-regression",
    "href": "exercise/ex-7-nonpar.html#nonparametric-regression",
    "title": "Exercise 7: Nonparametric Regression",
    "section": "Nonparametric Regression",
    "text": "Nonparametric Regression\n\n\nFor the Boston data, use medv as the response and lstat as the predictor, then do\n\nKernel Smoother\nLocal Linear Regression\nLOESS\n\nTry to find the best parameter of each method to get the best fit."
  },
  {
    "objectID": "exercise/ex-5-bootstrapping.html",
    "href": "exercise/ex-5-bootstrapping.html",
    "title": "Exercise 5: Bootstrapping",
    "section": "",
    "text": "Note"
  },
  {
    "objectID": "exercise/ex-5-bootstrapping.html#bootstrapping-standard-deviation",
    "href": "exercise/ex-5-bootstrapping.html#bootstrapping-standard-deviation",
    "title": "Exercise 5: Bootstrapping",
    "section": "Bootstrapping Standard Deviation",
    "text": "Bootstrapping Standard Deviation\n\n\nBased on the Boston data set, provide an estimate for the population mean of medv. Call the estimate \\(\\hat{\\mu}\\).\nProvide an estimate of the standard error of \\(\\hat{\\mu}\\). Interpret this result. Hint: We can compute the standard error of the sample mean by dividing the sample standard deviation by the square root of the number of observations.\nEstimate the standard error of \\(\\hat{\\mu}\\) using the bootstrap."
  },
  {
    "objectID": "exercise/ex-2-slr-sol.html",
    "href": "exercise/ex-2-slr-sol.html",
    "title": "Exercise 2: Simple Linear Regression Solution",
    "section": "",
    "text": "Note\n\n\n\nExercises are for practice purpose only."
  },
  {
    "objectID": "exercise/ex-2-slr-sol.html#simple-linear-regression",
    "href": "exercise/ex-2-slr-sol.html#simple-linear-regression",
    "title": "Exercise 2: Simple Linear Regression Solution",
    "section": "Simple Linear Regression",
    "text": "Simple Linear Regression\n\n\nBasic Understanding\n\nThe scatterplot and least squares summary below show the relationship between weight measured in kilograms and height measured in centimeters of 507 physically active individuals. (Heinz et al., 2003)\n\n\n\n\n\n\n\n\n \n  \n    term \n    estimate \n    std.error \n    statistic \n    p.value \n  \n \n\n  \n    (Intercept) \n    -105.01 \n    7.54 \n    -13.93 \n    <0.0001 \n  \n  \n    hgt \n    1.02 \n    0.04 \n    23.13 \n    <0.0001 \n  \n\n\n\n\n\n\nDescribe the relationship between height and weight.\n\n\n# positive association\n\n\nWrite the equation of the regression line. Interpret the slope and intercept in context.\n\n\n# weight_hat = -105.01 + 1.02 height\n\n\nDo the data provide convincing evidence that the true slope parameter is different than 0? State the null and alternative hypotheses, report the p-value, and state your conclusion.\n\n\n# Yes\n\n\nThe correlation coefficient for height and weight is 0.72. Calculate \\(R^2\\) and interpret it.\n\n\n0.72^2\n\n[1] 0.5184\n\nsummary(lm(wgt ~ hgt, data = bdims))$r.sq\n\n[1] 0.5145208\n\n\n\n\nSimulation\n\nGenerate a simulated data of size \\(n = 100\\) from the regression\n\n\\[y_i = 10 + 5x_i + \\epsilon_i, ~~ \\epsilon_i \\stackrel{iid}{\\sim} N(0, 2)\\]\nusing the code\n\nx <- runif(100)\ny <- 10 + 5 * x + rnorm(100, sd = sqrt(2))\n\nFit a simple linear regression model to the data, then check whether the true slope is captured by the 90% confidence interval for the slope.\n\nlm_out <- lm(y ~ x)\nconfint(lm_out, level = 0.9)\n\n                 5 %      95 %\n(Intercept) 9.347448 10.303577\nx           4.454147  6.015095"
  },
  {
    "objectID": "exercise/ex-8-cat.html",
    "href": "exercise/ex-8-cat.html",
    "title": "Exercise 8: Categorical Variables",
    "section": "",
    "text": "Note\n\n\n\n\n\n\n\n\nSuppose we have a data set with five predictors, \\(X_1 = \\text{GPA}\\), \\(X_2 = \\text{IQ}\\), \\(X_3 = \\text{Level}\\) (1 for College and 0 for High School), \\(X_4 =\\) Interaction between GPA and IQ, and \\(X_5 =\\) Interaction between GPA and Level. The response is starting salary after graduation (in thousands of dollars). Suppose we use least squares to fit the model, and get \\(b_0 = 50\\), \\(b_1 = 20\\), \\(b_2 = 0.07\\), \\(b_3 = 35\\), \\(b_4 = 0.01\\), \\(b_5 = ‚àí10.\\).\n\nWhich answer is correct, and why?\n\nFor a fixed value of IQ and GPA, high school graduates earn more, on average, than college graduates.\nFor a fixed value of IQ and GPA, college graduates earn more, on average, than high school graduates.\nFor a fixed value of IQ and GPA, high school graduates earn more, on average, than college graduates provided that the GPA is high enough.\nFor a fixed value of IQ and GPA, college graduates earn more, on average, than high school graduates provided that the GPA is high enough.\n\nPredict the salary of a college graduate with IQ of 110 and a GPA of 4.0.\nTrue or false: Since the coefficient for the GPA/IQ interaction term is very small, there is very little evidence of an interaction effect. Justify your answer."
  },
  {
    "objectID": "exercise/ex-1-overview-sol.html",
    "href": "exercise/ex-1-overview-sol.html",
    "title": "Exercise 1: Overview Solution",
    "section": "",
    "text": "Note\n\n\n\nExercises are for practice purpose only."
  },
  {
    "objectID": "exercise/ex-1-overview-sol.html#overview-of-regression",
    "href": "exercise/ex-1-overview-sol.html#overview-of-regression",
    "title": "Exercise 1: Overview Solution",
    "section": "Overview of Regression",
    "text": "Overview of Regression\n\n\nDescribe three real-life applications in which regression might be useful. Describe the response, as well as the predictors. Is the goal of each application inference or prediction? Explain your answer.\n\n\n# i. CEO salary. inference. predictors: age, industry experience, industry,\n# years of education. response: salary.\n# \n# ii. car part replacement. inference. response: life of car part. predictors: age\n# of part, mileage used for, current amperage.\n# \n# iii. illness classification, prediction, response: age of death,\n# input: current age, gender, resting heart rate, resting breath rate, mile run\n# time.\n\n\nDescribe the differences between a parametric and a non-parametric statistical learning approach.\n\n\n# A parametric approach reduces the problem of estimating f down to one of\n# estimating a set of parameters because it assumes a form for f.\n# \n# A non-parametric approach does not assume a functional form for f and so\n# requires a very large number of observations to accurately estimate f.\n# \n# The advantages of a parametric approach to regression or classification are the\n# simplifying of modeling f to a few parameters and not as many observations are\n# required compared to a non-parametric approach.\n# \n# The disadvantages of a parametric approach to regression or classification\n# are a potential to inaccurately estimate f if the form of f assumed is wrong or\n# to overfit the observations if more flexible models are used."
  },
  {
    "objectID": "exercise/ex-1-overview-sol.html#checking-data-with-r",
    "href": "exercise/ex-1-overview-sol.html#checking-data-with-r",
    "title": "Exercise 1: Overview Solution",
    "section": "Checking Data with R",
    "text": "Checking Data with R\n\n\nLoad in the Boston data set in ISLR2 package.\n\n\nlibrary(ISLR2)\ndata(\"Boston\")\n\n\nHow many rows are in this data set? How many columns? What do the rows and columns represent?\n\n\ndim(Boston)\n\n[1] 506  13\n\n?Boston\n\n\nMake some pairwise scatterplots of the predictors (columns) in this data set. Describe your findings.\n\n\npairs(Boston, cex = 0.4, pch = 16)\n\n\n\n\n\nAre any of the predictors associated with per capita crime rate? If so, explain the relationship.\n\n\n# check the pairs\n\n\nDo any of the census tracts of Boston appear to have particularly high crime rates? Tax rates? Pupil-teacher ratios? Comment on the range of each predictor.\n\n\npar(mfrow = c(1, 3))\nplot(Boston$crim)\nplot(Boston$tax)\nplot(Boston$ptratio)\n\n\n\nsummary(Boston$crim)\n\n    Min.  1st Qu.   Median     Mean  3rd Qu.     Max. \n 0.00632  0.08204  0.25651  3.61352  3.67708 88.97620 \n\nsummary(Boston$tax)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  187.0   279.0   330.0   408.2   666.0   711.0 \n\nsummary(Boston$ptratio)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  12.60   17.40   19.05   18.46   20.20   22.00 \n\n\n\nHow many of the census tracts in this data set bound the Charles river?\n\n\nsum(Boston$chas)\n\n[1] 35\n\n\n\nWhat is the median pupil-teacher ratio among the towns in this data set?\n\n\nmedian(Boston$ptratio)\n\n[1] 19.05\n\n\n\nWhich census tract of Boston has lowest median value of owneroccupied homes? What are the values of the other predictors for that census tract, and how do those values compare to the overall ranges for those predictors? Comment on your findings.\n\n\nidx <- which(Boston$medv == min(Boston$medv))\nBoston[idx, ]\n\n       crim zn indus chas   nox    rm age    dis rad tax ptratio lstat medv\n399 38.3518  0  18.1    0 0.693 5.453 100 1.4896  24 666    20.2 30.59    5\n406 67.9208  0  18.1    0 0.693 5.683 100 1.4254  24 666    20.2 22.98    5\n\napply(Boston, 2, range)\n\n         crim  zn indus chas   nox    rm   age     dis rad tax ptratio lstat\n[1,]  0.00632   0  0.46    0 0.385 3.561   2.9  1.1296   1 187    12.6  1.73\n[2,] 88.97620 100 27.74    1 0.871 8.780 100.0 12.1265  24 711    22.0 37.97\n     medv\n[1,]    5\n[2,]   50\n\n\n\nIn this data set, how many of the census tracts average more than seven rooms per dwelling? More than eight rooms per dwelling? Comment on the census tracts that average more than eight rooms per dwelling.\n\n\nsum(Boston$rm > 7)\n\n[1] 64\n\nsum(Boston$rm > 8)\n\n[1] 13\n\nsummary(Boston)\n\n      crim                zn             indus            chas        \n Min.   : 0.00632   Min.   :  0.00   Min.   : 0.46   Min.   :0.00000  \n 1st Qu.: 0.08205   1st Qu.:  0.00   1st Qu.: 5.19   1st Qu.:0.00000  \n Median : 0.25651   Median :  0.00   Median : 9.69   Median :0.00000  \n Mean   : 3.61352   Mean   : 11.36   Mean   :11.14   Mean   :0.06917  \n 3rd Qu.: 3.67708   3rd Qu.: 12.50   3rd Qu.:18.10   3rd Qu.:0.00000  \n Max.   :88.97620   Max.   :100.00   Max.   :27.74   Max.   :1.00000  \n      nox               rm             age              dis        \n Min.   :0.3850   Min.   :3.561   Min.   :  2.90   Min.   : 1.130  \n 1st Qu.:0.4490   1st Qu.:5.886   1st Qu.: 45.02   1st Qu.: 2.100  \n Median :0.5380   Median :6.208   Median : 77.50   Median : 3.207  \n Mean   :0.5547   Mean   :6.285   Mean   : 68.57   Mean   : 3.795  \n 3rd Qu.:0.6240   3rd Qu.:6.623   3rd Qu.: 94.08   3rd Qu.: 5.188  \n Max.   :0.8710   Max.   :8.780   Max.   :100.00   Max.   :12.127  \n      rad              tax           ptratio          lstat      \n Min.   : 1.000   Min.   :187.0   Min.   :12.60   Min.   : 1.73  \n 1st Qu.: 4.000   1st Qu.:279.0   1st Qu.:17.40   1st Qu.: 6.95  \n Median : 5.000   Median :330.0   Median :19.05   Median :11.36  \n Mean   : 9.549   Mean   :408.2   Mean   :18.46   Mean   :12.65  \n 3rd Qu.:24.000   3rd Qu.:666.0   3rd Qu.:20.20   3rd Qu.:16.95  \n Max.   :24.000   Max.   :711.0   Max.   :22.00   Max.   :37.97  \n      medv      \n Min.   : 5.00  \n 1st Qu.:17.02  \n Median :21.20  \n Mean   :22.53  \n 3rd Qu.:25.00  \n Max.   :50.00  \n\nsummary(subset(Boston,rm>8))\n\n      crim               zn            indus             chas       \n Min.   :0.02009   Min.   : 0.00   Min.   : 2.680   Min.   :0.0000  \n 1st Qu.:0.33147   1st Qu.: 0.00   1st Qu.: 3.970   1st Qu.:0.0000  \n Median :0.52014   Median : 0.00   Median : 6.200   Median :0.0000  \n Mean   :0.71879   Mean   :13.62   Mean   : 7.078   Mean   :0.1538  \n 3rd Qu.:0.57834   3rd Qu.:20.00   3rd Qu.: 6.200   3rd Qu.:0.0000  \n Max.   :3.47428   Max.   :95.00   Max.   :19.580   Max.   :1.0000  \n      nox               rm             age             dis       \n Min.   :0.4161   Min.   :8.034   Min.   : 8.40   Min.   :1.801  \n 1st Qu.:0.5040   1st Qu.:8.247   1st Qu.:70.40   1st Qu.:2.288  \n Median :0.5070   Median :8.297   Median :78.30   Median :2.894  \n Mean   :0.5392   Mean   :8.349   Mean   :71.54   Mean   :3.430  \n 3rd Qu.:0.6050   3rd Qu.:8.398   3rd Qu.:86.50   3rd Qu.:3.652  \n Max.   :0.7180   Max.   :8.780   Max.   :93.90   Max.   :8.907  \n      rad              tax           ptratio          lstat           medv     \n Min.   : 2.000   Min.   :224.0   Min.   :13.00   Min.   :2.47   Min.   :21.9  \n 1st Qu.: 5.000   1st Qu.:264.0   1st Qu.:14.70   1st Qu.:3.32   1st Qu.:41.7  \n Median : 7.000   Median :307.0   Median :17.40   Median :4.14   Median :48.3  \n Mean   : 7.462   Mean   :325.1   Mean   :16.36   Mean   :4.31   Mean   :44.2  \n 3rd Qu.: 8.000   3rd Qu.:307.0   3rd Qu.:17.40   3rd Qu.:5.12   3rd Qu.:50.0  \n Max.   :24.000   Max.   :666.0   Max.   :20.20   Max.   :7.44   Max.   :50.0"
  },
  {
    "objectID": "exercise/ex-1-overview-sol.html#probability-and-statistics",
    "href": "exercise/ex-1-overview-sol.html#probability-and-statistics",
    "title": "Exercise 1: Overview Solution",
    "section": "Probability and Statistics",
    "text": "Probability and Statistics\n\n\nPlot normal density curves with different choices of mean and standard deviation.\n\n\n\ncurve(dnorm(x, 0, 1), from = -10, to = 10, main = \"normal densities\",\n      ylab = \"density value\", las = 1)\ncurve(dnorm(x, 5, 1), col = 2, add = TRUE)\ncurve(dnorm(x, -5, 1), col = 3, add = TRUE)\ncurve(dnorm(x, 0, 2), col = 4, add = TRUE)\ncurve(dnorm(x, 0, 3), col = 5, add = TRUE)\nlegend(\"topright\", \n       c(\"N(0, 1)\", \"N(5, 1)\", \"N(-5, 1)\", \"N(0, 4)\", \"N(0, 9)\"),\n       col = 1:5, lty = 1, bty = \"n\")\n\n\n\n\n\nChoose a continuous variable in Boston. Use the sample() function to draw a simple random sample of size 20 from this population. Calculate the sample average.\n\n\nsize <- 100\ncrim_sam <- sample(Boston$crim, size = size)\nmean(crim_sam)\n\n[1] 4.743786\n\n\n\nRepeat 2. several times to get a sampling distribution of the sample mean.\n\n\nsample_mean_sample <- replicate(n = 1000, expr = mean(sample(Boston$crim, size = size)))\nhist(sample_mean_sample)\nabline(v = mean(Boston$crim), col = 2)"
  },
  {
    "objectID": "exercise/ex-4-diagnostics.html",
    "href": "exercise/ex-4-diagnostics.html",
    "title": "Exercise 4: Diagnostics",
    "section": "",
    "text": "Note"
  },
  {
    "objectID": "exercise/ex-4-diagnostics.html#diagnostics",
    "href": "exercise/ex-4-diagnostics.html#diagnostics",
    "title": "Exercise 4: Diagnostics",
    "section": "Diagnostics",
    "text": "Diagnostics\n\n\nThis problem involves the Boston data set. We now try to predict per capita crime rate using the median value of owner-occupied homes.\n\nFit a simple linear regression model. Check if there are any potential leverage points and outliers.\nCheck the normality, constant variance and linearity.\nTransform \\(y\\) and/or \\(x\\), and fit a new linear regression. Make sure it satisfies linear regression assumptions."
  },
  {
    "objectID": "exercise/ex-8-cat-sol.html",
    "href": "exercise/ex-8-cat-sol.html",
    "title": "Exercise 8: Categorical Variables Solution",
    "section": "",
    "text": "Note\n\n\n\nExercises are for practice purpose only.\n\n\n\n\nSuppose we have a data set with five predictors, \\(X_1 = \\text{GPA}\\), \\(X_2 = \\text{IQ}\\), \\(X_3 = \\text{Level}\\) (1 for College and 0 for High School), \\(X_4 =\\) Interaction between GPA and IQ, and \\(X_5 =\\) Interaction between GPA and Level. The response is starting salary after graduation (in thousands of dollars). Suppose we use least squares to fit the model, and get \\(b_0 = 50\\), \\(b_1 = 20\\), \\(b_2 = 0.07\\), \\(b_3 = 35\\), \\(b_4 = 0.01\\), \\(b_5 = ‚àí10.\\).\n\nWhich answer is correct, and why?\n\nFor a fixed value of IQ and GPA, high school graduates earn more, on average, than college graduates.\nFor a fixed value of IQ and GPA, college graduates earn more, on average, than high school graduates.\nFor a fixed value of IQ and GPA, high school graduates earn more, on average, than college graduates provided that the GPA is high enough.\nFor a fixed value of IQ and GPA, college graduates earn more, on average, than high school graduates provided that the GPA is high enough.\n\n\n\n## iii\n\n\nPredict the salary of a college graduate with IQ of 110 and a GPA of 4.0.\n\n\ngpa <- 4\niq <- 110\nlevel <- 1\n50 + 20 * gpa + 0.07 * iq + 35 * level + 0.01 * (gpa * iq) - 10 * (gpa * level)\n\n[1] 137.1\n\n\n\nTrue or false: Since the coefficient for the GPA/IQ interaction term is very small, there is very little evidence of an interaction effect. Justify your answer.\n\n\n## FALSE\n## 1. We must examine the p-value of the regression coefficient to\n## determine if the interaction term is statistically significant or not.\n## 2. The range of IQ is always between 100~200, which is wider range than GPA(0~4).\n## Therefore, although the coefficient is small, with GPA fixed, the interaction term can have\n## significant effects."
  },
  {
    "objectID": "exercise/ex-11-logistic.html",
    "href": "exercise/ex-11-logistic.html",
    "title": "Exercise 11: Logistic Regression",
    "section": "",
    "text": "Note"
  },
  {
    "objectID": "exercise/ex-11-logistic.html#logistic-regression",
    "href": "exercise/ex-11-logistic.html#logistic-regression",
    "title": "Exercise 11: Logistic Regression",
    "section": "Logistic Regression",
    "text": "Logistic Regression\n\n\nSuppose we collect data for a group of students in a statistics class with variables \\(X_1 =\\)hours studied, \\(X_2 =\\)undergrad GPA, and \\(Y =\\)receive an A. We fit a logistic regression and produce estimated coefficient, \\(\\hat{\\beta_0} = ‚àí6\\), \\(\\hat{\\beta_1} = 0.05\\), \\(\\hat{\\beta_2} = 1\\).\n\nEstimate the probability that a student who studies for 40 hours and has an undergrad GPA of 3.5 gets an A in the class.\nHow many hours would the student in part (a) need to study to have a 50% chance of getting an A in the class?\n\nSplit the Boston data set into training (80%) and test sets (20%).\nUsing the training set, fit logistic regression to predict whether a given census tract in the test set has a crime rate above or below the median."
  },
  {
    "objectID": "exercise/ex-5-bootstrapping-sol.html",
    "href": "exercise/ex-5-bootstrapping-sol.html",
    "title": "Exercise 5: Bootstrapping Solution",
    "section": "",
    "text": "Note\n\n\n\nExercises are for practice purpose only."
  },
  {
    "objectID": "exercise/ex-5-bootstrapping-sol.html#bootstrapping-standard-deviation",
    "href": "exercise/ex-5-bootstrapping-sol.html#bootstrapping-standard-deviation",
    "title": "Exercise 5: Bootstrapping Solution",
    "section": "Bootstrapping Standard Deviation",
    "text": "Bootstrapping Standard Deviation\n\n\nBased on the Boston data set, provide an estimate for the population mean of medv. Call the estimate \\(\\hat{\\mu}\\).\n\n\nlibrary(ISLR2)\nattach(Boston)\n(mu_hat <- mean(medv))\n\n[1] 22.53281\n\n\n\nProvide an estimate of the standard error of \\(\\hat{\\mu}\\). Interpret this result. Hint: We can compute the standard error of the sample mean by dividing the sample standard deviation by the square root of the number of observations.\n\n\n(se <- sd(medv)/sqrt(length(medv)))\n\n[1] 0.4088611\n\n\n\nEstimate the standard error of \\(\\hat{\\mu}\\) using the bootstrap.\n\n\n\nboot.fn <- function(data, idx) mean(data[idx])\nlibrary(boot)\nbstrap <- boot::boot(data = medv, statistic = boot.fn, R = 1000)\nbstrap\n\n\nORDINARY NONPARAMETRIC BOOTSTRAP\n\n\nCall:\nboot::boot(data = medv, statistic = boot.fn, R = 1000)\n\n\nBootstrap Statistics :\n    original        bias    std. error\nt1* 22.53281 -0.0005235178   0.4113452\n\n## bias = bt_mean - obs_mean\n\n\nlibrary(tidymodels)\nboot_sample <- Boston |> \n    # specify the variable of interest\n    specify(response = medv) |>  \n    # generate 15000 bootstrap samples\n    generate(reps = 1000, type = \"bootstrap\") |>  \n    # calculate the median of each bootstrap sample\n    calculate(stat = \"mean\")\nsd(boot_sample$stat)\n\n[1] 0.3958047"
  },
  {
    "objectID": "exercise/ex-4-diagnostics-sol.html",
    "href": "exercise/ex-4-diagnostics-sol.html",
    "title": "Exercise 4: Diagnostics Solution",
    "section": "",
    "text": "Note\n\n\n\nExercises are for practice purpose only."
  },
  {
    "objectID": "exercise/ex-4-diagnostics-sol.html#diagnostics",
    "href": "exercise/ex-4-diagnostics-sol.html#diagnostics",
    "title": "Exercise 4: Diagnostics Solution",
    "section": "Diagnostics",
    "text": "Diagnostics\n\n\nlibrary(ISLR2)\n\n\nThis problem involves the Boston data set. We now try to predict per capita crime rate using the median value of owner-occupied homes.\n\n\nFit a simple linear regression model. Check if there are any leverage points and outliers.\n\n\nlmout <- lm(crim ~ medv, data = Boston)\nhat_i <- hatvalues(lmout)\np <- 2\nn <- dim(Boston)[1]\nhat_i[hat_i > 2 * p/n]\n\n         98          99         158         162         163         164 \n0.008095218 0.012564598 0.010221559 0.019638086 0.019638086 0.019638086 \n        167         181         187         196         203         204 \n0.019638086 0.008956196 0.019638086 0.019638086 0.011123659 0.017761716 \n        205         225         226         229         233         234 \n0.019638086 0.013583750 0.019638086 0.015649129 0.010576780 0.017519493 \n        254         257         258         262         263         268 \n0.011592267 0.012764683 0.019638086 0.011879050 0.018128562 0.019638086 \n        269         281         283         284         369         370 \n0.012267983 0.014217715 0.014868535 0.019638086 0.019638086 0.019638086 \n        371         372         373         399         400         401 \n0.019638086 0.019638086 0.019638086 0.009172585 0.008144984 0.008688476 \n        406 \n0.009172585 \n\nr_student <- rstudent(lmout)\nr_student[r_student > 2]\n\n      379       381       387       399       404       405       406       411 \n 2.104670 11.482458  2.080661  3.635469  2.032062  4.218608  7.754484  5.836445 \n      414       415       418       419       428 \n 2.896013  4.712929  2.274124  8.812265  3.818927 \n\n\n\nCheck the normality, constant variance and linearity.\n\n\n## normality\ncar::qqPlot(lmout, id = TRUE, col.lines = \"blue\", \n            reps = 1000, ylab = \"Ordered R-Student Residuals\", pch = 16)\n\n\n\n\n[1] 381 419\n\n\n\n## normality\nrstud <- rstudent(lmout)\ncar::densityPlot(rstud)\n\n\n\n\n\n## constant variance\ncar::scatterplot(fitted(lmout), rstudent(lmout), \n            smooth = list(span = 2/3, lwd.smooth = 3, lwd.spread = 4), \n            regLine = FALSE, boxplots = FALSE, main = \"crim as y\",\n            xlab = \"Fitted Values\", ylab = \"R-Student Residuals\")\nabline(a = 0, b = 0, col = 2, lwd = 2)\n\n\n\n\n\n## linearity\ncar::crPlots(lmout, ylab = \"partial residual\", grid = FALSE, main = \"\")\n\n\n\n\n\nTransformming \\(y\\) to satisfy linear regression assumptions.\n\n\nlmout_log <- lm(log(crim) ~ poly(medv, degree = 2, raw = TRUE), data = Boston)\n\n\ncar::densityPlot(rstudent(lmout_log))\n\n\n\n\n\ncar::scatterplot(fitted(lmout_log), rstudent(lmout_log), \n            smooth = list(span = 2/3, lwd.smooth = 3, lwd.spread = 4), \n            regLine = FALSE, boxplots = FALSE, main = \"log(crim) as y, medv^2 as x\",\n            xlab = \"Fitted Values\", ylab = \"R-Student Residuals\")\nabline(a = 0, b = 0, col = 2, lwd = 2)\n\n\n\n\n\ncar::crPlots(lmout_log, ylab = \"partial residual\", grid = FALSE, main = \"\")"
  },
  {
    "objectID": "exercise/ex-9-collinearity.html",
    "href": "exercise/ex-9-collinearity.html",
    "title": "Exercise 9: Collinearity",
    "section": "",
    "text": "Note"
  },
  {
    "objectID": "exercise/ex-9-collinearity.html#collinearity",
    "href": "exercise/ex-9-collinearity.html#collinearity",
    "title": "Exercise 9: Collinearity",
    "section": "Collinearity",
    "text": "Collinearity\n\n\nPerform the code\n\n\nset.seed(1)\nx1 <- runif(100)\nx2 <- 0.5 * x1 + rnorm (100) / 10\ny <- 2 + 2 * x1 + 0.3 * x2 + rnorm (100)\n\nWrite out the form of the linear model. What are the regression coefficients?\n\nCreate a scatterplot displaying the relationship between x1 and x2.\nFit a least squares regression to the data using x1 and x2. How the LSEs relate to the true \\(\\beta_0\\), \\(\\beta_1\\) and \\(\\beta_2\\)? Can you reject \\(H0 : \\beta_1 = 0\\)? How about \\(H0 : \\beta_2 = 0\\)?\nFit a least squares regression using only x1. Comment on your results. Can you reject \\(H0 : \\beta_1 = 0\\)?\nFit a least squares regression using only x2. Comment on your results. Can you reject \\(H0 : \\beta_2 = 0\\)?\nDo the results obtained in (3)‚Äì(5) contradict each other? Explain your answer."
  },
  {
    "objectID": "exercise/ex-3-mlr-sol.html",
    "href": "exercise/ex-3-mlr-sol.html",
    "title": "Exercise 3: Multiple Linear Regression Solution",
    "section": "",
    "text": "Note\n\n\n\nExercises are for practice purpose only."
  },
  {
    "objectID": "exercise/ex-3-mlr-sol.html#multiple-linear-regression",
    "href": "exercise/ex-3-mlr-sol.html#multiple-linear-regression",
    "title": "Exercise 3: Multiple Linear Regression Solution",
    "section": "Multiple Linear Regression",
    "text": "Multiple Linear Regression\n\n\nThis problem involves the Boston data set. We now try to predict per capita crime rate using the other variables in this data set. In other words, per capita crime rate is the response, and the other variables are the predictors.\n\n\nlibrary(ISLR2)\n\n\nFor each predictor, fit a simple linear regression model to predict the response. Describe your results. In which of the models is there a statistically significant association between the predictor and the response? Create some plots to back up your assertions.\n\n\npval <- coef_slr <- rep(0, 12)\nnames(pval) <- names(coef_slr) <- names(Boston[-1])\nfor (i in 1:12) {\n  slr <- lm(Boston$crim ~ Boston[[i+1]])\n  summ <- summary(slr)\n  pp <- \n  coef_slr[i] <- summ$coefficients[2, 1]\n  pval[i] <- summ$coefficients[2, 4]\n}\n\npval[pval < 0.05]\n\n          zn        indus          nox           rm          age          dis \n5.506472e-06 1.450349e-21 3.751739e-23 6.346703e-07 2.854869e-16 8.519949e-19 \n         rad          tax      ptratio        lstat         medv \n2.693844e-56 2.357127e-47 2.942922e-11 2.654277e-27 1.173987e-19 \n\npval[pval > 0.05]\n\n     chas \n0.2094345 \n\n\n\nFit a multiple regression model to predict the response using all of the predictors. Describe your results. For which predictors can we reject the null hypothesis \\(H_0 : \\beta_j = 0\\)?\n\n\nmlr <- lm(crim ~., data = Boston)\n(summ_mlr <- summary(mlr))\n\n\nCall:\nlm(formula = crim ~ ., data = Boston)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-8.534 -2.248 -0.348  1.087 73.923 \n\nCoefficients:\n              Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 13.7783938  7.0818258   1.946 0.052271 .  \nzn           0.0457100  0.0187903   2.433 0.015344 *  \nindus       -0.0583501  0.0836351  -0.698 0.485709    \nchas        -0.8253776  1.1833963  -0.697 0.485841    \nnox         -9.9575865  5.2898242  -1.882 0.060370 .  \nrm           0.6289107  0.6070924   1.036 0.300738    \nage         -0.0008483  0.0179482  -0.047 0.962323    \ndis         -1.0122467  0.2824676  -3.584 0.000373 ***\nrad          0.6124653  0.0875358   6.997 8.59e-12 ***\ntax         -0.0037756  0.0051723  -0.730 0.465757    \nptratio     -0.3040728  0.1863598  -1.632 0.103393    \nlstat        0.1388006  0.0757213   1.833 0.067398 .  \nmedv        -0.2200564  0.0598240  -3.678 0.000261 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6.46 on 493 degrees of freedom\nMultiple R-squared:  0.4493,    Adjusted R-squared:  0.4359 \nF-statistic: 33.52 on 12 and 493 DF,  p-value: < 2.2e-16\n\n\n\nHow do your results from (a) compare to your results from (b)? Create a plot displaying the univariate regression coefficients from (a) on the x-axis, and the multiple regression coefficients from (b) on the y-axis. That is, each predictor is displayed as a single point in the plot. Its coefficient in a simple linear regression model is shown on the x-axis, and its coefficient estimate in the multiple linear regression model is shown on the y-axis.\n\n\ncoef_mlr <- summ_mlr$coef[-1, 1]\nplot(coef_slr, coef_mlr)"
  },
  {
    "objectID": "exercise/ex-10-var-sol.html",
    "href": "exercise/ex-10-var-sol.html",
    "title": "Exercise 10: Variable Selection Solution",
    "section": "",
    "text": "Note\n\n\n\nExercises are for practice purpose only."
  },
  {
    "objectID": "exercise/ex-10-var-sol.html#variable-selection",
    "href": "exercise/ex-10-var-sol.html#variable-selection",
    "title": "Exercise 10: Variable Selection Solution",
    "section": "Variable Selection",
    "text": "Variable Selection\n\n\nConsider Boston data with all variables except chas. Use crim as the response. Perform Best Subset Selection and Forward Selection.\n\n\nlibrary(ISLR2)\nlibrary(olsrr) ## 0.6.0 version\n\n\nAttaching package: 'olsrr'\n\n\nThe following object is masked from 'package:datasets':\n\n    rivers\n\n\n\nfit <- lm(crim ~ . -chas, data = Boston)\nbest_sub <- olsrr::ols_step_best_subset(fit, metric = \"predrsq\")\nord <- order(best_sub$metrics$predrsq, decreasing = TRUE)\nbest_sub$metrics$predrsq[ord]\n\n [1] 0.4301165 0.4299981 0.4299437 0.4288504 0.4287760 0.4254916 0.4252542\n [8] 0.4220163 0.4122490 0.4098917 0.3822294\n\nbest_sub$metrics$predictors[ord]\n\n [1] \"zn indus nox dis rad tax ptratio medv\"             \n [2] \"zn nox dis rad tax ptratio medv\"                   \n [3] \"zn indus nox age dis rad tax ptratio medv\"         \n [4] \"zn nox dis rad ptratio medv\"                       \n [5] \"zn indus nox age dis rad tax ptratio lstat medv\"   \n [6] \"zn indus dis rad medv\"                             \n [7] \"zn indus nox rm age dis rad tax ptratio lstat medv\"\n [8] \"zn dis rad medv\"                                   \n [9] \"zn rad lstat\"                                      \n[10] \"rad lstat\"                                         \n[11] \"rad\"                                               \n\n\n\nolsrr::ols_step_forward_p(fit, penter = 0.3)\n\n\n                             Stepwise Summary                             \n------------------------------------------------------------------------\nStep    Variable        AIC         SBC       SBIC      R2       Adj. R2 \n------------------------------------------------------------------------\n 0      Base Model    3616.730    3625.183      NA    0.00000    0.00000 \n 1      rad           3367.573    3380.252      NA    0.39126    0.39005 \n 2      lstat         3344.403    3361.309      NA    0.42080    0.41849 \n 3      medv          3343.164    3364.297      NA    0.42449    0.42105 \n 4      ptratio       3341.763    3367.122      NA    0.42835    0.42378 \n 5      rm            3341.886    3371.471      NA    0.43046    0.42477 \n 6      nox           3342.355    3376.167      NA    0.43218    0.42536 \n 7      dis           3337.024    3375.063      NA    0.44035    0.43248 \n 8      zn            3333.209    3375.474      NA    0.44675    0.43784 \n 9      indus         3333.796    3380.288      NA    0.44829    0.43828 \n------------------------------------------------------------------------\n\nFinal Model Output \n------------------\n\n                         Model Summary                           \n----------------------------------------------------------------\nR                       0.670       RMSE                  6.383 \nR-Squared               0.448       MSE                  41.560 \nAdj. R-Squared          0.438       Coef. Var           178.405 \nPred R-Squared          0.426       AIC                3333.796 \nMAE                     2.873       SBC                3380.288 \n----------------------------------------------------------------\n RMSE: Root Mean Square Error \n MSE: Mean Square Error \n MAE: Mean Absolute Error \n AIC: Akaike Information Criteria \n SBC: Schwarz Bayesian Criteria \n\n                                ANOVA                                  \n----------------------------------------------------------------------\n                 Sum of                                               \n                Squares         DF    Mean Square      F         Sig. \n----------------------------------------------------------------------\nRegression    16749.527          9       1861.059     44.78    0.0000 \nResidual      20613.695        496         41.560                     \nTotal         37363.222        505                                    \n----------------------------------------------------------------------\n\n                                   Parameter Estimates                                     \n------------------------------------------------------------------------------------------\n      model       Beta    Std. Error    Std. Beta      t        Sig       lower     upper \n------------------------------------------------------------------------------------------\n(Intercept)     13.182         6.970                  1.891    0.059     -0.512    26.877 \n        rad      0.561         0.049        0.568    11.471    0.000      0.465     0.657 \n      lstat      0.140         0.072        0.117     1.959    0.051      0.000     0.281 \n       medv     -0.220         0.058       -0.235    -3.782    0.000     -0.334    -0.106 \n    ptratio     -0.304         0.185       -0.077    -1.643    0.101     -0.668     0.060 \n         rm      0.635         0.595        0.052     1.067    0.287     -0.535     1.805 \n        nox    -10.468         5.077       -0.141    -2.062    0.040    -20.442    -0.494 \n        dis     -1.006         0.271       -0.246    -3.720    0.000     -1.538    -0.475 \n         zn      0.043         0.018        0.117     2.383    0.018      0.008     0.079 \n      indus     -0.088         0.075       -0.070    -1.178    0.239     -0.235     0.059 \n------------------------------------------------------------------------------------------\n\n\n\nolsrr::ols_step_forward_aic(fit)\n\n\n                             Stepwise Summary                             \n------------------------------------------------------------------------\nStep    Variable        AIC         SBC       SBIC      R2       Adj. R2 \n------------------------------------------------------------------------\n 0      Base Model    3616.730    3625.183      NA    0.00000    0.00000 \n 1      rad           3367.573    3380.252      NA    0.39126    0.39005 \n 2      lstat         3344.403    3361.309      NA    0.42080    0.41849 \n 3      medv          3343.164    3364.297      NA    0.42449    0.42105 \n 4      ptratio       3341.763    3367.122      NA    0.42835    0.42378 \n------------------------------------------------------------------------\n\nFinal Model Output \n------------------\n\n                         Model Summary                           \n----------------------------------------------------------------\nR                       0.654       RMSE                  6.497 \nR-Squared               0.428       MSE                  42.632 \nAdj. R-Squared          0.424       Coef. Var           180.692 \nPred R-Squared          0.414       AIC                3341.763 \nMAE                     2.792       SBC                3367.122 \n----------------------------------------------------------------\n RMSE: Root Mean Square Error \n MSE: Mean Square Error \n MAE: Mean Absolute Error \n AIC: Akaike Information Criteria \n SBC: Schwarz Bayesian Criteria \n\n                                ANOVA                                  \n----------------------------------------------------------------------\n                 Sum of                                               \n                Squares         DF    Mean Square      F         Sig. \n----------------------------------------------------------------------\nRegression    16004.437          4       4001.109    93.852    0.0000 \nResidual      21358.785        501         42.632                     \nTotal         37363.222        505                                    \n----------------------------------------------------------------------\n\n                                  Parameter Estimates                                    \n----------------------------------------------------------------------------------------\n      model      Beta    Std. Error    Std. Beta      t        Sig      lower     upper \n----------------------------------------------------------------------------------------\n(Intercept)     4.936         3.887                  1.270    0.205    -2.700    12.573 \n        rad     0.548         0.041        0.554    13.343    0.000     0.467     0.628 \n      lstat     0.144         0.064        0.119     2.228    0.026     0.017     0.270 \n       medv    -0.120         0.051       -0.128    -2.365    0.018    -0.219    -0.020 \n    ptratio    -0.307         0.167       -0.077    -1.838    0.067    -0.635     0.021 \n----------------------------------------------------------------------------------------"
  },
  {
    "objectID": "exercise/ex-3-mlr.html",
    "href": "exercise/ex-3-mlr.html",
    "title": "Exercise 3: Multiple Linear Regression",
    "section": "",
    "text": "Note"
  },
  {
    "objectID": "exercise/ex-3-mlr.html#multiple-linear-regression",
    "href": "exercise/ex-3-mlr.html#multiple-linear-regression",
    "title": "Exercise 3: Multiple Linear Regression",
    "section": "Multiple Linear Regression",
    "text": "Multiple Linear Regression\n\n\nThis problem involves the Boston data set. We now try to predict per capita crime rate using the other variables in this data set. In other words, per capita crime rate is the response, and the other variables are the predictors.\n\nFor each predictor, fit a simple linear regression model to predict the response. Describe your results. In which of the models is there a statistically significant association between the predictor and the response? Create some plots to back up your assertions.\nFit a multiple regression model to predict the response using all of the predictors. Describe your results. For which predictors can we reject the null hypothesis \\(H_0 : \\beta_j = 0\\)?\nHow do your results from (a) compare to your results from (b)? Create a plot displaying the univariate regression coefficients from (a) on the x-axis, and the multiple regression coefficients from (b) on the y-axis. That is, each predictor is displayed as a single point in the plot. Its coefficient in a simple linear regression model is shown on the x-axis, and its coefficient estimate in the multiple linear regression model is shown on the y-axis."
  },
  {
    "objectID": "exercise/ex-11-logistic-sol.html",
    "href": "exercise/ex-11-logistic-sol.html",
    "title": "Exercise 11: Logistic Regression Solution",
    "section": "",
    "text": "Note\n\n\n\nExercises are for practice purpose only."
  },
  {
    "objectID": "exercise/ex-11-logistic-sol.html#logistic-regression",
    "href": "exercise/ex-11-logistic-sol.html#logistic-regression",
    "title": "Exercise 11: Logistic Regression Solution",
    "section": "Logistic Regression",
    "text": "Logistic Regression\n\n\nSuppose we collect data for a group of students in a statistics class with variables \\(X_1 =\\)hours studied, \\(X_2 =\\)undergrad GPA, and \\(Y =\\)receive an A. We fit a logistic regression and produce estimated coefficient, \\(\\hat{\\beta_0} = ‚àí6\\), \\(\\hat{\\beta_1} = 0.05\\), \\(\\hat{\\beta_2} = 1\\).\n\nEstimate the probability that a student who studies for 40 hours and has an undergrad GPA of 3.5 gets an A in the class.\nHow many hours would the student in part (a) need to study to have a 50% chance of getting an A in the class?\n\n\n\nb0 <- -6\nb1 <- 0.05\nb2 <- 1\nx2 <- 3.5\nx1 <- 40\n1 / (1 + exp(-(b0 + b1 * x1 + b2 * x2)))\n\n[1] 0.3775407\n\n\nThe followings use Boston data set.\n\nlibrary(ISLR2)\nattach(Boston)\n\n\nCreate a new variable crim01 having value 0 if crim is smaller than its median, and value 1 otherwise. Add the variable into the Boston data set.\n\n\ncrim01 <- rep(0, length(crim))\ncrim01[crim > median(crim)] <- 1\nBoston1 <- cbind(Boston, crim01)\n\n\nSplit the new Boston data set into training (~80%) and test sets (~20%).\n\n\nset.seed(1)\nidx <- sample(nrow(Boston1), 406)\ntrain <- Boston1[idx, ]\ntest <- Boston1[-idx, ]\n\n\nUsing the training set, fit logistic regression to predict whether a given census tract in the test set has a crime rate above or below the median.\n\n\nfit <- glm(crim01 ~ . - crim01 - crim, data = train, \n           family = binomial)\nprob <- predict(fit, test, type = \"response\")\npred <- 1 * (prob > 0.5)\ntable(pred, test$crim01)\n\n    \npred  0  1\n   0 42  5\n   1  9 44\n\nmean(pred == test$crim01)\n\n[1] 0.86"
  },
  {
    "objectID": "exercise/ex-7-nonpar-sol.html",
    "href": "exercise/ex-7-nonpar-sol.html",
    "title": "Exercise 7: Nonparametric Regression Solution",
    "section": "",
    "text": "Note\n\n\n\nExercises are for practice purpose only."
  },
  {
    "objectID": "exercise/ex-7-nonpar-sol.html#nonparametric-regression",
    "href": "exercise/ex-7-nonpar-sol.html#nonparametric-regression",
    "title": "Exercise 7: Nonparametric Regression Solution",
    "section": "Nonparametric Regression",
    "text": "Nonparametric Regression\n\n\nFor the Boston data, use medv as the response and lstat as the predictor, then do\n\nKernel Smoother\nLocal Linear Regression\nLOESS\n\nTry to find the best parameter of each method to get the best fit.\n\n\nlibrary(ISLR2)\nattach(Boston)\n\n\nbw <- c(0.2, 0.5, 0.8, 1, 2, 3, 4)\nplot(lstat, medv, main = \"Kernel Smoother\")\nfor (i in 1: length(bw)) {\n    ks_fit <- KernSmooth::locpoly(lstat, medv, degree = 0, bandwidth = bw[i], \n                                   kernel = \"normal\")\n    lines(ks_fit$x, ks_fit$y, col = i, lwd = 2)\n}\nlegend(\"topright\", paste(\"bandwidth =\", bw), lwd = 2, col = 1:length(bw))\n\n\n\n\n\nbw <- c(0.2, 0.5, 0.8, 1, 2, 3, 4)\nplot(lstat, medv, main = \"Local Linear Regression\")\nfor (i in 1: length(bw)) {\n    local_fit <- KernSmooth::locpoly(lstat, medv, degree = 1, bandwidth = bw[i], \n                                   kernel = \"normal\")\n    lines(local_fit$x, local_fit$y, col = i, lwd = 2)\n}\nlegend(\"topright\", paste(\"bandwidth =\", bw), lwd = 2, col = 1:length(bw))\n\n\n\n\n\nspan_vec <- c(0.1, 0.3, 0.5, 1)\nplot(lstat, medv, main = \"LOESS\")\nfor (i in 1: length(span_vec)) {\n  loess_fit <- loess(medv ~ lstat, span = span_vec[i], degree = 2)\n  lines(lstat[order(lstat)], loess_fit$fitted[order(lstat)], col = i, lwd = 2)\n}\nlegend(\"topright\", paste(\"span =\", span_vec), lwd = 2, col = 1:length(span_vec))"
  },
  {
    "objectID": "exercise/ex-10-var.html",
    "href": "exercise/ex-10-var.html",
    "title": "Exercise 10: Variable Selection",
    "section": "",
    "text": "Note"
  },
  {
    "objectID": "exercise/ex-10-var.html#variable-selection",
    "href": "exercise/ex-10-var.html#variable-selection",
    "title": "Exercise 10: Variable Selection",
    "section": "Variable Selection",
    "text": "Variable Selection\n\n\nConsider Boston data with all variables except chas. Use crim as the response. Perform Best Subset Selection and Forward Selection."
  },
  {
    "objectID": "exercise/ex-6-poly.html",
    "href": "exercise/ex-6-poly.html",
    "title": "Exercise 6: Polynomial Regression",
    "section": "",
    "text": "Note"
  },
  {
    "objectID": "exercise/ex-6-poly.html#polynomial-regression",
    "href": "exercise/ex-6-poly.html#polynomial-regression",
    "title": "Exercise 6: Polynomial Regression",
    "section": "Polynomial Regression",
    "text": "Polynomial Regression\n\n\nConsider the values of x shown below: \\[x = 1.00, 1.70, 1.25, 1.20, 1.45, 1.85, 1.60, 1.50, 1.95, 2.00\\] Suppose that we wish to fit a second-order model using these levels for the regressor variable \\(x\\). Calculate the correlation between \\(x\\) and \\(x^2\\). Do you see any potential difficulties in fitting the model?\n\n\n\nIs there evidence of non-linear association between any of the predictors and the response crim in the Boston data set? To answer this question, for each predictor \\(X\\), fit a model of the form \\[Y = \\beta_0 + \\beta_1X+\\beta_2X^2+\\beta_3X^3+\\epsilon\\]"
  },
  {
    "objectID": "exercise/ex-6-poly-sol.html",
    "href": "exercise/ex-6-poly-sol.html",
    "title": "Exercise 6: Polynomial Regression Solution",
    "section": "",
    "text": "Note\n\n\n\nExercises are for practice purpose only."
  },
  {
    "objectID": "exercise/ex-6-poly-sol.html#polynomial-regression",
    "href": "exercise/ex-6-poly-sol.html#polynomial-regression",
    "title": "Exercise 6: Polynomial Regression Solution",
    "section": "Polynomial Regression",
    "text": "Polynomial Regression\n\n\nConsider the values of x shown below: \\[x = 1.00, 1.70, 1.25, 1.20, 1.45, 1.85, 1.60, 1.50, 1.95, 2.00\\] Suppose that we wish to fit a second-order model using these levels for the regressor variable \\(x\\). Calculate the correlation between \\(x\\) and \\(x^2\\). Do you see any potential difficulties in fitting the model?\n\n\nx <- c(1.00, 1.70, 1.25, 1.20, 1.45, 1.85, 1.60, 1.50, 1.95, 2.00)\nx2 <- x ^ 2\ncor(x, x2)\n\n[1] 0.9954134\n\n\n\n\nIs there evidence of non-linear association between any of the predictors and the response crim in the Boston data set? To answer this question, for each predictor \\(X\\), fit a model of the form \\[Y = \\beta_0 + \\beta_1X+\\beta_2X^2+\\beta_3X^3+\\epsilon\\]\n\n\nlibrary(ISLR2)\nname <- names(Boston)\n# 'degree' must be less than number of unique points\nfor (i in 2:13) {\n  if (name[i] == \"chas\") next\n  print(name[i])\n  poly_reg <- lm(Boston$crim ~ poly(Boston[[i]], degree = 3))\n  print(summary(poly_reg), digits = 2)\n}\n\n[1] \"zn\"\n\nCall:\nlm(formula = Boston$crim ~ poly(Boston[[i]], degree = 3))\n\nResiduals:\n   Min     1Q Median     3Q    Max \n  -4.8   -4.6   -1.3    0.5   84.1 \n\nCoefficients:\n                               Estimate Std. Error t value Pr(>|t|)    \n(Intercept)                        3.61       0.37     9.7   <2e-16 ***\npoly(Boston[[i]], degree = 3)1   -38.75       8.37    -4.6    5e-06 ***\npoly(Boston[[i]], degree = 3)2    23.94       8.37     2.9    0.004 ** \npoly(Boston[[i]], degree = 3)3   -10.07       8.37    -1.2    0.230    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 8.4 on 502 degrees of freedom\nMultiple R-squared:  0.058, Adjusted R-squared:  0.053 \nF-statistic:  10 on 3 and 502 DF,  p-value: 1.3e-06\n\n[1] \"indus\"\n\nCall:\nlm(formula = Boston$crim ~ poly(Boston[[i]], degree = 3))\n\nResiduals:\n   Min     1Q Median     3Q    Max \n  -8.3   -2.5    0.1    0.8   79.7 \n\nCoefficients:\n                               Estimate Std. Error t value Pr(>|t|)    \n(Intercept)                        3.61       0.33    11.0   <2e-16 ***\npoly(Boston[[i]], degree = 3)1    78.59       7.42    10.6   <2e-16 ***\npoly(Boston[[i]], degree = 3)2   -24.39       7.42    -3.3    0.001 ** \npoly(Boston[[i]], degree = 3)3   -54.13       7.42    -7.3    1e-12 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 7.4 on 502 degrees of freedom\nMultiple R-squared:  0.26,  Adjusted R-squared:  0.26 \nF-statistic:  59 on 3 and 502 DF,  p-value: <2e-16\n\n[1] \"nox\"\n\nCall:\nlm(formula = Boston$crim ~ poly(Boston[[i]], degree = 3))\n\nResiduals:\n   Min     1Q Median     3Q    Max \n  -9.1   -2.1   -0.3    0.7   78.3 \n\nCoefficients:\n                               Estimate Std. Error t value Pr(>|t|)    \n(Intercept)                        3.61       0.32    11.2   <2e-16 ***\npoly(Boston[[i]], degree = 3)1    81.37       7.23    11.2   <2e-16 ***\npoly(Boston[[i]], degree = 3)2   -28.83       7.23    -4.0    8e-05 ***\npoly(Boston[[i]], degree = 3)3   -60.36       7.23    -8.3    7e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 7.2 on 502 degrees of freedom\nMultiple R-squared:  0.3,   Adjusted R-squared:  0.29 \nF-statistic:  71 on 3 and 502 DF,  p-value: <2e-16\n\n[1] \"rm\"\n\nCall:\nlm(formula = Boston$crim ~ poly(Boston[[i]], degree = 3))\n\nResiduals:\n   Min     1Q Median     3Q    Max \n -18.5   -3.5   -2.2    0.0   87.2 \n\nCoefficients:\n                               Estimate Std. Error t value Pr(>|t|)    \n(Intercept)                        3.61       0.37     9.8   <2e-16 ***\npoly(Boston[[i]], degree = 3)1   -42.38       8.33    -5.1    5e-07 ***\npoly(Boston[[i]], degree = 3)2    26.58       8.33     3.2    0.002 ** \npoly(Boston[[i]], degree = 3)3    -5.51       8.33    -0.7    0.509    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 8.3 on 502 degrees of freedom\nMultiple R-squared:  0.068, Adjusted R-squared:  0.062 \nF-statistic:  12 on 3 and 502 DF,  p-value: 1.1e-07\n\n[1] \"age\"\n\nCall:\nlm(formula = Boston$crim ~ poly(Boston[[i]], degree = 3))\n\nResiduals:\n   Min     1Q Median     3Q    Max \n  -9.8   -2.7   -0.5    0.0   82.8 \n\nCoefficients:\n                               Estimate Std. Error t value Pr(>|t|)    \n(Intercept)                        3.61       0.35    10.4   <2e-16 ***\npoly(Boston[[i]], degree = 3)1    68.18       7.84     8.7   <2e-16 ***\npoly(Boston[[i]], degree = 3)2    37.48       7.84     4.8    2e-06 ***\npoly(Boston[[i]], degree = 3)3    21.35       7.84     2.7    0.007 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 7.8 on 502 degrees of freedom\nMultiple R-squared:  0.17,  Adjusted R-squared:  0.17 \nF-statistic:  35 on 3 and 502 DF,  p-value: <2e-16\n\n[1] \"dis\"\n\nCall:\nlm(formula = Boston$crim ~ poly(Boston[[i]], degree = 3))\n\nResiduals:\n   Min     1Q Median     3Q    Max \n -10.8   -2.6    0.0    1.3   76.4 \n\nCoefficients:\n                               Estimate Std. Error t value Pr(>|t|)    \n(Intercept)                        3.61       0.33    11.1   <2e-16 ***\npoly(Boston[[i]], degree = 3)1   -73.39       7.33   -10.0   <2e-16 ***\npoly(Boston[[i]], degree = 3)2    56.37       7.33     7.7    8e-14 ***\npoly(Boston[[i]], degree = 3)3   -42.62       7.33    -5.8    1e-08 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 7.3 on 502 degrees of freedom\nMultiple R-squared:  0.28,  Adjusted R-squared:  0.27 \nF-statistic:  64 on 3 and 502 DF,  p-value: <2e-16\n\n[1] \"rad\"\n\nCall:\nlm(formula = Boston$crim ~ poly(Boston[[i]], degree = 3))\n\nResiduals:\n   Min     1Q Median     3Q    Max \n -10.4   -0.4   -0.3    0.2   76.2 \n\nCoefficients:\n                               Estimate Std. Error t value Pr(>|t|)    \n(Intercept)                         3.6        0.3    12.2   <2e-16 ***\npoly(Boston[[i]], degree = 3)1    120.9        6.7    18.1   <2e-16 ***\npoly(Boston[[i]], degree = 3)2     17.5        6.7     2.6    0.009 ** \npoly(Boston[[i]], degree = 3)3      4.7        6.7     0.7    0.482    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6.7 on 502 degrees of freedom\nMultiple R-squared:  0.4,   Adjusted R-squared:  0.4 \nF-statistic: 1.1e+02 on 3 and 502 DF,  p-value: <2e-16\n\n[1] \"tax\"\n\nCall:\nlm(formula = Boston$crim ~ poly(Boston[[i]], degree = 3))\n\nResiduals:\n   Min     1Q Median     3Q    Max \n -13.3   -1.4    0.0    0.5   76.9 \n\nCoefficients:\n                               Estimate Std. Error t value Pr(>|t|)    \n(Intercept)                         3.6        0.3    11.9   <2e-16 ***\npoly(Boston[[i]], degree = 3)1    112.7        6.8    16.4   <2e-16 ***\npoly(Boston[[i]], degree = 3)2     32.1        6.8     4.7    4e-06 ***\npoly(Boston[[i]], degree = 3)3     -8.0        6.8    -1.2      0.2    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6.9 on 502 degrees of freedom\nMultiple R-squared:  0.37,  Adjusted R-squared:  0.37 \nF-statistic:  98 on 3 and 502 DF,  p-value: <2e-16\n\n[1] \"ptratio\"\n\nCall:\nlm(formula = Boston$crim ~ poly(Boston[[i]], degree = 3))\n\nResiduals:\n   Min     1Q Median     3Q    Max \n  -6.8   -4.1   -1.7    1.4   82.7 \n\nCoefficients:\n                               Estimate Std. Error t value Pr(>|t|)    \n(Intercept)                        3.61       0.36    10.0   <2e-16 ***\npoly(Boston[[i]], degree = 3)1    56.05       8.12     6.9    2e-11 ***\npoly(Boston[[i]], degree = 3)2    24.77       8.12     3.1    0.002 ** \npoly(Boston[[i]], degree = 3)3   -22.28       8.12    -2.7    0.006 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 8.1 on 502 degrees of freedom\nMultiple R-squared:  0.11,  Adjusted R-squared:  0.11 \nF-statistic:  21 on 3 and 502 DF,  p-value: 4.2e-13\n\n[1] \"lstat\"\n\nCall:\nlm(formula = Boston$crim ~ poly(Boston[[i]], degree = 3))\n\nResiduals:\n   Min     1Q Median     3Q    Max \n -15.2   -2.2   -0.5    0.1   83.4 \n\nCoefficients:\n                               Estimate Std. Error t value Pr(>|t|)    \n(Intercept)                        3.61       0.34    10.7   <2e-16 ***\npoly(Boston[[i]], degree = 3)1    88.07       7.63    11.5   <2e-16 ***\npoly(Boston[[i]], degree = 3)2    15.89       7.63     2.1     0.04 *  \npoly(Boston[[i]], degree = 3)3   -11.57       7.63    -1.5     0.13    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 7.6 on 502 degrees of freedom\nMultiple R-squared:  0.22,  Adjusted R-squared:  0.21 \nF-statistic:  47 on 3 and 502 DF,  p-value: <2e-16\n\n[1] \"medv\"\n\nCall:\nlm(formula = Boston$crim ~ poly(Boston[[i]], degree = 3))\n\nResiduals:\n   Min     1Q Median     3Q    Max \n -24.4   -2.0   -0.4    0.4   73.7 \n\nCoefficients:\n                               Estimate Std. Error t value Pr(>|t|)    \n(Intercept)                        3.61       0.29    12.4   <2e-16 ***\npoly(Boston[[i]], degree = 3)1   -75.06       6.57   -11.4   <2e-16 ***\npoly(Boston[[i]], degree = 3)2    88.09       6.57    13.4   <2e-16 ***\npoly(Boston[[i]], degree = 3)3   -48.03       6.57    -7.3    1e-12 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6.6 on 502 degrees of freedom\nMultiple R-squared:  0.42,  Adjusted R-squared:  0.42 \nF-statistic: 1.2e+02 on 3 and 502 DF,  p-value: <2e-16"
  },
  {
    "objectID": "exercise/ex-2-slr.html",
    "href": "exercise/ex-2-slr.html",
    "title": "Exercise 2: Simple Linear Regression",
    "section": "",
    "text": "Note"
  },
  {
    "objectID": "exercise/ex-2-slr.html#simple-linear-regression",
    "href": "exercise/ex-2-slr.html#simple-linear-regression",
    "title": "Exercise 2: Simple Linear Regression",
    "section": "Simple Linear Regression",
    "text": "Simple Linear Regression\n\n\nBasic Understanding\n\nThe scatterplot and least squares summary below show the relationship between weight measured in kilograms and height measured in centimeters of 507 physically active individuals. (Heinz et al., 2003)\n\n\n\n\n\n\n\n\n \n  \n    term \n    estimate \n    std.error \n    statistic \n    p.value \n  \n \n\n  \n    (Intercept) \n    -105.01 \n    7.54 \n    -13.93 \n    <0.0001 \n  \n  \n    hgt \n    1.02 \n    0.04 \n    23.13 \n    <0.0001 \n  \n\n\n\n\n\n\nDescribe the relationship between height and weight.\nWrite the equation of the regression line. Interpret the slope and intercept in context.\nDo the data provide convincing evidence that the true slope parameter is different than 0? State the null and alternative hypotheses, report the p-value, and state your conclusion.\nThe correlation coefficient for height and weight is 0.72. Calculate \\(R^2\\) and interpret it.\n\n\n\nSimulation\n\nGenerate a simulated data of size \\(n = 100\\) from the regression\n\n\\[y_i = 10 + 5x_i + \\epsilon_i, ~~ \\epsilon_i \\stackrel{iid}{\\sim} N(0, 2)\\]\nby completing the code\n\nx <- runif(_____)\ny <- ____ + ____ * ____ + r____(_____, sd = _____)\n\nFit a simple linear regression model to the data, then check whether the true slope is captured by the 90% confidence interval for the slope."
  },
  {
    "objectID": "exercise/ex-1-overview.html",
    "href": "exercise/ex-1-overview.html",
    "title": "Exercise 1: Overview",
    "section": "",
    "text": "Note"
  },
  {
    "objectID": "exercise/ex-1-overview.html#overview-of-regression",
    "href": "exercise/ex-1-overview.html#overview-of-regression",
    "title": "Exercise 1: Overview",
    "section": "Overview of Regression",
    "text": "Overview of Regression\n\n\nDescribe three real-life applications in which regression might be useful. Describe the response, as well as the predictors. Is the goal of each application inference or prediction? Explain your answer.\nDescribe the differences between a parametric and a non-parametric statistical learning approach."
  },
  {
    "objectID": "exercise/ex-1-overview.html#checking-data-with-r",
    "href": "exercise/ex-1-overview.html#checking-data-with-r",
    "title": "Exercise 1: Overview",
    "section": "Checking Data with R",
    "text": "Checking Data with R\n\n\nLoad in the Boston data set in ISLR2 package.\nHow many rows are in this data set? How many columns? What do the rows and columns represent?\nMake some pairwise scatterplots of the predictors (columns) in this data set. Describe your findings.\nAre any of the predictors associated with per capita crime rate? If so, explain the relationship.\nDo any of the census tracts of Boston appear to have particularly high crime rates? Tax rates? Pupil-teacher ratios? Comment on the range of each predictor.\nHow many of the census tracts in this data set bound the Charles river?\nWhat is the median pupil-teacher ratio among the towns in this data set?"
  },
  {
    "objectID": "exercise/ex-1-overview.html#probability-and-statistics",
    "href": "exercise/ex-1-overview.html#probability-and-statistics",
    "title": "Exercise 1: Overview",
    "section": "Probability and Statistics",
    "text": "Probability and Statistics\n\n\nPlot normal density curves with different choices of mean and standard deviation.\nChoose a continuous variable in Boston. Use the sample() function to draw a simple random sample of size 20 from this population. Calculate the sample average.\nRepeat 2. several times to get a sampling distribution of the sample mean."
  },
  {
    "objectID": "exercise/ex-9-collinearity-sol.html",
    "href": "exercise/ex-9-collinearity-sol.html",
    "title": "Exercise 9: Collinearity Solution",
    "section": "",
    "text": "Note\n\n\n\nExercises are for practice purpose only."
  },
  {
    "objectID": "exercise/ex-9-collinearity-sol.html#collinearity",
    "href": "exercise/ex-9-collinearity-sol.html#collinearity",
    "title": "Exercise 9: Collinearity Solution",
    "section": "Collinearity",
    "text": "Collinearity\n\n\nPerform the code\n\n\nset.seed(1)\nx1 <- runif(100)\nx2 <- 0.5 * x1 + rnorm (100) / 10\ny <- 2 + 2 * x1 + 0.3 * x2 + rnorm (100)\n\nWrite out the form of the linear model. What are the regression coefficients?\n\n# y = beta0 + beta1 x1 + beta2 x2 + epsilon\n# beta0 = 2; beta1 = 2, beta2 = 0.3\n\n\nCreate a scatterplot displaying the relationship between x1 and x2.\n\n\nplot(x1, x2)\n\n\n\n\n\nFit a least squares regression to the data using x1 and x2. How the LSEs relate to the true \\(\\beta_0\\), \\(\\beta_1\\) and \\(\\beta_2\\)? Can you reject \\(H0 : \\beta_1 = 0\\)? How about \\(H0 : \\beta_2 = 0\\)?\n\n\nfit <- lm(y ~ x1 + x2)\nround(summary(fit)$coef, 3)\n\n            Estimate Std. Error t value Pr(>|t|)\n(Intercept)     2.13      0.232   9.188    0.000\nx1              1.44      0.721   1.996    0.049\nx2              1.01      1.134   0.891    0.375\n\n\n\nFit a least squares regression using only x1. Comment on your results. Can you reject \\(H0 : \\beta_1 = 0\\)?\n\n\nfit1 <- lm(y ~ x1)\nround(summary(fit1)$coef, 3)\n\n            Estimate Std. Error t value Pr(>|t|)\n(Intercept)    2.112      0.231   9.155        0\nx1             1.976      0.396   4.986        0\n\n\n\nFit a least squares regression using only x2. Comment on your results. Can you reject \\(H0 : \\beta_2 = 0\\)?\n\n\nfit2 <- lm(y ~ x2)\nround(summary(fit2)$coef, 3)\n\n            Estimate Std. Error t value Pr(>|t|)\n(Intercept)     2.39      0.195  12.261        0\nx2              2.90      0.633   4.580        0\n\n\n\nDo the results obtained in (3)‚Äì(5) contradict each other? Explain your answer.\n\n\n# No, because x1 and x2 have collinearity, it is hard to distinguish their effects when regressed upon together. When they are regressed upon separately, the linear relationship between y and each predictor is indicated more clearly. When using two variables that are highly collinear, the effect on the response of one variable can be masked by the other.\n\n\nNow suppose we obtain one additional observation, which was unfortunately mismeasured. Re-fit the linear models from (3) to (5) using this new data. What effect does this new observation have on the each of the models? In each model, is this observation an outlier? A high-leverage point? Both? Explain your answers.\n\n\nx1 <- c(x1, 0.1)\nx2 <- c(x2, 0.8)\ny <- c(y, 6)\nplot(x1, x2)\npoints(0.1, 0.8, pch = 16, col = 2)\n\n\n\n\n\nmisfit <- lm(y ~ x1 + x2)\nround(summary(misfit)$coef, 3)\n\n            Estimate Std. Error t value Pr(>|t|)\n(Intercept)    2.227      0.231   9.624    0.000\nx1             0.539      0.592   0.911    0.365\nx2             2.515      0.898   2.801    0.006\n\nmisfit1 <- lm(y ~ x1)\nround(summary(misfit1)$coef, 3)\n\n            Estimate Std. Error t value Pr(>|t|)\n(Intercept)    2.257      0.239   9.445        0\nx1             1.766      0.412   4.282        0\n\nmisfit2 <- lm(y ~ x2)\nround(summary(misfit2)$coef, 3)\n\n            Estimate Std. Error t value Pr(>|t|)\n(Intercept)    2.345      0.191  12.264        0\nx2             3.119      0.604   5.164        0\n\n\n\nplot(misfit$fitted.values, rstudent(misfit))\n\n\n\nrstud <- round(sort(rstudent(misfit), decreasing = TRUE), 2)\nrstud[abs(rstud) > 2]\n\n   21   101    16    55    82 \n 2.23  2.11  2.01 -2.27 -2.64 \n\nhatval <- round(sort(hatvalues(misfit), decreasing = TRUE), 2)\nhatval[hatval > 2 * 3 / 101]\n\n 101   11    6   47   18 \n0.41 0.06 0.06 0.06 0.06 \n\n\n\nplot(misfit1$fitted.values, rstudent(misfit1))\n\n\n\nrstud1 <- round(sort(rstudent(misfit1), decreasing = TRUE), 2)\nrstud1[abs(rstud1) > 2]\n\n  101    21    56    55    82 \n 3.44  2.27  2.17 -2.38 -2.70 \n\nhatval1 <- round(sort(hatvalues(misfit1), decreasing = TRUE), 2)\nhatval1[hatval1 > 2 * 2 / 101]\n\n  27   47   18   92   10   80   55    7   69 \n0.04 0.04 0.04 0.04 0.04 0.04 0.04 0.04 0.04 \n\n\n\nplot(misfit2$fitted.values, rstudent(misfit2))\n\n\n\nrstud2 <- round(sort(rstudent(misfit2), decreasing = TRUE), 2)\nrstud2[abs(rstud2) > 2]\n\n   21     5    55    82 \n 2.30 -2.01 -2.33 -2.55 \n\nhatval2 <- round(sort(hatvalues(misfit2), decreasing = TRUE), 2)\nhatval2[hatval2 > 2 * 2 / 101]\n\n 101    6   18   47   20   91   34   86   27   24   55   72 \n0.10 0.06 0.06 0.06 0.05 0.05 0.04 0.04 0.04 0.04 0.04 0.04"
  },
  {
    "objectID": "course-support.html",
    "href": "course-support.html",
    "title": "Course support",
    "section": "",
    "text": "Most of you will need help at some point and we want to make sure you can identify when that is without getting too frustrated and feel comfortable seeking help."
  },
  {
    "objectID": "course-support.html#lectures-and-labs",
    "href": "course-support.html#lectures-and-labs",
    "title": "Course support",
    "section": "Lectures and labs",
    "text": "Lectures and labs\nIf you have a question during lecture or lab, feel free to ask it! There are likely other students with the same question, so by asking you will create a learning opportunity for everyone."
  },
  {
    "objectID": "course-support.html#office-hours",
    "href": "course-support.html#office-hours",
    "title": "Course support",
    "section": "Office hours",
    "text": "Office hours\nThe teaching team is here to help you be successful in the course. You are encouraged to attend office hours during the times posted on the home page to ask questions about the course content and assignments. A lot of questions are most effectively answered in-person, so office hours are a valuable resource. I encourage each and every one of you to take advantage of this resource! Make a pledge to stop by office hours at least once during the first three weeks of class. If you truly have no questions to ask, just stop by and say hi and introduce yourself. You can find a list of everyone‚Äôs office hours here."
  },
  {
    "objectID": "course-support.html#discussion-forum",
    "href": "course-support.html#discussion-forum",
    "title": "Course support",
    "section": "Discussion forum",
    "text": "Discussion forum\nHave a question that can‚Äôt wait for office hours? Prefer to write out your question in detail rather than asking in person? The online discussion forum is the best venue for these! We will use Conversations as the online discussion forum. There is a chance another student has already asked a similar question, so please check the other posts on Ed Discussion before adding a new question. If you know the answer to a question that is posted, I encourage you to respond!"
  },
  {
    "objectID": "course-support.html#email",
    "href": "course-support.html#email",
    "title": "Course support",
    "section": "Email",
    "text": "Email\nPlease refrain from emailing any course content questions (those should go Conversations), and only use email for questions about personal matters that may not be appropriate for the public course forum (e.g., illness, accommodations, etc.). For such matters, you may email Dr.¬†Mine √áetinkaya-Rundel at mc301@duke.edu.\nIf there is a question that‚Äôs not appropriate for the public forum, you are welcome to email me directly. If you email me, please include ‚ÄúSTA 210‚Äù in the subject line. Barring extenuating circumstances, I will respond to STA 210 emails within 48 hours Monday - Friday. Response time may be slower for emails sent Friday evening - Sunday."
  },
  {
    "objectID": "course-support.html#academic-support",
    "href": "course-support.html#academic-support",
    "title": "Course support",
    "section": "Academic support",
    "text": "Academic support\nThere are times you may need help with the class that is beyond what can be provided by the teaching team. In those instances, I encourage you to visit the Academic Resource Center. The Academic Resource Center (ARC) offers free services to all students during their undergraduate careers at Duke. Services include Learning Consultations, Peer Tutoring and Study Groups, ADHD/LD Coaching, Outreach Workshops, and more. Because learning is a process unique to every individual, they work with each student to discover and develop their own academic strategy for success at Duke. ARC services are available free to any Duke undergraduate student, in any year, studying in any discipline. Contact the ARC to schedule an appointment. Undergraduates in any year, studying any discipline can benefit! Contact theARC@duke.edu, 919-684-5917."
  },
  {
    "objectID": "course-support.html#mental-health-and-wellness",
    "href": "course-support.html#mental-health-and-wellness",
    "title": "Course support",
    "section": "Mental health and wellness",
    "text": "Mental health and wellness\nStudent mental health and wellness is of primary importance at Duke, and the university offers resources to support students in managing daily stress and self-care. Duke offers several resources for students to seek assistance on coursework and to nurture daily habits that support overall well-being, some of which are listed below:\n\nThe Academic Resource Center: (919) 684-5917, theARC@duke.edu, or arc.duke.edu,\nDuWell: (919) 681-8421, duwell@studentaffairs.duke.edu, or studentaffairs.duke.edu/duwell\n\nIf your mental health concerns and/or stressful events negatively affect your daily emotional state, academic performance, or ability to participate in your daily activities, many resources are available to help you through difficult times. Duke encourages all students to access these resources.\n\nDukeReach. Provides comprehensive outreach services to identify and support students in managing all aspects of well-being. If you have concerns about a student‚Äôs behavior or health visit the website for resources and assistance. studentaffairs.duke.edu/dukereach\nCounseling and Psychological Services (CAPS). CAPS helps Duke Students enhance strengths and develop abilities to successfully live, grow and learn in their personal and academic lives. CAPS recognizes that we are living in unprecedented times and that the changes, challenges and stressors brought on by the COVID-19 pandemic have impacted everyone, often in ways that are tax our well-being. CAPS offers many services to Duke undergraduate students, including brief individual and group counseling, couples counseling and more. CAPS staff also provides outreach to student groups, particularly programs supportive of at-risk populations, on a wide range of issues impacting them in various aspects of campus life. CAPS provides services to students via Telehealth. To initiate services, you can contact their front desk at 919-660-1000. studentaffairs.duke.edu/caps\nBlue Devils Care. A convenient and cost-effective way for Duke students to receive 24/7 mental health support through TalkNow and scheduled counseling. bluedevilscare.duke.edu\nTwo-Click Support. Duke Student Government and DukeReach partnership that connects students to help in just two clicks. bit.ly/TwoClickSupport\nWellTrack. Sign up for WellTrack at app.welltrack.com."
  },
  {
    "objectID": "course-support.html#technology-accommodations",
    "href": "course-support.html#technology-accommodations",
    "title": "Course support",
    "section": "Technology accommodations",
    "text": "Technology accommodations\nStudents with demonstrated high financial need who have limited access to computers may request assistance in the form of loaner laptops. For technology assistance requests, please go here. Please note that supplies are limited.\nNote that we will be using Duke‚Äôs computational resources in this course. These resources are freely available to you. As long as your computer can connect to the internet and open a browser window, you can perform the necessary computing for this course. All software we use is open-source and/or freely available."
  },
  {
    "objectID": "course-support.html#course-materials-costs",
    "href": "course-support.html#course-materials-costs",
    "title": "Course support",
    "section": "Course materials costs",
    "text": "Course materials costs\nThere are no costs associated with this course. All readings will come from freely available, open resources (open-source textbooks, journal articles, etc.)."
  },
  {
    "objectID": "course-support.html#assistance-with-zoom-or-sakai",
    "href": "course-support.html#assistance-with-zoom-or-sakai",
    "title": "Course support",
    "section": "Assistance with Zoom or Sakai",
    "text": "Assistance with Zoom or Sakai\nFor technical help with Sakai or Zoom, contact the Duke OIT Service Desk at oit.duke.edu/help. You can also access the self-service help documentation for Zoom here and for Sakai here.\nNote that we will be making minimal use of Sakai in this course (primarily for announcements and grade book). All assignment submission and discussion will take place on GitHub instead.\nZoom will be used for online office hours as well as as a backup option should we need to hold the course online instead of in person."
  },
  {
    "objectID": "project-description.html",
    "href": "project-description.html",
    "title": "Project description",
    "section": "",
    "text": "To be announced."
  },
  {
    "objectID": "course-syllabus.html",
    "href": "course-syllabus.html",
    "title": "Syllabus",
    "section": "",
    "text": "Click here to download the syllabus."
  },
  {
    "objectID": "course-syllabus.html#time-and-location",
    "href": "course-syllabus.html#time-and-location",
    "title": "Syllabus",
    "section": "Time and location",
    "text": "Time and location\n\n\n\n\nDay\nTime\nLocation\n\n\n\n\nLectures\nTu & Th\n5:00 - 6:15 PM\nCudahy Hall 120\n\n\nLab\nNone\nNone\nNone"
  },
  {
    "objectID": "course-syllabus.html#office-hours",
    "href": "course-syllabus.html#office-hours",
    "title": "Syllabus",
    "section": "Office Hours",
    "text": "Office Hours\n\nMy in-person office hours are TuTh 3:20 - 4:50 PM, in Cudahy Hall room 353.\nYou are welcome to schedule an online meeting via Microsoft Teams if you need/prefer."
  },
  {
    "objectID": "course-syllabus.html#learning-objectives",
    "href": "course-syllabus.html#learning-objectives",
    "title": "Syllabus",
    "section": "Learning objectives",
    "text": "Learning objectives\nBy the end of the semester, you will be able to‚Ä¶\n\nanalyze real data to answer questions about relationships among variables.\nfit and evaluate linear and logistic regression models.\nassess whether a proposed model is appropriate and describe its limitations.\ncommunicate results from statistical analyses to a general audience."
  },
  {
    "objectID": "course-syllabus.html#prerequisites",
    "href": "course-syllabus.html#prerequisites",
    "title": "Syllabus",
    "section": "Prerequisites",
    "text": "Prerequisites\n\nMATH 2780 (Intro to Regression and Classification), MATH 4720 (Intro to Statistics) or equivalent.\nProgramming experience is helpful because the course involves doing regression analysis using  programming language.\nSome linear algebra exposure at the level of MATH 3100 (Linear Algebra and Matrix Theory) would helps especially for MSSC 5780 students as some matrix operations are used for modeling and computation.\nMSSC 5780 students, having taken MATH 4700 (Probability) and MATH 4710 (Statistical Inference) would help too.\nTalk to me if you are not sure whether or not this is the right course for you."
  },
  {
    "objectID": "course-syllabus.html#e-mail-policy",
    "href": "course-syllabus.html#e-mail-policy",
    "title": "Syllabus",
    "section": "E-mail Policy",
    "text": "E-mail Policy\n\nI will attempt to reply your email quickly, at least within 24 hours.\nExpect a reply on Monday if you send a question during weekends. If you do not receive a response from me within two days, re-send your question/comment in case there was a ‚Äúmix-up‚Äù with email communication (Hope this won‚Äôt happen!).\nPlease start your subject line with [math4780] or [mssc5780] followed by a clear description of your question. See an example below.\n\n\n\n\nEmail Subject Line Example\n\n\n\nEmail etiquette is important. Please read this article to learn more about email etiquette.\nI am more than happy to answer your questions about this course or statistics in general. However, due to time constraint, I may choose NOT to respond to students‚Äô e-mail if\n\nThe student could answer his/her own inquiry by reading the syllabus or information on the course website or D2L.\nThe student is asking for an extra credit opportunity. The answer is ‚Äúno‚Äù.\nThe student is requesting an extension on homework. The answer is ‚Äúno‚Äù.\nThe student is asking for a grade to be raised for no legitimate reason. The answer is ‚Äúno‚Äù.\nThe student is sending an email with no etiquette."
  },
  {
    "objectID": "course-syllabus.html#textbooks",
    "href": "course-syllabus.html#textbooks",
    "title": "Syllabus",
    "section": "Textbooks",
    "text": "Textbooks\n\n\n(IS) Introduction to Statistics by Cheng-Han Yu. (Good resource for brushing up your basic probability, statistics and simple linear regression knowledge.)\n(LRA) Introduction to Linear Regression Analysis, 6th edition, by D. C. Montgomery, E. A. Peck and G. G. Vining. Publisher: Wiley. (Graduate-level book that requires knowledge of matrix algebra.)\n(CAR) An R Companion to Applied Regression, by John Fox and Sanford Weisberg. Publisher: SAGE. (Doing regression much easier by the car and effects R packages.)\n(RD) Regression Daignostics, by John Fox. Publisher: SAGE. (Great concise book for learning regression diagnostics.)\n(CMR) Classical and Modern Regression with Applications, by Raymond Myers. Publisher: Duxbury Press. (The textbook when I was a master student.)."
  },
  {
    "objectID": "course-syllabus.html#programming-languages",
    "href": "course-syllabus.html#programming-languages",
    "title": "Syllabus",
    "section": "Programming Languages",
    "text": "Programming Languages\nThe default language of this course is . Python code will be provided as well. Due to time constraint, I will NOT teach you or Python programming from A to Z. If you never use R/Python and would like to grasp its syntax quickly, read my MATH 3570 R/Python programming slides on D2L. I am more than happy to answer any questions about if you have any. There are a lot of online resources for learning including the sites below.\n\nQuick-R\nHands-On Programming with R\n\n\n\nYou may use any other language to complete your homework, exams and project. However, I will not debug your code or provide comments on any technical issues of those languages."
  },
  {
    "objectID": "course-syllabus.html#grading-policy",
    "href": "course-syllabus.html#grading-policy",
    "title": "Syllabus",
    "section": "Grading Policy",
    "text": "Grading Policy\n\nStudents in this class range from juniors to second-year PhD students, so it makes sense to evaluate student performance using a different scale.\nThe homework/exams questions involving matrix algebra (MATH 3100) and calculus-based probability and statistical inference (MATH 4700/4710) are optional for MATH 4780 students.\nFor students in MATH 4780, the final grade is earned out of 1000 total points distributed as follows:\n\nHomework 1 to 6: 480 pts (80 pts each)\nIn-class Exam: 160 pts\nFinal project: 300 pts\nClass Participation: 60 pts\n\nFor students in MSSC 5780, the final grade is earned out of 1200 total points distributed as follows:\n\nHomework 1 to 6: 480 pts (80 pts each)\nIn-class Exam: 160 pts\nFinal project: 300 pts\nClass Participation: 60 pts\nMSSC 5780 work: 200 pts\n\nYou will NOT be allowed any extra credit projects/homework/exam to compensate for a poor average. Everyone must be given the same opportunity to do well in this class. Individual exam will NOT be curved.\nThe final grade is based on your percentage of points earned out of 1000 or 1200 points and the grade-percentage conversion Table. \\([x, y)\\) means greater than or equal to \\(x\\) and less than \\(y\\). For example, 94.1 is in \\([93, 100]\\) and the grade is A and 92.8 is in \\([90, 94)\\) and the grade is A-.\n\n\n\n\nGrade-Percentage Conversion\n\n\nGrade\nPercentage\n\n\n\n\nA\n[94, 100]\n\n\nA-\n[90, 94)\n\n\nB+\n[87, 90)\n\n\nB\n[83, 87)\n\n\nB-\n[80, 83)\n\n\nC+\n[77, 80)\n\n\nC\n[73, 77)\n\n\nC-\n[70, 73)\n\n\nD+\n[65, 70)\n\n\nD\n[60, 65)\n\n\nF\n[0, 60)\n\n\n\n\n\n\nThis is not a course that gives most of students grade A. If you want to obtain a good grade, study hard. No pain, no gain."
  },
  {
    "objectID": "course-syllabus.html#homework",
    "href": "course-syllabus.html#homework",
    "title": "Syllabus",
    "section": "Homework",
    "text": "Homework\n\nHomework will be assigned through the course website in weekly modules.\nTo submit your homework, please go to D2L > Assessments > Dropbox and upload your homework in PDF format.\n\nCombine images into a PDF file\nMicrosoft Word to PDF in 10 Seconds\nConvert images to PDF in Macbook/iMac\nCombine two or more images to get a single PDF file\n\nThere will be 6 homework sets to be graded, Homework 1 to 6.\nSome questions are required for MSSC students.\nYou will get a better understanding of the material if you discuss it with others. However, you must submit YOUR OWN work.\nNO LATE HOMEWORK WILL BE ACCEPTED NOR WILL YOU BE ALLOWED TO MAKE UP MISSED HOMEWORK!\n\n\n\nHandwriting is not allowed for data analysis part."
  },
  {
    "objectID": "course-syllabus.html#in-class-exam",
    "href": "course-syllabus.html#in-class-exam",
    "title": "Syllabus",
    "section": "In-Class Exam",
    "text": "In-Class Exam\n\nThe in-class exam covers materials in Week 1 to Week 7 on simple linear regression and multiple linear regression.\nSome questions are required for MSSC students.\nNO make-up midterm exam unless you miss exams due to COVID-19 symptoms, exposure, diagnosis, quarantine, and/or isolation, or you have an excused absence as defined in Attendance in Academic Regulations."
  },
  {
    "objectID": "course-syllabus.html#class-participation",
    "href": "course-syllabus.html#class-participation",
    "title": "Syllabus",
    "section": "Class Participation",
    "text": "Class Participation\n\nYou will be presenting exercise problems.\nMore details will be released later."
  },
  {
    "objectID": "course-syllabus.html#final-project",
    "href": "course-syllabus.html#final-project",
    "title": "Syllabus",
    "section": "Final Project",
    "text": "Final Project\n\nA detailed final project guideline will be released later."
  },
  {
    "objectID": "course-syllabus.html#university-and-college-policies",
    "href": "course-syllabus.html#university-and-college-policies",
    "title": "Syllabus",
    "section": "University and college policies",
    "text": "University and college policies\nAs a student in this course, you have agreed to comply with Marquette undergraduate policies and regulations."
  },
  {
    "objectID": "course-syllabus.html#accommodation",
    "href": "course-syllabus.html#accommodation",
    "title": "Syllabus",
    "section": "Accommodation",
    "text": "Accommodation\nIf you need to request accommodations, or modify existing accommodations that address disability-related needs, please contact Disability Service."
  },
  {
    "objectID": "course-syllabus.html#important-dates",
    "href": "course-syllabus.html#important-dates",
    "title": "Syllabus",
    "section": "Important dates",
    "text": "Important dates\n\nSept 4: Labor day\nSept 5: Last day to add/swap/drop\nOct 19-20: Midterm break\nNov 17: Withdrawal deadline\nNov 22-26: Thanksgiving\nDec 7: Last day of class\nDec 19: Final grade submission\n\nClick here for the full Marquette academic calendar."
  },
  {
    "objectID": "computing-troubleshooting.html",
    "href": "computing-troubleshooting.html",
    "title": "Computing troubleshooting",
    "section": "",
    "text": "If you‚Äôre having difficulty launching an RStudio session from your reserved container, go to status.oit.duke.edu and scroll down to Teaching and Learning Tools. Under this heading you‚Äôll find an entry called Container Manager (CMGR Coursework Containers).\n\nIf the status shows something other than Operational, this means there is a known incident with the containers. Check back later to see if it‚Äôs been resolved. If there‚Äôs a deadline coming up soon, post on the course forum to let us know that there‚Äôs an issue. We can look into how quickly it might get resolved and decide on what to do about the deadline accordingly. Note: We don‚Äôt anticipate this to happen regularly, the systems are Operational a huge majority of the time!\nIf the status shows Operational, this means the system is expected to be working. Check your internet connection, if need be, restart your computer to ensure a fresh new connection. If your issue persists, post on the course forum with details on what you‚Äôve tried and the errors you see (including verbatim errors and/or screenshots).\nEither way you can also fill out the form here, which will notify our the R TA for the department as well as our undergraduate coordinator. They‚Äôll be able to help diagnose the issue."
  }
]